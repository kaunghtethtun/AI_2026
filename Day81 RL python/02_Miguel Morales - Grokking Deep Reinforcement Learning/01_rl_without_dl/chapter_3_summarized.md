# Chapter 3: Balancing Immediate and Long-Term Goals - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ **sequential feedback** á€›á€²á€· challenges á€€á€­á€¯ isolation á€‘á€²á€™á€¾á€¬ á€œá€±á€·á€œá€¬á€•á€«á€á€šá€ºá‹ Agent á€€ MDP á€›á€²á€· dynamics (transition function, reward function) á€€á€­á€¯ **á€á€­**á€‘á€¬á€¸á€•á€¼á€®á€¸ optimal policy á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€•á€«á€á€šá€ºá‹ á€’á€«á€€á€­á€¯ **planning** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph Planning["ğŸ“š Planning: Agent knows MDP"]
        PE["Policy Evaluation<br/>VÏ€ á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€„á€ºá€¸"] --> PI["Policy Improvement<br/>Ï€' greedy policy á€›á€šá€°á€á€¼á€„á€ºá€¸"]
        PI --> CONV{"Ï€' == Ï€?"}
        CONV -->|No| PE
        CONV -->|Yes| OPT["ğŸ† Optimal Policy Ï€*"]
    end
    
    style OPT fill:#ffd43b,color:#000
    style CONV fill:#ff922b,color:#fff
```

á€¡á€“á€­á€€ algorithms (áƒ) á€á€¯:
1. **Policy Evaluation** â€” Policy á€á€…á€ºá€á€¯á€›á€²á€· value function á€€á€­á€¯ á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€„á€ºá€¸
2. **Policy Iteration (PI)** â€” Policy evaluation + policy improvement á€€á€­á€¯ alternate á€œá€¯á€•á€ºá€•á€¼á€®á€¸ optimal policy á€›á€¾á€¬á€á€¼á€„á€ºá€¸
3. **Value Iteration (VI)** â€” Truncated policy evaluation + improvement á€€á€­á€¯ merge á€œá€¯á€•á€ºá€•á€¼á€®á€¸ optimal policy á€›á€¾á€¬á€á€¼á€„á€ºá€¸

---

## 2. Return (á€•á€¼á€”á€ºá€›á€á€²á€· á€…á€¯á€…á€¯á€•á€±á€«á€„á€ºá€¸ Reward)

### Return á á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º

Return $G_t$ á€†á€­á€¯á€á€¬ time step $t$ á€€á€”á€± episode á€•á€¼á€®á€¸á€†á€¯á€¶á€¸á€á€²á€·á€¡á€‘á€­ á€›á€á€²á€· (discounted) rewards á€•á€±á€«á€„á€ºá€¸á€œá€’á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

### Recursive Definition

$$G_t = R_{t+1} + \gamma G_{t+1}$$

á€’á€® recursive form á€Ÿá€¬ RL algorithms á€¡á€á€±á€¬á€ºá€™á€»á€¬á€¸á€™á€»á€¬á€¸á€›á€²á€· á€¡á€á€¼á€±á€á€¶á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 3. Policy (á€™á€°á€á€«á€’)

### Policy á€†á€­á€¯á€á€¬á€˜á€¬á€œá€²

Policy $\pi$ á€†á€­á€¯á€á€¬ agent á€›á€²á€· behavior plan á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ State á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€¡á€á€½á€€á€º á€˜á€šá€º action á€šá€°á€›á€™á€œá€² á€†á€­á€¯á€á€¬á€€á€­á€¯ á€Šá€½á€¾á€”á€ºá€•á€¼á€•á€«á€á€šá€ºá‹

| Policy á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ | á€–á€±á€¬á€ºá€•á€¼á€á€»á€€á€º | Formula |
|---|---|---|
| **Deterministic** | State á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€¡á€á€½á€€á€º action á€á€…á€ºá€á€¯á€á€Šá€ºá€¸ | $\pi(s) = a$ |
| **Stochastic** | State á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€¡á€á€½á€€á€º action probability distribution | $\pi(a|s) = P(a_t = a | s_t = s)$ |

### Policy á€¥á€•á€™á€¬á€™á€»á€¬á€¸ (Frozen Lake)
- **Go-get-it policy** â€” Goal á€†á€® á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€‘á€­á€¯á€¸á€á€½á€¬á€¸á€á€²á€· aggressive policy (3.4% success rate)
- **Careful policy** â€” Holes á€€á€­á€¯ á€›á€¾á€±á€¬á€„á€ºá€•á€¼á€®á€¸ safe á€›á€½á€±á€¸á€á€»á€šá€ºá€á€²á€· policy (53.7% success rate)
- **Adversarial policy** â€” Goal á€€á€­á€¯ á€›á€±á€¬á€€á€ºá€”á€­á€¯á€„á€ºá€á€¼á€± 0% á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€–á€”á€ºá€á€®á€¸á€‘á€¬á€¸á€á€²á€· policy

---

## 4. Value Functions (á€á€”á€ºá€–á€­á€¯á€¸ Functions á€™á€»á€¬á€¸)

### State-Value Function $V^\pi(s)$

State $s$ á€€á€”á€± policy $\pi$ á€€á€­á€¯ follow á€•á€¼á€®á€¸ episode á€•á€¼á€®á€¸á€†á€¯á€¶á€¸á€á€²á€·á€¡á€‘á€­ á€›á€”á€­á€¯á€„á€ºá€™á€šá€·á€º expected return:

$$V^\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]$$

- "á€’á€® state á€€á€”á€± policy $\pi$ á€€á€­á€¯á€œá€­á€¯á€€á€ºá€›á€„á€º á€•á€»á€™á€ºá€¸á€™á€»á€¾ á€˜á€šá€ºá€œá€±á€¬á€€á€º reward á€›á€™á€œá€²?"

### Action-Value Function $Q^\pi(s, a)$

State $s$ á€™á€¾á€¬ action $a$ á€šá€°á€•á€¼á€®á€¸ á€€á€»á€”á€ºá€á€¬ policy $\pi$ follow á€œá€¯á€•á€ºá€›á€„á€º á€›á€”á€­á€¯á€„á€ºá€™á€šá€·á€º expected return:

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t | s_t = s, a_t = a]$$

- "á€’á€® state á€™á€¾á€¬ á€’á€® action á€€á€­á€¯ á€›á€½á€±á€¸á€•á€¼á€®á€¸ á€€á€»á€”á€ºá€á€¬ policy follow á€›á€„á€º á€˜á€šá€ºá€œá€±á€¬á€€á€º reward á€›á€™á€œá€²?"

### Action-Advantage Function $A^\pi(s, a)$

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

- "Default (V) á€‘á€€á€º action $a$ á€€á€­á€¯ á€›á€½á€±á€¸á€›á€„á€º á€˜á€šá€ºá€œá€±á€¬á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€™á€œá€²?" á€€á€­á€¯ á€•á€¼á€•á€«á€á€šá€ºá‹
- $A > 0$ á€†á€­á€¯á€›á€„á€º default á€‘á€€á€º á€€á€±á€¬á€„á€ºá€¸á€á€šá€º, $A < 0$ á€†á€­á€¯á€›á€„á€º default á€‘á€€á€º á€Šá€¶á€·á€á€šá€º, $A = 0$ á€†á€­á€¯á€›á€„á€º default á€”á€²á€· á€á€°á€á€šá€ºá‹
### Value Functions á€¡á€á€»á€„á€ºá€¸á€¡á€”á€¾á€­á€¯á€„á€ºá€¸á€•á€¯á€¶

```mermaid
graph LR
    subgraph VF["Value Functions Relationship"]
        V["VÏ€(s)<br/>State Value<br/>á€•á€»á€™á€ºá€¸á€™á€»á€¾ expected return"] 
        Q["QÏ€(s,a)<br/>Action Value<br/>action á€›á€½á€±á€¸á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º return"]
        A["AÏ€(s,a)<br/>Advantage<br/>Q - V = á€˜á€šá€ºá€œá€±á€¬á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸?"]
    end
    Q -->|"minus"| V
    Q -.->|"= V + A"| A
    V -.->|"baseline"| A
    
    style V fill:#4CAF50,color:#fff
    style Q fill:#2196F3,color:#fff
    style A fill:#ff9800,color:#fff
```

> ğŸ’¡ **á€¡á€™á€¾á€á€ºá€‘á€¬á€¸á€•á€«:** á€¡á€¡á€¬á€¸á€œá€¯á€¶á€¸ policy $\pi$ á€€á€­á€¯ follow á€œá€¯á€•á€ºá€™á€¾ á€á€½á€€á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ $V^\pi$, $Q^\pi$, $A^\pi$ á€¡á€¬á€¸á€œá€¯á€¶á€¸ policy-dependent á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
---

## 5. Optimality

### Optimal Policy $\pi^\ast$

$$V^\ast(s) = \max_\pi V^\pi(s), \quad \forall s \in S$$

- MDP á€á€…á€ºá€á€¯á€™á€¾á€¬ **optimal state-value function** $V^\ast$ á€€ **á€á€…á€ºá€á€¯á€á€Šá€ºá€¸** á€›á€¾á€­á€•á€«á€á€šá€ºá‹
- á€’á€«á€•á€±á€™á€šá€·á€º **optimal policy** $\pi^\ast$ á€€á€á€±á€¬á€· **á€á€…á€ºá€á€¯á€‘á€€á€ºá€•á€­á€¯á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º** (Q-function á€™á€¾á€¬ equal values á€›á€¾á€­á€á€²á€· actions á€›á€¾á€­á€”á€­á€¯á€„á€ºá€œá€­á€¯á€·)á‹

### Bellman Equation

$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^\pi(s') \right]$$

á€’á€® equation á€Ÿá€¬ state-value function á€€á€­á€¯ recursive form á€–á€¼á€„á€·á€º á€–á€±á€¬á€ºá€•á€¼á€•á€«á€á€šá€º â€” current state á€›á€²á€· value á€€á€­á€¯ immediate reward á€”á€²á€· discounted next state value á€•á€±á€«á€„á€ºá€¸á€‘á€¬á€¸á€á€¬á€•á€«á‹

```mermaid
graph TD
    S["State s<br/>VÏ€(s)"] -->|"action a, Ï€(a|s)"| T1{"Transition T(s,a,s')"}
    T1 -->|"pâ‚, râ‚"| S1["s'â‚: Râ‚ + Î³VÏ€(s'â‚)"]
    T1 -->|"pâ‚‚, râ‚‚"| S2["s'â‚‚: Râ‚‚ + Î³VÏ€(s'â‚‚)"]
    T1 -->|"pâ‚ƒ, râ‚ƒ"| S3["s'â‚ƒ: Râ‚ƒ + Î³VÏ€(s'â‚ƒ)"]
    
    style S fill:#4CAF50,color:#fff
    style S1 fill:#64B5F6,color:#fff
    style S2 fill:#64B5F6,color:#fff
    style S3 fill:#64B5F6,color:#fff
```

### Bellman Optimality Equation

$$V^\ast(s) = \max_a \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma V^\ast(s') \right]$$

$$Q^\ast(s,a) = \sum_{s'} T(s,a,s') \left[ R(s,a,s') + \gamma \max_{a'} Q^\ast(s', a') \right]$$

---

## 6. Policy Evaluation (PE): Policy á€›á€²á€· Value Function á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€„á€ºá€¸

### Algorithm

Policy $\pi$ á€”á€²á€· MDP $P$ á€€á€­á€¯ input á€šá€°á€•á€¼á€®á€¸ $V^\pi$ á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

1. $V(s) = 0$ for all states (initialize)
2. State space á€€á€­á€¯ sweep (iterate) á€œá€¯á€•á€ºá€•á€¼á€®á€¸ Bellman equation á€á€¯á€¶á€¸ update:

$$V(s) \leftarrow \sum_{s', r} T(s, \pi(s), s') \left[ R + \gamma V(s') \right]$$

3. $V$ values á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯ $\theta$ (threshold) á€‘á€€á€º á€„á€šá€ºá€›á€„á€º converged á€Ÿá€¯ á€šá€°á€†

### Python Implementation

```python
def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):
    V = np.zeros(len(P))
    while True:
        prev_V = V.copy()
        for s in range(len(P)):
            V[s] = 0
            for prob, next_state, reward, done in P[s][pi(s)]:
                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))
        if np.max(np.abs(prev_V - V)) < theta:
            break
    return V
```

### á€¥á€•á€™á€¬ á€›á€œá€’á€ºá€™á€»á€¬á€¸ (Frozen Lake)

| Policy | Start $V^\pi(s_0)$ | Success Rate | Convergence |
|---|---|---|---|
| Random | $0.0955$ | ~9.6% | 218 iterations |
| Go-get-it | $0.0342$ | ~3.4% | 66 iterations |
| Careful | $0.5370$ | ~53.7% | 546 iterations |
| **Careful+** (improved) | $0.5420$ | **~73.2%** | 574 iterations |
| **Optimal** $\pi^\ast$ | $0.5420$ | **~73.2%** | PI/VI converge |

---

## 7. Policy Improvement (PI): Value Function á€€á€”á€± Better Policy á€›á€šá€°á€á€¼á€„á€ºá€¸

### á€šá€¯á€á€¹á€á€­

State-value function $V^\pi$ á€€á€”á€± Q-function á€€á€­á€¯ compute á€œá€¯á€•á€ºá€•á€¼á€®á€¸ greedy action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸:

$$\pi'(s) = \arg\max_a \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma V^\pi(s') \right]$$

### Python Implementation

```python
def policy_improvement(V, P, gamma=1.0):
    Q = np.zeros((len(P), len(P[0])))
    for s in range(len(P)):
        for a in range(len(P[s])):
            for prob, next_state, reward, done in P[s][a]:
                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))
    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return new_pi
```

### á€¥á€•á€™á€¬
- **Careful policy** ($V(s_0) = 0.4079$, 53.7%) $\xrightarrow{\text{improvement}}$ **Careful+** ($V(s_0) = 0.5420$, 73.2%)

```mermaid
graph LR
    C["ğŸ›¡ï¸ Careful Policy<br/>V(sâ‚€)=0.4079<br/>53.7% success"] -->|"Policy Improvement<br/>Ï€'(s) = argmax QÏ€(s,a)"| CP["ğŸ† Careful+ Policy<br/>V(sâ‚€)=0.5420<br/>73.2% success"]
    CP -->|"Re-improve<br/>Ï€'' == Ï€' â†’ Optimal!"| CP
    
    style C fill:#ff922b,color:#fff
    style CP fill:#51cf66,color:#fff
```

> ğŸ’¡ Careful+ á€›á€²á€· Q-function á€€á€”á€± greedy policy á€šá€°á€›á€„á€º á€™á€°á€œ policy á€›á€•á€¼á€”á€ºá€›á€á€²á€·á€¡á€á€½á€€á€º á€…á€…á€á€»á€„á€ºá€¸á€™á€¾á€¬á€•á€² optimal á€–á€¼á€…á€ºá€á€½á€¬á€¸!

---

## 8. Policy Iteration: Evaluation + Improvement Alternative á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

### Algorithm Flow

```mermaid
graph TD
    START(["ğŸ² Random Policy Ï€"]) --> EVAL["Policy Evaluation<br/>VÏ€ = PE(Ï€, P, Î³)"]
    EVAL --> IMPROVE["Policy Improvement<br/>Ï€' = argmax QÏ€"]
    IMPROVE --> CHECK{"Ï€' == Ï€?"}
    CHECK -->|"No â†’ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€á€±á€¸"| EVAL
    CHECK -->|"Yes â†’ Converged!"| OPTIMAL["ğŸ† V*, Ï€*"]
    
    style START fill:#868e96,color:#fff
    style OPTIMAL fill:#ffd43b,color:#000,stroke:#333,stroke-width:2px
    style CHECK fill:#ff922b,color:#fff
```

```
1. Random policy Ï€ á€–á€”á€ºá€á€®á€¸á€•á€«
2. Loop:
   a. V = PolicyEvaluation(Ï€, P)    â† value function á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸ (converge á€‘á€­)
   b. Ï€' = PolicyImprovement(V, P)  â† greedy policy á€‘á€¯á€á€ºá€šá€°
   c. Ï€' == Ï€ á€†á€­á€¯á€›á€„á€º break          â† improvement á€™á€›á€¾á€­á€á€±á€¬á€· = optimal
   d. Ï€ = Ï€'
3. Return V*, Ï€*
```

### Python Implementation

```python
def policy_iteration(P, gamma=1.0, theta=1e-10):
    random_actions = np.random.choice(tuple(P[0].keys()), len(P))
    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]
    while True:
        old_pi = {s:pi(s) for s in range(len(P))}
        V = policy_evaluation(pi, P, gamma, theta)
        pi = policy_improvement(V, P, gamma)
        if old_pi == {s:pi(s) for s in range(len(P))}:
            break
    return V, pi
```

### Key Properties
- **Convergence guarantee** â€” á€˜á€šá€º policy á€€á€”á€±á€…á€•á€²á€… (adversarial policy á€•á€„á€º) optimal policy á€€á€­á€¯ converge á€•á€«á€á€šá€ºá‹ Local optima á€™á€¾á€¬ stuck á€–á€¼á€…á€ºá€™á€á€½á€¬á€¸á€•á€«á‹
- **Multiple optimal policies** â€” FL environment á€™á€¾á€¬ state 6 á€€á€­á€¯ Left/Right á€˜á€šá€ºá€Ÿá€¬ á€›á€½á€±á€¸á€›á€½á€±á€¸ optimal á€–á€¼á€…á€ºá€•á€«á€á€šá€º (equal Q-values)á‹
- **Tie-breaking** â€” Q-function ties á€€á€­á€¯ randomly break á€™á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€á€­á€‘á€¬á€¸á€›á€•á€«á€á€šá€º (infinite loop á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º)á‹

---

## 9. Value Iteration (VI): Greedily Greedifying Policies

### á€šá€¯á€á€¹á€á€­

Policy evaluation á€€ convergence á€‘á€­ run á€›á€á€¬ slow á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ VI á€™á€¾á€¬ policy evaluation á€€á€­á€¯ **single state sweep** (1 iteration) á€•á€² run á€•á€¼á€®á€¸ improvement á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

$$V(s) \leftarrow \max_a \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma V(s') \right]$$

```mermaid
graph TD
    V0["Vâ‚€ = zeros"] --> SW1["Sweep 1: V(s) â† max_a âˆ‘ TÂ·[R+Î³V]"]
    SW1 --> SW2["Sweep 2: V(s) â† max_a âˆ‘ TÂ·[R+Î³V]"]
    SW2 --> SW3["..."]
    SW3 --> SWN["Sweep N: converged!"]
    SWN --> EXT["Extract Ï€* = argmax Q*"]
    
    style V0 fill:#868e96,color:#fff
    style EXT fill:#ffd43b,color:#000
```

- **argmax** (PI á€¡á€á€½á€€á€º) á€¡á€…á€¬á€¸ **max** (VI á€¡á€á€½á€€á€º) á€€á€­á€¯ á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€º V á€‘á€² á€‘á€Šá€·á€ºá€•á€«á€á€šá€ºá‹
- Separate policy-improvement phase á€™á€œá€­á€¯á€•á€« â€” V-function converge á€•á€¼á€®á€¸á€™á€¾ argmax á€šá€°á€•á€¼á€®á€¸ optimal policy extract á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

### Python Implementation

```python
def value_iteration(P, gamma=1.0, theta=1e-10):
    V = np.zeros(len(P))
    while True:
        Q = np.zeros((len(P), len(P[0])))
        for s in range(len(P)):
            for a in range(len(P[s])):
                for prob, next_state, reward, done in P[s][a]:
                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))
        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:
            break
        V = np.max(Q, axis=1)
    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return V, pi
```

---

## 10. Policy Iteration vs Value Iteration

| Feature | Policy Iteration (PI) | Value Iteration (VI) |
|---|---|---|
| **Policy Evaluation** | Full convergence á€‘á€­ run | Single sweep (truncated) |
| **Policy Improvement** | Separate phase | V-function á€‘á€² built-in (max) |
| **Convergence** | Optimal policy á€€á€­á€¯ guaranteed | Optimal V-function á€€á€­á€¯ guaranteed |
| **Speed** | Policy evaluation slow á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º | Typically faster |
| **Policy tracking** | Policy á€€á€­á€¯ explicitly track | V-function á€€á€­á€¯á€•á€² track; policy á€€á€­á€¯ á€¡á€†á€¯á€¶á€¸á€™á€¾ extract |

---

## 11. Generalized Policy Iteration (GPI)

PI á€”á€¾á€„á€·á€º VI á€Ÿá€¬ **Generalized Policy Iteration (GPI)** á€›á€²á€· instances (á‚) á€á€¯ á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

```mermaid
graph LR
    subgraph GPI["Generalized Policy Iteration"]
        direction TB
        EVAL["Evaluation<br/>V â†’ VÏ€<br/>á€á€”á€ºá€–á€­á€¯á€¸ á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸"] <-->|"á€¡á€•á€¼á€”á€ºá€¡á€œá€¾á€”á€º"| IMPROV["Improvement<br/>Ï€ â†’ greedy(Ï€)<br/>á€™á€°á€á€«á€’ á€á€­á€¯á€¸á€á€€á€º"]
    end
    
    PI_box["Policy Iteration<br/>Full evaluation"] -.-> GPI
    VI_box["Value Iteration<br/>Truncated evaluation"] -.-> GPI
    
    style GPI fill:#e8f5e9,stroke:#4CAF50,stroke-width:2px
    style EVAL fill:#4CAF50,color:#fff
    style IMPROV fill:#2196F3,color:#fff
```

$$\text{GPI: } \underbrace{V \xrightarrow{\text{evaluate}} V^\pi}_{\text{Value function á€€á€­á€¯ policy á€†á€® á€á€»á€‰á€ºá€¸á€€á€•á€º}} \quad \underbrace{\pi \xrightarrow{\text{improve}} \pi'}_{\text{Policy á€€á€­á€¯ value function á€á€¯á€¶á€¸ improve}}$$

- GPI á€›á€²á€· general idea: **policy á€€á€­á€¯ value function estimates á€á€¯á€¶á€¸á€•á€¼á€®á€¸ improve** á€œá€¯á€•á€ºá€•á€¼á€®á€¸ **value function estimates á€€á€­á€¯ current policy á€›á€²á€· actual value á€†á€® improve** á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹
- PI $\Rightarrow$ evaluation á€€á€­á€¯ fully converge á€‘á€­ run
- VI $\Rightarrow$ evaluation á€€á€­á€¯ single sweep á€•á€² run
- á€’á€«á€•á€±á€™á€šá€·á€º á€˜á€šá€ºá€•á€¯á€¶á€…á€¶á€•á€² á€–á€¼á€…á€ºá€–á€¼á€…á€º optimal policy á€€á€­á€¯ converge á€•á€«á€á€šá€ºá‹

---

## 12. Slippery Walk Environments

á€…á€¬á€¡á€¯á€•á€ºá€‘á€²á€™á€¾á€¬ Frozen Lake á€¡á€•á€¼á€„á€º **Slippery Walk Five (SWF)** á€”á€²á€· **Slippery Walk Seven (SWS)** environments á€á€½á€±á€€á€­á€¯á€œá€Šá€ºá€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€º:

```
H(0) -- 1 -- 2 -- S(3) -- 4 -- 5 -- G(6)    (SWF: 7 states)
```

- Stochastic transitions á€›á€¾á€­á€•á€¼á€®á€¸ value propagation á€€á€­á€¯ observe á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€„á€·á€ºá€á€±á€¬á€ºá€•á€«á€á€šá€ºá‹
- VI á€€á€­á€¯ "always-left" adversarial policy á€€á€”á€±á€…á€•á€¼á€®á€¸ optimal policy á€›á€¾á€¬á€á€¬á€•á€¼á€á€•á€«á€á€šá€ºá‹

```python
import numpy as np

# áá‹ á€•á€á€ºá€á€”á€ºá€¸á€€á€»á€„á€º á€á€Šá€ºá€†á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸ (States: 0 1 2 3 4 5 6)
# H(0) - 1 - 2 - S(3) - 4 - 5 - G(6)
num_states = 7
actions = [0, 1] # 0: Left, 1: Right
gamma = 0.99     # Discount factor
threshold = 1e-6 # Convergence threshold

# Value table á€€á€­á€¯ zero á€”á€²á€· á€…á€á€„á€ºá€™á€šá€º
V = np.zeros(num_states)
# Goal (State 6) á€›á€²á€· value á€€ á€¡á€™á€¼á€² 1 á€–á€¼á€…á€ºá€™á€šá€ºá€œá€­á€¯á€· á€šá€°á€†á€”á€­á€¯á€„á€ºá€á€šá€º
V[6] = 0 

def get_transitions(s, a):
    """ á€œá€™á€ºá€¸á€á€»á€±á€¬á€ºá€”á€­á€¯á€„á€ºá€á€¼á€± (Slippery nature) á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€á€¼á€„á€ºá€¸ """
    # á€Šá€¬á€˜á€€á€ºá€á€½á€¬á€¸á€›á€„á€º á€Šá€¬á€›á€±á€¬á€€á€ºá€–á€­á€¯á€· 80%, á€˜á€šá€ºá€›á€±á€¬á€€á€ºá€á€½á€¬á€¸á€–á€­á€¯á€· 20% á€œá€­á€¯á€· á€¥á€•á€™á€¬á€•á€±á€¸á€‘á€¬á€¸á€•á€«á€á€šá€º
    if a == 1: # Right
        return [(0.8, s + 1 if s < 6 else 6), (0.2, s - 1 if s > 0 else 0)]
    else: # Left
        return [(0.8, s - 1 if s > 0 else 0), (0.2, s + 1 if s < 6 else 6)]

# á‚á‹ Value Iteration Main Loop
while True:
    delta = 0
    V_new = np.copy(V)
    
    for s in range(1, 6): # Hole (0) á€”á€²á€· Goal (6) á€€ Terminal states á€™á€­á€¯á€·á€œá€­á€¯á€· á€á€»á€”á€ºá€‘á€¬á€¸á€™á€šá€º
        old_v = V[s]
        
        action_values = []
        for a in actions:
            v_a = 0
            for prob, next_s in get_transitions(s, a):
                reward = 1.0 if next_s == 6 else 0.0
                v_a += prob * (reward + gamma * V[next_s])
            action_values.append(v_a)
        
        # á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ action á€›á€²á€· value á€€á€­á€¯ á€šá€°á€™á€šá€º (Bellman Update)
        V_new[s] = max(action_values)
        delta = max(delta, abs(old_v - V_new[s]))
    
    V = V_new
    if delta < threshold: # á€á€”á€ºá€–á€­á€¯á€¸á€á€½á€± á€™á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€±á€¬á€·á€›á€„á€º á€›á€•á€ºá€™á€šá€º
        break

print("Optimal Value Function:", V)
```

---

## 13. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

> Agent á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€ºá€€á€á€±á€¬á€· **expected return á€€á€­á€¯ maximize** á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€¡á€á€½á€€á€º policies, value functions (V, Q, A) á€á€½á€±á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹

### Chapter 3 Algorithms Summary

```mermaid
graph TB
    subgraph Algorithms["ğŸ“š Chapter 3 Planning Algorithms"]
        PE["Policy Evaluation<br/>Ï€ + MDP â†’ VÏ€"]
        PImp["Policy Improvement<br/>VÏ€ + MDP â†’ Ï€'"]
        PI["Policy Iteration<br/>PE + PI alternate â†’ Ï€*"]
        VI["Value Iteration<br/>Truncated PE + PI merged â†’ Ï€*"]
        GPI["GPI<br/>General Framework"]
    end
    
    PE --> PI
    PImp --> PI
    PI --> GPI
    VI --> GPI
    
    style GPI fill:#ffd43b,color:#000,stroke:#333,stroke-width:2px
    style PI fill:#4CAF50,color:#fff
    style VI fill:#2196F3,color:#fff
```

### Key Equations Summary

| Concept | Equation |
|---|---|
| Return | $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}$ |
| State-Value | $V^\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s]$ |
| Action-Value | $Q^\pi(s,a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]$ |
| Advantage | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ |
| Bellman | $V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} T(s,a,s')[R + \gamma V^\pi(s')]$ |
| Bellman Optimality | $V^\ast(s) = \max_a \sum_{s'} T(s,a,s')[R + \gamma V^\ast(s')]$ |
| PE Update | $V(s) \leftarrow \sum_{s'} T(s, \pi(s), s')[R + \gamma V(s')]$ |
| PI Improvement | $\pi'(s) = \arg\max_a Q^\pi(s,a)$ |
| VI Update | $V(s) \leftarrow \max_a \sum_{s'} T(s,a,s')[R + \gamma V(s')]$ |

á€¡á€“á€­á€€ á€á€­á€›á€™á€šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. **Policy** â€” State-to-action mapping (deterministic/stochastic)
2. **Value Functions** â€” $V^\pi(s)$, $Q^\pi(s,a)$, $A^\pi(s,a)$ = expected return estimates
3. **Policy Evaluation** â€” Policy + MDP $\rightarrow$ value function estimate
4. **Policy Improvement** â€” Value function + MDP $\rightarrow$ greedy (improved) policy
5. **Policy Iteration** â€” Evaluation + Improvement alternate $\rightarrow$ optimal policy
6. **Value Iteration** â€” Truncated evaluation + improvement merged $\rightarrow$ optimal policy
7. **GPI** â€” PI á€”á€²á€· VI á€›á€²á€· general framework

> **Chapter 3 vs Chapter 4:** Chapter 3 á€™á€¾á€¬ agent á€€ MDP á€€á€­á€¯ **á€á€­**á€•á€¼á€®á€¸ planning á€œá€¯á€•á€ºá€•á€«á€á€šá€º (sequential feedback only)á‹ Chapter 4 á€™á€¾á€¬ agent á€€ MDP á€€á€­á€¯ **á€™á€á€­**á€˜á€² evaluative feedback á€€á€”á€± interact á€•á€¼á€®á€¸ á€á€„á€ºá€šá€°á€•á€«á€™á€šá€ºá‹
