# á€¡á€á€”á€ºá€¸ á‡ â€” á€•á€”á€ºá€¸á€á€­á€¯á€„á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€•á€­á€¯á€™á€­á€¯á€‘á€­á€›á€±á€¬á€€á€ºá€…á€½á€¬ á€”á€¾á€„á€·á€º á€•á€­á€¯á€™á€­á€¯á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€…á€½á€¬ á€›á€›á€¾á€­á€á€¼á€„á€ºá€¸ (Achieving Goals More Effectively and Efficiently)

> *"Efficiency á€†á€­á€¯á€á€¬ á€¡á€™á€¾á€”á€ºá€€á€”á€ºá€†á€¯á€¶á€¸á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€”á€²á€· á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€¼á€®á€¸ã€Effectiveness á€†á€­á€¯á€á€¬ á€¡á€™á€¾á€”á€ºá€€á€”á€ºá€†á€¯á€¶á€¸á€¡á€›á€¬á€€á€­á€¯ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€á€šá€ºá‹"*
> â€” Peter Drucker (á€á€±á€á€ºá€á€…á€º management á á€–á€á€„á€º)

## á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ á€á€„á€ºá€šá€°á€›á€™á€Šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

- Reinforcement learning agent á€™á€»á€¬á€¸á€€á€­á€¯ á€á€€á€ºá€á€²á€á€²á€· environments á€™á€»á€¬á€¸á€™á€¾á€¬ optimal performance á€†á€® á€›á€±á€¬á€€á€ºá€›á€¾á€­á€›á€¬á€á€½á€„á€º **á€•á€­á€¯á€™á€­á€¯á€‘á€­á€›á€±á€¬á€€á€ºá€…á€½á€¬ (more effective)** á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€”á€Šá€ºá€¸á€€á€­á€¯ á€á€„á€ºá€šá€°á€›á€™á€Šá€ºá‹
- Agent á€™á€»á€¬á€¸á€€ experience data á€€á€­á€¯ á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€†á€¯á€¶á€¸ á€¡á€á€¯á€¶á€¸á€á€»á€•á€¼á€®á€¸ á€•á€”á€ºá€¸á€á€­á€¯á€„á€ºá€™á€»á€¬á€¸á€€á€­á€¯ **á€•á€­á€¯á€™á€­á€¯á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€…á€½á€¬ (more efficient)** á€›á€›á€¾á€­á€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€”á€Šá€ºá€¸á€€á€­á€¯ á€á€„á€ºá€šá€°á€›á€™á€Šá€ºá‹
- á€šá€á€„á€º chapters á€™á€»á€¬á€¸á€™á€¾ agent á€™á€»á€¬á€¸á€€á€­á€¯ improve á€œá€¯á€•á€ºá€•á€¼á€®á€¸ data á€€á€­á€¯ á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ á€¡á€á€¯á€¶á€¸á€á€»á€€á€¬ performance á€€á€­á€¯ á€•á€­á€¯á€™á€¼á€”á€ºá€†á€”á€ºá€…á€½á€¬ optimize á€œá€¯á€•á€ºá€›á€™á€Šá€ºá‹

---

## á‡.á â€” á€”á€­á€’á€«á€”á€ºá€¸

á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ á€šá€á€„á€ºá€¡á€á€”á€ºá€¸á€€ agent á€™á€»á€¬á€¸á€€á€­á€¯ improvement á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Improvement line á‚ á€á€¯ á€›á€¾á€­á€•á€«á€á€šá€º:

### á€•á€‘á€™ improvement line â€” Î»-return á€–á€¼á€„á€·á€º á€•á€­á€¯á€á€­á€¯á€„á€ºá€™á€¬á€á€²á€· target á€™á€»á€¬á€¸á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸

Chapter 5 á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· **Î»-return** á€€á€­á€¯ generalized policy iteration (GPI) pattern á€›á€²á€· policy evaluation á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€¡á€á€½á€€á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹ On-policy á€”á€¾á€„á€·á€º off-policy methods á€”á€¾á€…á€ºá€™á€»á€­á€¯á€¸á€œá€¯á€¶á€¸á€¡á€á€½á€€á€º Î»-return á€€á€­á€¯ explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ **Eligibility traces** á€–á€¼á€„á€·á€º Î»-return á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º credit á€€á€­á€¯ á€™á€¾á€”á€ºá€€á€”á€ºá€á€²á€· state-action pairs á€™á€»á€¬á€¸á€†á€® á€•á€­á€¯á€™á€¼á€”á€ºá€†á€”á€ºá€…á€½á€¬ á€•á€¼á€”á€·á€ºá€•á€½á€¬á€¸á€…á€±á€•á€¼á€®á€¸ value-function estimates á€™á€»á€¬á€¸ actual values á€™á€»á€¬á€¸á€”á€²á€· á€•á€­á€¯á€™á€¼á€”á€ºá€”á€®á€¸á€€á€•á€ºá€œá€¬á€…á€±á€•á€«á€á€šá€ºá‹

### á€’á€¯á€á€­á€š improvement line â€” Environment model á€€á€­á€¯ á€á€„á€ºá€šá€°á€á€¼á€„á€ºá€¸

Experience samples á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ environment á€›á€²á€· model (MDP) á€€á€­á€¯ á€á€„á€ºá€šá€°á€á€²á€· algorithms á€™á€»á€¬á€¸á€€á€­á€¯ explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€®á€”á€Šá€ºá€¸á€–á€¼á€„á€·á€º data á€€á€­á€¯ á€¡á€™á€»á€¬á€¸á€†á€¯á€¶á€¸ exploit á€œá€¯á€•á€ºá€•á€¼á€®á€¸ model á€™á€á€„á€ºá€šá€°á€á€²á€· methods á€™á€»á€¬á€¸á€‘á€€á€º optimal policy á€†á€® á€•á€­á€¯á€™á€¼á€”á€ºá€›á€±á€¬á€€á€ºá€á€á€ºá€•á€«á€á€šá€ºá‹ á€’á€® algorithms group á€€á€­á€¯ **model-based reinforcement learning** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

> ğŸ’¡ á€’á€® improvement lines á‚ á€á€¯á€€á€­á€¯ á€á€®á€¸á€á€¼á€¬á€¸á€…á€® explore á€œá€¯á€•á€ºá€•á€±á€™á€šá€·á€ºã€á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€á€¬á€¸á€™á€¼á€…á€ºá€á€¬ á€™á€›á€¾á€­á€•á€«á‹ Chapter á€•á€¼á€®á€¸á€›á€„á€º á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€€á€¼á€Šá€·á€ºá€á€„á€·á€ºá€•á€«á€á€šá€ºá‹

---

### RL á€¡á€á€¯á€¶á€¸á€¡á€”á€¾á€¯á€”á€ºá€¸ â€” Planning vs. Model-free RL vs. Model-based RL

| Method Type | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | á€¥á€•á€™á€¬ |
|---|---|---|
| **Planning** | Environment á€›á€²á€· model (MDP) á€€á€­á€¯ **á€œá€­á€¯á€¡á€•á€º**á€•á€¼á€®á€¸ policy á€‘á€¯á€á€ºá€•á€±á€¸á€á€šá€ºá‹ State-space planning (state space á€‘á€² search) á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º plan-space planning (plan á€¡á€¬á€¸á€œá€¯á€¶á€¸ space á€‘á€² search) á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€šá€ºá‹ | Value Iteration, Policy Iteration |
| **Model-free RL** | Environment model **á€™á€œá€­á€¯á€¡á€•á€ºá€˜á€²** policy á€‘á€¯á€á€ºá€•á€±á€¸á€”á€­á€¯á€„á€ºá€á€šá€ºá‹ MDP/map/model á€™á€á€¯á€¶á€¸á€˜á€² trial-and-error learning á€–á€¼á€„á€·á€º policies á€›á€›á€¾á€­á€á€šá€ºá‹ | MC, SARSA, Q-learning |
| **Model-based RL** | Model á€€á€­á€¯ **á€€á€¼á€­á€¯á€á€„á€ºá€™á€œá€­á€¯á€¡á€•á€º**á€•á€±á€™á€šá€·á€ºã€interaction á€–á€¼á€„á€·á€º **á€á€„á€ºá€šá€°**á€•á€¼á€®á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€šá€ºá‹ | Dyna-Q, Trajectory Sampling |

```mermaid
graph TD
    subgraph PLAN["ğŸ“ Planning Methods"]
        P["Value Iteration / Policy Iteration"]
        P -->|"require"| MDP1["MDP á€€á€¼á€­á€¯á€á€„á€ºá€•á€±á€¸á€›á€™á€Šá€º"]
    end
    
    subgraph MF["ğŸ“Š Model-free RL"]
        MF1["MC / SARSA / Q-learning"]
        MF1 -->|"don't need"| MDP2["MDP á€™á€œá€­á€¯á€¡á€•á€º"]
    end
    
    subgraph MB["ğŸ§  Model-based RL"]
        MB1["Dyna-Q / Trajectory Sampling"]
        MB1 -->|"learn from interaction"| MDP3["MDP á€€á€­á€¯ á€á€„á€ºá€šá€°"]
        MB1 -->|"and use for"| PLAN2["Planning / Simulation"]
    end
    
    style P fill:#ef5350,color:#fff
    style MF1 fill:#2196F3,color:#fff
    style MB1 fill:#4CAF50,color:#fff
```

---

## á‡.á‚ â€” á€•á€­á€¯á€á€­á€¯á€„á€ºá€™á€¬á€á€±á€¬ Target á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º Policy á€™á€»á€¬á€¸á€€á€­á€¯ Improve á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

á€’á€® section á€™á€¾á€¬ policy-evaluation methods á€™á€»á€¬á€¸á€á€½á€„á€º á€•á€­á€¯á€á€­á€¯á€„á€ºá€™á€¬á€á€²á€· targets á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸á€€á€­á€¯ á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€á€šá€ºá‹ Chapter 5 á€™á€¾á€¬ MC approach, TD approach, á€”á€¾á€„á€·á€º Î»-return á€€á€­á€¯ á€á€¯á€¶á€¸á€á€²á€· targets á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á value functions estimate á€œá€¯á€•á€ºá€á€²á€· policy-evaluation methods á€™á€»á€¬á€¸á€€á€­á€¯ explore á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹

TD(Î») á€á€Šá€º policy evaluation á€¡á€á€½á€€á€º Î»-return á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€²á€· prediction method á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Control problem á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€¬á€á€½á€„á€ºã€action-value functions estimate á€œá€¯á€•á€ºá€›á€”á€º policy-evaluation method á€”á€¾á€„á€·á€º exploration á€á€½á€„á€·á€ºá€•á€¼á€¯á€á€²á€· policy-improvement method á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€® section á€™á€¾á€¬ SARSA á€”á€¾á€„á€·á€º Q-learning á€”á€¾á€„á€·á€ºá€†á€„á€ºá€á€°á€•á€¼á€®á€¸ Î»-return á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€²á€· control methods á€™á€»á€¬á€¸á€€á€­á€¯ á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€á€šá€ºá‹

---

### á€”á€™á€°á€”á€¬ Environment â€” Slippery Walk Seven (SWS)

á€’á€®á€¡á€á€”á€ºá€¸á€›á€²á€· algorithms á€™á€»á€¬á€¸á€€á€­á€¯ á€™á€­á€á€ºá€†á€€á€ºá€›á€¬á€á€½á€„á€º á€šá€á€„á€ºá€¡á€á€”á€ºá€¸á€€ **Slippery Walk Seven (SWS)** environment á€€á€­á€¯ á€•á€² á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹ Chapter á€¡á€†á€¯á€¶á€¸á€™á€¾á€¬ á€•á€­á€¯á€á€€á€ºá€á€²á€á€²á€· environments á€™á€»á€¬á€¸á€á€½á€„á€º test á€•á€«á€™á€šá€ºá‹

```
[â˜ ï¸ State 0] â€” [S1] â€” [S2] â€” [S3] â€” [S4] â€” [S5] â€” [S6] â€” [S7] â€” [ğŸ† State 8, +1]
```

SWS á properties:
- Non-terminal states á‡ á€á€¯ (states 1â€“7)
- Terminal states: State 0 (reward = 0) á€”á€¾á€„á€·á€º State 8 (reward = +1)
- Actions: Left (0), Right (1)
- **Slippery** (á€á€»á€±á€¬á€ºá€á€á€º): 50% intended, 33.3% stay, 16.7% opposite

> Agent á€€ transition probabilities á€€á€­á€¯ access á€™á€›á€•á€«á‹ Environment á dynamics á€€á€­á€¯ agent á€™á€á€­á€•á€«á‹

---

## á‡.áƒ â€” SARSA(Î»): Multi-step Estimates á€¡á€•á€±á€«á€º á€¡á€á€¼á€±á€á€¶á Step á€á€­á€¯á€„á€ºá€¸á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º Policy á€€á€­á€¯ Improve á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

### SARSA(Î») á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²

SARSA(Î») á€á€Šá€º á€™á€°á€œ SARSA agent á€€á€­á€¯ á€›á€­á€¯á€¸á€›á€­á€¯á€¸á€á€”á€ºá€¸á€á€”á€ºá€¸ improve á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€¬á€•á€«á‹ SARSA á€”á€¾á€„á€·á€º SARSA(Î») á á€¡á€“á€­á€€ á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€á€á€±á€¬á€· **one-step bootstrapping target (TD target)** á€¡á€…á€¬á€¸ **Î»-return** á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€•á€«á€•á€²! Basics á€€á€­á€¯ á€€á€±á€¬á€„á€ºá€¸á€€á€±á€¬á€„á€ºá€¸ á€á€„á€ºá€šá€°á€‘á€¬á€¸á€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º á€•á€­á€¯á€›á€¾á€¯á€•á€ºá€‘á€½á€±á€¸á€á€²á€· concepts á€á€½á€±á€€ á€•á€­á€¯á€œá€½á€šá€ºá€€á€°á€á€½á€¬á€¸á€á€¬ á€™á€¼á€„á€ºá€›á€™á€œá€¬á€¸?

Chapter 5 á€™á€¾á€¬ á€™á€­á€á€ºá€†á€€á€ºá€á€²á€·á€á€²á€· **eligibility traces** concept á€€á€­á€¯ á€•á€­á€¯á€”á€€á€ºá€”á€²á€…á€½á€¬ á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€™á€šá€ºá‹ Chapter 5 á€™á€¾á€¬ introduce á€œá€¯á€•á€ºá€á€²á€·á€á€²á€· eligibility trace á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€€á€­á€¯ **accumulating trace** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º state á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º state-action pairs á€™á€»á€¬á€¸ reward á€¡á€á€½á€€á€º á€á€¬á€á€”á€ºá€›á€¾á€­á€™á€›á€¾á€­ trace á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€²á€· á€”á€Šá€ºá€¸á€œá€™á€ºá€¸ á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€›á€¾á€­á€•á€«á€á€šá€ºá‹

á€’á€® section á€™á€¾á€¬ accumulating trace á€€á€­á€¯ control problem á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€”á€º adapt á€œá€¯á€•á€ºá€•á€¼á€®á€¸ **replacing trace** á€†á€­á€¯á€á€²á€· trace á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ á€¡á€á€…á€ºá€€á€­á€¯á€œá€Šá€ºá€¸ explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€”á€¾á€…á€ºá€™á€»á€­á€¯á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ SARSA(Î») agent á€™á€¾á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    SARSA["SARSA<br/>(one-step TD control)"] -->|"+ eligibility traces<br/>+ Î» parameter"| SARSA_L["SARSA(Î»)<br/>(multi-step credit)"]
    
    TDL["TD(Î»)<br/>(prediction with traces)"] -->|"+ Q-function<br/>+ Îµ-greedy"| SARSA_L
    
    style SARSA fill:#ff922b,color:#fff
    style TDL fill:#2196F3,color:#fff
    style SARSA_L fill:#4CAF50,color:#fff
```

---

### á€á€™á€­á€¯á€„á€ºá€¸á€¡á€€á€»á€‰á€ºá€¸ â€” SARSA á€”á€¾á€„á€·á€º SARSA(Î») á á€™á€­á€á€ºá€†á€€á€º

1994 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Gavin Rummery** á€”á€¾á€„á€·á€º **Mahesan Niranjan** á€á€­á€¯á€·á€€ "Online Q-Learning Using Connectionist Systems" á€†á€­á€¯á€á€²á€· paper á€á€…á€ºá€•á€¯á€’á€ºá€‘á€¯á€á€ºá€á€±á€á€²á€·á€•á€¼á€®á€¸ "Modified Connectionist Q-Learning" á€œá€­á€¯á€· á€¡á€™á€Šá€ºá€•á€±á€¸á€á€²á€·á€•á€«á€á€šá€ºá‹ 1996 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Singh** á€”á€¾á€„á€·á€º **Sutton** á€á€­á€¯á€·á€€ á€’á€® algorithm á€€á€­á€¯ **SARSA** á€œá€­á€¯á€· á€¡á€™á€Šá€ºá€•á€¼á€±á€¬á€„á€ºá€¸á€á€²á€·á€•á€«á€á€šá€º â€” algorithm á€€ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€²á€· quintuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

á€•á€»á€±á€¬á€ºá€…á€›á€¬á€€á€á€±á€¬á€· 1995 á€á€¯á€”á€¾á€…á€º PhD thesis "Problem Solving with Reinforcement Learning" á€™á€¾á€¬ Gavin á€€ Sutton á€€á€­á€¯ "Modified Q-Learning" á€¡á€™á€Šá€ºá€€á€­á€¯ á€†á€€á€ºá€á€¯á€¶á€¸á€”á€±á€á€²á€·á€¡á€á€½á€€á€º á€á€±á€¬á€„á€ºá€¸á€•á€”á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€™á€¾á€¬ SARSA á€†á€­á€¯á€á€²á€· á€¡á€™á€Šá€ºá€•á€² RL community á€™á€¾á€¬ á€€á€»á€”á€ºá€›á€¾á€­á€á€²á€·á€•á€«á€á€šá€ºá‹ Gavin á€›á€²á€· thesis á€€á€œá€Šá€ºá€¸ SARSA(Î») á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹

PhD á€•á€¼á€®á€¸á€œá€­á€¯á€· Gavin á€€ **Tomb Raider** game series á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€á€²á€· company á€™á€¾á€¬ programmer/lead programmer á€¡á€–á€¼á€…á€º á€¡á€±á€¬á€„á€ºá€™á€¼á€„á€ºá€…á€½á€¬ á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ Mahesan á€€á€á€±á€¬á€· academic career á€€á€­á€¯ á€†á€€á€ºá€œá€€á€ºá€œá€»á€¾á€±á€¬á€€á€ºá€œá€¾á€™á€ºá€¸á€á€²á€·á€•á€«á€á€šá€ºá‹

---

### Control Problem á€¡á€á€½á€€á€º Accumulating Trace á€€á€­á€¯ Adapt á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

Accumulating trace á€€á€­á€¯ control problem á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€”á€º adapt á€œá€¯á€•á€ºá€›á€¬á€á€½á€„á€º á€œá€­á€¯á€¡á€•á€ºá€á€²á€· á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯ á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€€á€á€±á€¬á€· â€”visited **states** á€¡á€…á€¬á€¸ visited **state-action pairs** á€€á€­á€¯ track á€œá€¯á€•á€ºá€›á€™á€šá€ºá€†á€­á€¯á€á€¬á€•á€«á‹ Eligibility **vector** (states track á€œá€¯á€•á€º) á€¡á€…á€¬á€¸ eligibility **matrix** (state-action pairs track á€œá€¯á€•á€º) á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹

### Replacing Trace Mechanism

Replacing trace mechanism á€€á€œá€Šá€ºá€¸ á€›á€­á€¯á€¸á€›á€­á€¯á€¸á€á€”á€ºá€¸á€á€”á€ºá€¸á€•á€«á€•á€²á‹ Eligibility traces á€€á€­á€¯ **maximum value 1** á€‘á€­á€á€¬ clip á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€•á€« â€” boundlessly accumulate á€œá€¯á€•á€ºá€™á€Šá€·á€ºá€¡á€…á€¬á€¸ traces á€€á€­á€¯ 1 á€‘á€­á€á€¬ grow á€á€½á€„á€·á€ºá€•á€¼á€¯á€•á€«á€á€šá€ºá‹ á€’á€® strategy á€›á€²á€· á€¡á€¬á€¸á€á€¬á€á€»á€€á€ºá€€á€á€±á€¬á€· agent á€€ loop á€‘á€² á€•á€­á€á€ºá€™á€­á€”á€±á€›á€„á€ºá€á€±á€¬á€„á€º traces á€€ proportion á€•á€¼á€„á€ºá€•á€€á€­á€¯ grow á€™á€á€½á€¬á€¸á€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Replacing trace strategy á€™á€¾á€¬ state-action pair á€€á€­á€¯ visit á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€« trace á€€á€­á€¯ 1 á€á€­á€¯á€· set á€•á€¼á€®á€¸ã€Î» value á€¡á€•á€±á€«á€º á€¡á€á€¼á€±á€á€¶á decay á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

### á€á€™á€­á€¯á€„á€ºá€¸á€¡á€€á€»á€‰á€ºá€¸ â€” Eligibility Trace Mechanism á á€™á€­á€á€ºá€†á€€á€º

Eligibility trace mechanism á á€šá€±á€˜á€¯á€šá€» idea á€€á€­á€¯ **A. Harry Klopf** á€€ 1972 á€á€¯á€”á€¾á€…á€º paper "Brain Function and Adaptive Systemsâ€”A Heterostatic Theory" á€™á€¾á€¬ á€–á€±á€¬á€ºá€•á€¼á€á€²á€·á€•á€«á€á€šá€ºá‹ Synapses á€™á€»á€¬á€¸ reinforcing events á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º changes á€™á€»á€¬á€¸ á€á€¶á€šá€°á€›á€”á€º "eligible" á€–á€¼á€…á€ºá€œá€¬á€™á€šá€ºá€Ÿá€¯ á€šá€°á€†á€á€²á€·á€•á€«á€á€šá€ºá‹

RL context á€™á€¾á€¬á€á€±á€¬á€· **Richard Sutton** á 1984 PhD thesis á€€ eligibility traces mechanism á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º â€” **conventional accumulating trace** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

**Replacing trace** á€€á€­á€¯á€á€±á€¬á€· **Satinder Singh** á€”á€¾á€„á€·á€º **Richard Sutton** á€á€­á€¯á€·á€€ 1996 á€á€¯á€”á€¾á€…á€º paper "Reinforcement Learning with Replacing Eligibility Traces" á€™á€¾á€¬ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ á€á€°á€á€­á€¯á€· á€á€½á€±á€·á€›á€¾á€­á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. Replacing trace á€€ accumulating trace á€‘á€€á€º **á€•á€­á€¯á€™á€¼á€”á€ºá€•á€¼á€®á€¸ á€•á€­á€¯á€šá€¯á€¶á€€á€¼á€Šá€ºá€›** á€á€²á€· learning á€•á€±á€¸á€á€šá€º
2. Accumulating trace á€€ **biased** á€–á€¼á€…á€ºá€•á€¼á€®á€¸ replacing trace á€€ **unbiased** á€–á€¼á€…á€ºá€á€šá€º
3. TD(1) with replacing traces â‰ˆ **first-visit MC** áŠ TD(1) with accumulating traces â‰ˆ **every-visit MC**
4. Offline version of replace-trace TD(1) = first-visit MC (identical!)

---

## á‡.á„ â€” Accumulating Traces vs. Replacing Traces (Frequency á€”á€¾á€„á€·á€º Recency Heuristics)

### Accumulating Trace á Frequency + Recency Heuristic

Accumulating trace á€á€Šá€º **frequency heuristic** á€”á€¾á€„á€·á€º **recency heuristic** á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Agent á€€ state-action pair á€á€…á€ºá€á€¯á€€á€­á€¯ try á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€« trace á€€á€­á€¯ 1 á€‘á€•á€ºá€•á€±á€«á€„á€ºá€¸á€•á€«á€á€šá€ºá‹ Environment á€™á€¾á€¬ loop á€›á€¾á€­á€•á€¼á€®á€¸ agent á€€ state-action pair á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€€á€­á€¯ á€¡á€€á€¼á€­á€™á€ºá€€á€¼á€­á€™á€º try á€–á€¼á€…á€ºá€›á€„á€ºá€›á€±á€¬? á€’á€® state-action pair á€€á€­á€¯ future rewards á€¡á€á€½á€€á€º **á€•á€­á€¯á€á€¬á€á€”á€ºá€›á€¾á€­** á€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€á€„á€·á€ºá€á€œá€¬á€¸ã€á€’á€«á€™á€Ÿá€¯á€á€º **á€›á€­á€¯á€¸á€›á€­á€¯á€¸á€á€¬á€á€”á€ºá€›á€¾á€­** á€›á€¯á€¶á€œá€±á€¬á€€á€ºá€•á€² á€–á€¼á€…á€ºá€á€„á€·á€ºá€á€œá€¬á€¸?

- **Accumulating traces** â€” trace values 1 á€‘á€€á€º á€€á€¼á€®á€¸á€á€½á€„á€·á€ºá€•á€¼á€¯á€á€šá€º
- **Replacing traces** â€” trace values 1 á€‘á€€á€º á€€á€¼á€®á€¸á€á€½á€„á€·á€ºá€™á€•á€¼á€¯á€˜á€°á€¸

### SWS Environment á€á€½á€„á€º Accumulating Traces á á€¥á€•á€™á€¬

$\gamma = 0.9$, $\lambda = 0.5$, TD error $= 1$, $\alpha = 0.1$ á€–á€¼á€„á€·á€º:

Agent á trajectory á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€€á€¼á€™á€šá€º:

1. Agent á€€ action á€šá€°á€•á€¼á€®á€¸ state-action pair á€›á€²á€· trace á€€á€­á€¯ 1 á€á€­á€¯á€· set
2. á€”á€±á€¬á€€á€º step á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ trace á€€á€­á€¯ $\gamma \lambda = 0.9 \times 0.5 = 0.45$ á€–á€¼á€„á€·á€º decay
3. Same state á€€á€­á€¯ á€•á€¼á€”á€ºá€›á€±á€¬á€€á€ºá€›á€„á€º trace á€€á€­á€¯ **+1 á€‘á€•á€ºá€•á€±á€«á€„á€ºá€¸** (accumulating) â€” trace value > 1 á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º
4. Episode á€¡á€†á€¯á€¶á€¸á€™á€¾á€¬ reward á€á€½á€±á€·á€›á€„á€º Q-function á€€á€­á€¯ traces á€–á€¼á€„á€·á€º update

**Accumulating trace á€–á€¼á€„á€·á€º Q update:**

$$Q(s,a) \leftarrow Q(s,a) + \alpha \times \delta_t \times E_t(s,a)$$

Trace values: $0.0041, 0.00911, 0.0203, 0.045, 0.1$ (trace á€€á€¼á€®á€¸á€œá€± Q update á€•á€­á€¯á€™á€»á€¬á€¸á€œá€±)

### SWS Environment á€á€½á€„á€º Replacing Traces á á€¥á€•á€™á€¬

Replacing traces á€™á€¾á€¬á€á€±á€¬á€· same state á€€á€­á€¯ á€•á€¼á€”á€ºá€›á€±á€¬á€€á€ºá€›á€„á€º trace á€€á€­á€¯ 1 á€á€­á€¯á€· **replace** á€œá€¯á€•á€ºá€•á€«á€á€šá€º (accumulate á€™á€œá€¯á€•á€º)á‹ Same state á€›á€²á€· á€á€á€¼á€¬á€¸ actions á€€á€­á€¯ **0 á€á€­á€¯á€· reset** á€œá€¯á€•á€ºá€•á€¼á€®á€¸ current action á€€á€­á€¯ 1 á€á€­á€¯á€· set á€•á€«á€á€šá€ºá‹

**á€›á€œá€’á€º á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º:**

| Feature | Accumulating Traces | Replacing Traces |
|---|---|---|
| **Multiple visits** | Trace value > 1 á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º | Trace value â‰¤ 1 á€¡á€™á€¼á€² |
| **Heuristic** | Frequency + Recency | Recency dominant |
| **Loop environments** | Frequent states á€€á€­á€¯ over-credit | á€•á€­á€¯á€™á€»á€¾á€á€²á€· credit |
| **á€˜á€šá€ºá€¡á€á€« á€á€¯á€¶á€¸á€›á€™á€œá€²** | Frequency á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€›á€„á€º | Loops/repeated visits á€›á€¾á€­á€›á€„á€º |

---

### Eligibility Trace Update Equations

**Accumulating Trace:**

$$E_t(s, a) = \gamma \lambda \, E_{t-1}(s, a) + \mathbb{1}[s = S_t, a = A_t]$$

state-action pair $(S_t, A_t)$ á€€á€­á€¯ visit á€œá€¯á€•á€ºá€›á€„á€º trace á€€á€­á€¯ 1 á€‘á€•á€ºá€•á€±á€«á€„á€ºá€¸áŠ á€™á€Ÿá€¯á€á€ºá€›á€„á€º decay á€•á€² á€–á€¼á€…á€º

**Replacing Trace:**

$$E_t(s, a) = \begin{cases} 1 & \text{if } s = S_t, a = A_t \\ 0 & \text{if } s = S_t, a \neq A_t \\ \gamma \lambda \, E_{t-1}(s,a) & \text{otherwise} \end{cases}$$

Visit á€œá€¯á€•á€ºá€›á€„á€º trace á€€á€­á€¯ 1 á€á€­á€¯á€· set (accumulate á€™á€œá€¯á€•á€º)ã€same state á€›á€²á€· á€á€á€¼á€¬á€¸ actions á€€á€­á€¯ 0 á€á€­á€¯á€· reset

**Q-function Update (state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º):**

$$Q(s, a) \leftarrow Q(s, a) + \alpha \, \delta_t \, E_t(s, a), \quad \forall s, a$$

TD error: $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$

> ğŸ’¡ Q-table á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ update á€œá€¯á€•á€ºá€”á€±á€•á€±á€™á€šá€·á€ºã€E matrix á€™á€¾á€¬ eligible pairs á€á€½á€±á€€á€á€¬ 0 á€‘á€€á€ºá€€á€¼á€®á€¸á€á€²á€· values á€›á€¾á€­á€•á€«á€á€šá€ºá‹ á€€á€»á€”á€ºá€á€¬á€á€½á€±á€€ 0 á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º update effect á€™á€›á€¾á€­á€•á€«á‹

---

### Python Code â€” SARSA(Î») Agent (1/2)

```python
def sarsa_lambda(env,
                 gamma=1.0,
                 init_alpha=0.5,
                 min_alpha=0.01,
                 alpha_decay_ratio=0.5,
                 init_epsilon=1.0,
                 min_epsilon=0.1,
                 epsilon_decay_ratio=0.9,
                 lambda_=0.5,              # Î» hyperparameter
                 replacing_traces=True,     # replacing/accumulating traces á€›á€½á€±á€¸á€á€»á€šá€º
                 n_episodes=3000):
    nS, nA = env.observation_space.n, env.action_space.n
    pi_track = []
    Q = np.zeros((nS, nA), dtype=np.float64)           # Q-function initialize
    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)
    E = np.zeros((nS, nA), dtype=np.float64)            # Eligibility traces matrix
    
    # Îµ-greedy action selection function
    select_action = lambda state, Q, epsilon: \
        np.argmax(Q[state]) \
        if np.random.random() > epsilon \
        else np.random.randint(len(Q[state]))
    
    # Alpha á€”á€¾á€„á€·á€º epsilon decay schedules
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    epsilons = decay_schedule(
        init_epsilon, min_epsilon, 
        epsilon_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 2/2 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. SARSA(Î») agent á€á€Šá€º SARSA + TD(Î») methods á€›á€²á€· á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€»á€€á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º
2. `lambda_` â€” Python á€™á€¾á€¬ `lambda` á€€ reserved word á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º `_` á€‘á€Šá€·á€ºá€‘á€¬á€¸á€á€šá€º
3. `replacing_traces` â€” True á€†á€­á€¯á€›á€„á€º replacing traces, False á€†á€­á€¯á€›á€„á€º accumulating traces á€á€¯á€¶á€¸á€™á€šá€º
4. `E` matrix â€” state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á eligibility á€€á€­á€¯ track á€œá€¯á€•á€ºá€•á€«á€á€šá€º

---

### Python Code â€” SARSA(Î») Agent (2/2)

```python
    for e in tqdm(range(n_episodes), leave=False):
        E.fill(0)  # Episode á€¡á€á€…á€ºá€á€­á€¯á€„á€ºá€¸á€á€½á€„á€º eligibility traces á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ 0 á€á€­á€¯á€· reset
        state, done = env.reset(), False
        action = select_action(state, Q, epsilons[e])  # á€•á€‘á€™ action á€›á€½á€±á€¸á€á€»á€šá€º
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            
            # Next action á€›á€½á€±á€¸á€á€»á€šá€º (SARSA-style: on-policy)
            next_action = select_action(next_state, Q, epsilons[e])
            
            # TD target á€”á€¾á€„á€·á€º TD error á€á€½á€€á€ºá€á€»á€€á€º
            td_target = reward + gamma * Q[next_state][next_action] * (not done)
            td_error = td_target - Q[state][action]
            
            # Eligibility trace update
            E[state][action] = E[state][action] + 1     # current pair á trace á€€á€­á€¯ increment
            if replacing_traces: E.clip(0, 1, out=E)     # replacing á€†á€­á€¯á€›á€„á€º 1 á€‘á€­á€á€¬ clip
            
            # Q-function update â€” eligible pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º update!
            Q = Q + alphas[e] * td_error * E
            
            # Traces decay
            E = gamma * lambda_ * E
            
            # Variables update
            state, action = next_state, next_action
        
        Q_track[e] = Q
        pi_track.append(np.argmax(Q, axis=1))
    
    V = np.max(Q, axis=1)
    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return Q, V, pi, Q_track, pi_track
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Episode á€¡á€á€…á€ºá€á€­á€¯á€„á€ºá€¸á€á€½á€„á€º `E.fill(0)` á€–á€¼á€„á€·á€º traces á€€á€­á€¯ reset
2. á€•á€‘á€™ action á€€á€­á€¯ interaction loop á€™á€á€­á€¯á€„á€ºá€á€„á€º select (SARSA pattern)
3. `next_action` á€€á€­á€¯ Îµ-greedy á€–á€¼á€„á€·á€º select â€” á€’á€«á€€ SARSA á€›á€²á€· on-policy characteristic
4. `E[state][action] += 1` â€” current state-action pair á€€á€­á€¯ eligible á€¡á€–á€¼á€…á€º mark
5. `Q = Q + alphas[e] * td_error * E` â€” **key line!** Q-table á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ update á€•á€±á€™á€šá€·á€º E=0 á€–á€¼á€…á€ºá€á€²á€· pairs á€á€½á€±á€€á€­á€¯ effect á€™á€›á€¾á€­
6. `E = gamma * lambda_ * E` â€” traces á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ decay

---

## á‡.á… â€” Miguel á Analogy â€” Accumulating á€”á€¾á€„á€·á€º Replacing TracesáŠ Gluten-free á€”á€¾á€„á€·á€º Banana-free Diet

> Miguel á á€á€™á€®á€¸á€€ á€Šá€á€­á€¯á€„á€ºá€¸ á€¡á€­á€•á€ºá€™á€•á€»á€±á€¬á€ºá€˜á€² á€„á€­á€¯á€”á€±á€•á€«á€á€šá€ºá‹ Miguel á€”á€¾á€„á€·á€º á€‡á€”á€®á€¸á€€ "credit assignment" á€œá€¯á€•á€ºá€–á€­á€¯á€· á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€á€²á€·á€•á€«á€á€šá€º â€” á€˜á€¬á€€ á€Šá€¡á€­á€•á€ºá€•á€»á€€á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€–á€¼á€…á€ºá€…á€±á€á€œá€²?
>
> á€á€™á€®á€¸á€€ á€á€…á€ºá€”á€±á€·á€á€¬á€œá€¯á€¶á€¸ **gluten á€•á€«á€á€²á€· carbs** (cereal, pasta, crackers, bread) á€€á€­á€¯ á€¡á€€á€¼á€­á€™á€ºá€€á€¼á€­á€™á€º á€…á€¬á€¸á€•á€¼á€®á€¸ã€á€¡á€­á€•á€ºá€á€»á€­á€”á€ºá€”á€®á€¸á€™á€¾á€¬ **á€„á€¾á€€á€ºá€•á€»á€±á€¬á€á€®á€¸** á€€á€­á€¯ snack á€¡á€–á€¼á€…á€º á€…á€¬á€¸á€•á€«á€á€šá€ºá‹
>
> Miguel á á€¦á€¸á€”á€¾á€±á€¬á€€á€ºá€‘á€²á€€ **"accumulating trace"** á€€ gluten á€€á€­á€¯ á€Šá€½á€¾á€”á€ºá€•á€¼á€á€²á€·á€á€šá€º â€” "á€Ÿá€¯á€á€ºá€•á€«á€•á€¼á€®! Gluten á€€á€­á€¯ á€á€…á€ºá€”á€±á€·á€œá€¯á€¶á€¸ á€¡á€€á€¼á€­á€™á€ºá€€á€¼á€­á€™á€º á€…á€¬á€¸á€á€²á€·á€¡á€á€½á€€á€º frequency á€¡á€› gluten á€€á€•á€² á€á€¬á€á€”á€ºá€›á€¾á€­á€á€šá€º!" Gluten á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€á€²á€·á€•á€±á€™á€šá€·á€º á€•á€¼á€¿á€”á€¬á€€ **á€¡á€•á€¼á€Šá€·á€ºá€¡á€ á€•á€»á€±á€¬á€€á€ºá€™á€á€½á€¬á€¸**á€•á€«á‹
>
> á€”á€±á€¬á€€á€ºá€†á€¯á€¶á€¸á€™á€¾á€¬ á€‡á€”á€®á€¸á€€ á€„á€šá€ºá€…á€‰á€ºá€€ á€Šá€˜á€€á€º **á€„á€¾á€€á€ºá€•á€»á€±á€¬á€á€®á€¸** á€…á€¬á€¸á€›á€„á€º á€•á€¼á€¿á€”á€¬á€›á€¾á€­á€á€²á€·á€€á€¼á€±á€¬á€„á€ºá€¸ á€á€á€­á€›á€á€²á€·á€•á€«á€á€šá€ºá‹ á€„á€¾á€€á€ºá€•á€»á€±á€¬á€á€®á€¸á€€á€­á€¯ á€–á€šá€ºá€›á€¾á€¬á€¸á€á€¬á€”á€²á€· á€•á€¼á€¿á€”á€¬ **á€¡á€•á€¼á€Šá€·á€ºá€¡á€ á€•á€»á€±á€¬á€€á€ºá€á€½á€¬á€¸**á€•á€«á€á€šá€º!
>
> **"Replacing trace"** á€€á€­á€¯ á€á€¯á€¶á€¸á€á€²á€·á€›á€„á€ºã€gluten á€€á€­á€¯ á€¡á€€á€¼á€­á€™á€ºá€€á€¼á€­á€™á€º á€…á€¬á€¸á€á€²á€·á€•á€±á€™á€šá€·á€º conservative amount á€‘á€€á€º blame á€™á€•á€­á€¯á€•á€±á€¸á€á€²á€·á€˜á€°á€¸á€™á€œá€¬á€¸ã€‚á€„á€¾á€€á€ºá€•á€»á€±á€¬á€á€®á€¸á **recency** (á€¡á€­á€•á€ºá€á€»á€­á€”á€ºá€”á€®á€¸á€”á€®á€¸á€™á€¾á€¬ á€…á€¬á€¸á€á€¼á€„á€ºá€¸) á€€á€­á€¯ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€…á€½á€¬ á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€á€²á€·á€™á€œá€¬á€¸?

**á€á€„á€ºá€á€”á€ºá€¸á€…á€¬:**

- **Accumulating traces** â€” frequency á€€á€­á€¯ exaggerate á€œá€¯á€•á€ºá€á€á€ºá€•á€¼á€®á€¸ frequent events á€€á€­á€¯ over-blame
- **Replacing traces** â€” frequent events á€€á€­á€¯ moderate blame á€•á€±á€¸á€•á€¼á€®á€¸ rare but recent events á€€á€­á€¯ surface á€•á€±á€¸á€á€á€º

```mermaid
graph TD
    subgraph ACC["Accumulating Traces"]
        A1["E(s,a) += 1 every visit"]
        A2["Trace value > 1 á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º"]
        A3["Frequency heuristic:<br/>gluten á€€á€­á€¯ over-blame"]
    end
    
    subgraph REP["Replacing Traces"]
        R1["E(s,a) = min(E+1, 1)"]
        R2["Trace value â‰¤ 1 á€¡á€™á€¼á€²"]
        R3["Recency heuristic:<br/>á€„á€¾á€€á€ºá€•á€»á€±á€¬á€á€®á€¸ á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€º"]
    end
    
    style A1 fill:#ff922b,color:#fff
    style R1 fill:#4CAF50,color:#fff
```

> ğŸ’¡ á€€á€­á€›á€­á€šá€¬á€á€½á€±á€€á€­á€¯ á€á€­á€‘á€¬á€¸á€•á€¼á€®á€¸ á€•á€‘á€™á€†á€¯á€¶á€¸ á€€á€¼á€Šá€·á€ºá€›á€¾á€¯á€™á€¾á€¯á€–á€¼á€„á€·á€º dismiss á€™á€œá€¯á€•á€ºá€•á€«á€”á€²á€·á‹ á€›á€”á€­á€¯á€„á€ºá€á€²á€· options á€á€½á€±á€€á€­á€¯ á€•á€¼á€”á€±á€á€¬ á€–á€¼á€…á€ºá€•á€¼á€®á€¸ã€á€™á€¾á€”á€ºá€€á€”á€ºá€á€²á€· á€€á€­á€›á€­á€šá€¬á€€á€­á€¯ á€›á€½á€±á€¸á€á€¯á€¶á€¸á€á€¬á€€ á€á€„á€·á€ºá€›á€²á€· á€á€¬á€á€”á€ºá€•á€«á‹

---

## á‡.á† â€” Watkins's Q(Î»): Behavior á€™á€¾ Learning á€€á€­á€¯ á€•á€¼á€”á€ºá€á€½á€²á€á€¼á€„á€ºá€¸ (Off-policy Control with Traces)

### Q(Î») á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²

Î» algorithms á **off-policy control** version á€›á€¾á€­á€•á€«á€á€šá€ºá‹ Q(Î») á€á€Šá€º Q-learning á€€á€­á€¯ Î»-return á€–á€¼á€„á€·á€º extend á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€¬á€–á€¼á€…á€ºá€•á€¼á€®á€¸ GPI pattern á€›á€²á€· policy-evaluation requirements á€¡á€á€½á€€á€º off-policy control TD target (next state á€™á€¾á€¬ max action á€šá€°á€á€²á€·) á€€á€­á€¯ Î»-return á€–á€¼á€„á€·á€º á€¡á€…á€¬á€¸á€‘á€­á€¯á€¸á€•á€«á€á€šá€ºá‹

Q-learning á€€á€­á€¯ eligibility traces á€”á€¾á€„á€·á€º extend á€œá€¯á€•á€ºá€›á€”á€º á€”á€Šá€ºá€¸á€œá€™á€ºá€¸ á‚ á€™á€»á€­á€¯á€¸á€›á€¾á€­á€•á€±á€™á€šá€·á€ºã€á€’á€®á€™á€¾á€¬ original version á€–á€¼á€…á€ºá€á€²á€· **Watkins's Q(Î»)** á€€á€­á€¯á€á€¬ introduce á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

### SARSA(Î») á€”á€¾á€„á€·á€º Q(Î») á Key Difference

```mermaid
graph TD
    subgraph SL["SARSA(Î»)"]
        SL1["On-policy"]
        SL2["Traces always decay:<br/>E = Î³Î»E"]
        SL3["Îµ-greedy policy á€€á€­á€¯ follow"]
    end
    
    subgraph QL["Watkins's Q(Î»)"]
        QL1["Off-policy"]
        QL2["Traces conditional:<br/>Greedy action â†’ E = Î³Î»E<br/>Exploratory â†’ E = 0 (reset!)"]
        QL3["Greedy policy á€€á€­á€¯ learn"]
    end
    
    style SL1 fill:#2196F3,color:#fff
    style QL1 fill:#4CAF50,color:#fff
```

**Q(Î») á á€¡á€“á€­á€€ á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º:** Off-policy method á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º eligibility traces á€€á€­á€¯ **attention á€–á€¼á€„á€·á€º** manage á€œá€¯á€•á€ºá€›á€•á€«á€á€šá€ºá‹ Greedy policy á€€á€­á€¯ learn á€”á€±á€á€¬á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º exploratory policy á€€á€­á€¯ follow á€”á€±á€á€²á€·á€¡á€á€½á€€á€º:

- **Next action á€€ greedy á€–á€¼á€…á€ºá€›á€„á€º** â†’ traces á€€á€­á€¯ á€•á€¯á€¶á€™á€¾á€”á€ºá€¡á€á€­á€¯á€„á€ºá€¸ decay: $E = \gamma \lambda E$
- **Next action á€€ exploratory á€–á€¼á€…á€ºá€›á€„á€º** â†’ traces **á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ reset**: $E = 0$

---

### á€á€™á€­á€¯á€„á€ºá€¸á€¡á€€á€»á€‰á€ºá€¸ â€” Q-learning á€”á€¾á€„á€·á€º Q(Î») á á€™á€­á€á€ºá€†á€€á€º

1989 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Chris Watkins** á€€ PhD thesis "Learning from Delayed Rewards" á€™á€¾á€¬ Q-learning á€”á€¾á€„á€·á€º Q(Î») methods á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ RL á current theory á€–á€½á€¶á€·á€–á€¼á€­á€¯á€¸á€›á€±á€¸á€¡á€á€½á€€á€º foundational á€–á€¼á€…á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹

Q-learning á€á€Šá€º á€šá€”á€±á€·á€á€­á€¯á€„á€º **á€¡á€›á€±á€•á€”á€ºá€¸á€¡á€†á€¯á€¶á€¸** RL algorithms á€‘á€² á€•á€«á€á€„á€ºá€•á€«á€á€šá€º â€” á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€¼á€®á€¸ á€¡á€œá€¯á€•á€ºá€€á€±á€¬á€„á€ºá€¸á€€á€±á€¬á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Q(Î») á€€á€­á€¯ **Watkins's Q(Î»)** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºáŠ á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬ Jing Peng á€”á€¾á€„á€·á€º Ronald Williams á version (**Peng's Q(Î»)**) á€€á€œá€Šá€ºá€¸ á€›á€¾á€­á€á€²á€·á€¡á€á€½á€€á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

1992 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º Chris á€”á€¾á€„á€·á€º **Peter Dayan** á€á€­á€¯á€·á€€ Q-learning convergence theorem á€€á€­á€¯ prove á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º â€” state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ repeatedly sample á€œá€¯á€•á€ºá€•á€¼á€®á€¸ discretely represent á€œá€¯á€•á€ºá€‘á€¬á€¸á€›á€„á€º Q-learning á€á€Šá€º probability 1 á€–á€¼á€„á€·á€º optimal action-value function á€†á€® converge á€–á€¼á€…á€ºá€€á€¼á€±á€¬á€„á€ºá€¸ á€•á€¼á€á€²á€·á€•á€«á€á€šá€ºá‹

Peter Dayan á€€á€á€±á€¬á€· Geoff Hinton á€›á€²á€· lab á€™á€¾á€¬ postdoc á€œá€¯á€•á€ºá€á€²á€·á€•á€¼á€®á€¸ã€DeepMind co-founder **Demis Hassabis** á€›á€²á€· postdoc advisor á€–á€¼á€…á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ 2018 á€€á€á€Šá€ºá€¸á€€ **Fellow of the Royal Society** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

### Q(Î») Trace Reset Logic

$$E_t = \begin{cases} \gamma \lambda \, E_{t-1} & \text{if } A_{t+1} = \arg\max_a Q(S_{t+1}, a) \text{ (greedy action)} \\ 0 & \text{otherwise (exploratory action)} \end{cases}$$

> ğŸ’¡ **á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º exploratory action á€šá€°á€›á€„á€º traces á€€á€­á€¯ reset á€œá€¯á€•á€ºá€›á€á€œá€²?** Off-policy learning á€™á€¾á€¬ greedy policy á€€á€­á€¯ learn á€”á€±á€á€¬á€–á€¼á€…á€ºá€•á€¼á€®á€¸ã€exploratory action á€šá€°á€œá€­á€¯á€€á€ºá€›á€„á€º greedy trajectory á€€á€”á€± á€‘á€½á€€á€ºá€á€½á€¬á€¸á€á€¬á€™á€­á€¯á€· á€šá€á€„á€º trace á€‘á€¬á€¸á€á€²á€·á€á€²á€· state-action pairs á€›á€²á€· eligibility (validity) á€€á€»á€á€½á€¬á€¸á€•á€«á€á€šá€ºá‹ Greedy policy á€›á€²á€· trajectory á€™á€Ÿá€¯á€á€ºá€á€±á€¬á€·á€á€²á€· experience á€€á€­á€¯á€á€¯á€¶á€¸á€•á€¼á€®á€¸ greedy policy á€€á€­á€¯ update á€œá€¯á€•á€ºá€›á€„á€º á€Šá€¶á€·á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

### Python Code â€” Watkins's Q(Î») Agent (1/3)

```python
def q_lambda(env,
             gamma=1.0,
             init_alpha=0.5,
             min_alpha=0.01,
             alpha_decay_ratio=0.5,
             init_epsilon=1.0,
             min_epsilon=0.1,
             epsilon_decay_ratio=0.9,
             lambda_=0.5,
             replacing_traces=True,
             n_episodes=3000):
    nS, nA = env.observation_space.n, env.action_space.n
    pi_track = []
    Q = np.zeros((nS, nA), dtype=np.float64)            # Q-table
    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)
    E = np.zeros((nS, nA), dtype=np.float64)             # Eligibility traces matrix
    
    select_action = lambda state, Q, epsilon: \
        np.argmax(Q[state]) \
        if np.random.random() > epsilon \
        else np.random.randint(len(Q[state]))
    
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    epsilons = decay_schedule(
        init_epsilon, min_epsilon, 
        epsilon_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 2/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Q(Î») agent á€á€Šá€º Q-learning + TD(Î») methods á€›á€²á€· á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€»á€€á€º
2. Structure á€€ SARSA(Î») á€”á€¾á€„á€·á€º á€¡á€á€±á€¬á€ºá€†á€„á€ºá€á€° â€” á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€ trace management logic

---

### Python Code â€” Watkins's Q(Î») Agent (2/3)

```python
    for e in tqdm(range(n_episodes), leave=False):
        E.fill(0)                                        # Traces reset
        state, done = env.reset(), False
        action = select_action(state, Q, epsilons[e])    # á€•á€‘á€™ action select
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            
            # SARSA-style: next action á€€á€­á€¯ Îµ-greedy á€–á€¼á€„á€·á€º select
            next_action = select_action(next_state, Q, epsilons[e])
            
            # Next action á€€ greedy á€–á€¼á€…á€ºá€™á€–á€¼á€…á€º á€…á€…á€ºá€†á€±á€¸
            next_action_is_greedy = \
                Q[next_state][next_action] == Q[next_state].max()
            
            # TD target â€” Q-learning style (max á€á€¯á€¶á€¸, off-policy!)
            td_target = reward + gamma * Q[next_state].max() * (not done)
            td_error = td_target - Q[state][action]
            
            # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 3/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Q-learning á€€á€œá€­á€¯á€•á€² action á€€á€­á€¯ á€€á€¼á€­á€¯á€á€„á€º select á€•á€«á€á€šá€º â€” á€’á€«á€•á€±á€™á€šá€·á€º SARSA á€™á€¾á€¬ á€Ÿá€¯á€á€ºá€•á€¼á€®á€¸ Q-learning á€™á€¾á€¬ á€’á€®á€œá€­á€¯ á€€á€¼á€­á€¯á€™ select á€á€²á€·á€˜á€°á€¸á‹ **á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º?** Next action á€€ greedy á€–á€¼á€…á€ºá€™á€–á€¼á€…á€º á€…á€…á€ºá€–á€­á€¯á€· á€œá€­á€¯á€á€²á€·á€¡á€á€½á€€á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º!
2. `next_action_is_greedy` â€” next action á€€ greedy policy á€€á€”á€± á€–á€¼á€…á€ºá€á€œá€¬á€¸ á€…á€…á€ºá€†á€±á€¸
3. TD target á€€ Q-learning style â€” `max` á€á€¯á€¶á€¸á€á€šá€º (off-policy)

---

### Python Code â€” Watkins's Q(Î») Agent (3/3)

```python
            td_error = td_target - Q[state][action]
            
            # Replacing trace approach: current state á€›á€²á€· actions á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ 0 á€á€­á€¯á€· reset
            # á€•á€¼á€®á€¸á€™á€¾ current action á€€á€­á€¯ increment
            if replacing_traces: E[state].fill(0)
            E[state][action] = E[state][action] + 1
            
            # Q-function update â€” eligible pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸
            Q = Q + alphas[e] * td_error * E
            
            # Conditional trace decay â€” KEY DIFFERENCE!
            if next_action_is_greedy:
                E = gamma * lambda_ * E     # Greedy action: á€•á€¯á€¶á€™á€¾á€”á€º decay
            else:
                E.fill(0)                    # Exploratory action: traces á€¡á€¬á€¸á€œá€¯á€¶á€¸ reset!
            
            state, action = next_state, next_action
        
        Q_track[e] = Q
        pi_track.append(np.argmax(Q, axis=1))
    
    V = np.max(Q, axis=1)
    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return Q, V, pi, Q_track, pi_track
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. **Replacing traces approach** â€” current state á€›á€²á€· action values á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ zero-out á€•á€¼á€®á€¸á€™á€¾ current action á€€á€­á€¯ increment
2. `Q = Q + alphas[e] * td_error * E` â€” eligibility trace matrix á€€á€­á€¯ error á€”á€¾á€„á€·á€º learning rate á€–á€¼á€„á€·á€º multiply á€•á€¼á€®á€¸ Q-table á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ shift â€” visited states á€¡á€¬á€¸á€œá€¯á€¶á€¸á€†á€® signal drop
3. **Critical line:** next action á€€ greedy á€–á€¼á€…á€ºá€›á€„á€º traces á€€á€­á€¯ á€•á€¯á€¶á€™á€¾á€”á€º decayã€exploratory action á€†á€­á€¯á€›á€„á€º **traces á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ zero**! Greedy policy á€€á€­á€¯ learn á€™á€”á€±á€á€±á€¬á€·á€˜á€² stray á€–á€¼á€…á€ºá€á€½á€¬á€¸á€á€²á€·á€¡á€á€½á€€á€º
4. Step á€¡á€†á€¯á€¶á€¸á€™á€¾á€¬ state, action á€€á€­á€¯ update

---

## á‡.á‡ â€” Interact, Learn, á€”á€¾á€„á€·á€º Plan á€œá€¯á€•á€ºá€á€²á€· Agent á€™á€»á€¬á€¸ (Model-based RL)

### Planning Methods vs. Model-free RL: á€˜á€¬á€€ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€œá€²?

Chapter 3 á€™á€¾á€¬ **Value Iteration (VI)** á€”á€¾á€„á€·á€º **Policy Iteration (PI)** á€†á€­á€¯á€á€²á€· planning algorithms á€á€½á€±á€€á€­á€¯ á€†á€½á€±á€¸á€”á€½á€±á€¸á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€«á€á€½á€±á€€ environment á€›á€²á€· model (MDP) á€€á€­á€¯ á€œá€­á€¯á€¡á€•á€ºá€•á€¼á€®á€¸ optimal policies á€€á€­á€¯ offline calculate á€œá€¯á€•á€ºá€•á€±á€¸á€á€šá€ºá‹ á€šá€á€„á€ºá€¡á€á€”á€ºá€¸á€™á€¾á€¬á€á€±á€¬á€· model-free RL methods (SARSA, Q-learning) á€€á€­á€¯ present á€œá€¯á€•á€ºá€•á€¼á€®á€¸ planning methods á€‘á€€á€º improve á€–á€¼á€…á€ºá€á€šá€ºá€†á€­á€¯á€á€œá€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º á€¡á€™á€¾á€”á€ºá€á€€á€šá€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€œá€¬á€¸?

**Model-free RL á advantage:** MDP á€™á€œá€­á€¯á€¡á€•á€ºá€á€¼á€„á€ºá€¸á‹ Go game ($10^{170}$ possible states) á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º StarCraft II ($10^{1685}$ states) á€€á€­á€¯ MDP á€–á€¼á€„á€·á€º represent á€œá€¯á€•á€ºá€–á€­á€¯á€· á€…á€‰á€ºá€¸á€…á€¬á€¸á€€á€¼á€Šá€·á€ºá€•á€«! MDP á€€á€¼á€­á€¯á€á€„á€ºá€™á€œá€­á€¯á€á€¬ practical benefit á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

**á€’á€«á€•á€±á€™á€šá€·á€º...** MDP á€€á€­á€¯ á€€á€¼á€­á€¯á€á€„á€ºá€™á€œá€­á€¯á€á€¬á€€á€±á€¬á€„á€ºá€¸á€•á€±á€™á€šá€·á€ºã€environment á€”á€¾á€„á€·á€º interact á€œá€¯á€•á€ºá€›á€„á€ºá€¸ model á€€á€­á€¯ **á€á€„á€ºá€šá€°**á€œá€­á€¯á€€á€ºá€›á€„á€ºá€›á€±á€¬? á€œá€°á€á€¬á€¸á€á€½á€± á€’á€®á€œá€­á€¯ á€œá€¯á€•á€ºá€œá€±á€·á€›á€¾á€­á€á€šá€º â€” á€”á€±á€›á€¬á€¡á€á€…á€ºá€€á€­á€¯ á€œá€Šá€ºá€•á€á€ºá€›á€„á€ºá€¸ á€¦á€¸á€”á€¾á€±á€¬á€€á€ºá€‘á€² map á€›á€±á€¸á€†á€½á€²á€•á€¼á€®á€¸ coffee shop á€á€½á€±á€·á€›á€„á€º á€•á€¼á€”á€ºá€œá€¬á€”á€­á€¯á€„á€ºá€á€šá€ºá‹ RL agents á€á€½á€±á€œá€Šá€ºá€¸ á€’á€®á€œá€­á€¯ á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€œá€¬á€¸?

á€’á€® section á€™á€¾á€¬ environment á€”á€¾á€„á€·á€º interact á€œá€¯á€•á€ºá€•á€¼á€®á€¸ (model-free methods á€€á€²á€·á€á€­á€¯á€·) environment models (MDPs) á€€á€­á€¯á€œá€Šá€ºá€¸ interaction á€€á€”á€± **á€á€„á€ºá€šá€°**á€á€²á€· agents á€á€½á€±á€€á€­á€¯ explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Maps/models á€á€„á€ºá€šá€°á€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º agents á€á€½á€±á€€ optimal policies learn á€›á€¬á€á€½á€„á€º experience samples á€•á€­á€¯á€”á€Šá€ºá€¸á€œá€­á€¯á€á€á€ºá€•á€«á€á€šá€ºá‹ á€’á€® methods á€á€½á€±á€€á€­á€¯ **model-based reinforcement learning** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    VF["Value Function"] -->|"1. Action á€›á€½á€±á€¸á€á€»á€šá€º<br/>(Îµ-greedy)"| ACT["Action"]
    ACT -->|"2. Environment á€”á€²á€·<br/>interact"| EXP["Experience<br/>(s, a, r, s')"]
    EXP -->|"3a. Q update<br/>(model-free RL)"| VF
    EXP -->|"3b. Model á€á€„á€ºá€šá€°<br/>(T, R functions)"| MODEL["Environment Model<br/>T_count, R_model"]
    MODEL -->|"4. Simulated<br/>experience"| PLAN["Planning Updates"]
    PLAN -->|"5. Q á€€á€­á€¯ á€‘á€•á€ºá€™á€¶<br/>improve"| VF
    
    style VF fill:#2196F3,color:#fff
    style MODEL fill:#4CAF50,color:#fff
    style PLAN fill:#9C27B0,color:#fff
```

---

### RL á€¡á€á€¯á€¶á€¸á€¡á€”á€¾á€¯á€”á€ºá€¸ â€” Sampling Models vs. Distributional Models

| Model Type | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º |
|---|---|
| **Sampling models** | Environment á€€ probabilities á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€¼á€®á€¸ transition **sample á€á€…á€ºá€á€¯á€á€Šá€ºá€¸** á€‘á€¯á€á€ºá€•á€±á€¸á€á€²á€· modelsá‹ Model á€€á€”á€± transition á€€á€­á€¯ sample á€šá€°á€á€¬á‹ |
| **Distributional models** | Transition á€”á€¾á€„á€·á€º reward functions á€›á€²á€· **probability distribution** á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ á€‘á€¯á€á€ºá€•á€±á€¸á€á€²á€· modelsá‹ |

---

## á‡.áˆ â€” Dyna-Q: Sample Models á€€á€­á€¯ á€á€„á€ºá€šá€°á€á€¼á€„á€ºá€¸

### Dyna-Q Architecture

Planning á€”á€¾á€„á€·á€º model-free methods á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€²á€· á€¡á€‘á€„á€ºá€›á€¾á€¬á€¸á€†á€¯á€¶á€¸ architecture á€á€…á€ºá€á€¯á€€á€­á€¯ **Dyna-Q** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹ Dyna-Q á€á€Šá€º model-free RL method (Q-learning) á€”á€¾á€„á€·á€º planning method (value iteration á€”á€¾á€„á€·á€ºá€†á€„á€ºá€á€°) á€€á€­á€¯ interleave á€œá€¯á€•á€ºá€•á€¼á€®á€¸ã€environment á€€á€”á€± sample á€šá€°á€á€²á€· experiences á€›á€±á€¬ learned model á€€á€”á€± sample á€šá€°á€á€²á€· experiences á€›á€±á€¬ á€”á€¾á€…á€ºá€™á€»á€­á€¯á€¸á€œá€¯á€¶á€¸á€–á€¼á€„á€·á€º action-value function á€€á€­á€¯ improve á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

Dyna-Q á€™á€¾á€¬ **transition function** á€”á€¾á€„á€·á€º **reward function** á€€á€­á€¯ three-dimensional tensors (state, action, next_state á€–á€¼á€„á€·á€º index) á€¡á€–á€¼á€…á€º track á€•á€«á€á€šá€º:

- **Transition tensor** â€” $(s, a, s')$ triplet á€€á€­á€¯ á€˜á€šá€ºá€”á€¾á€…á€ºá€€á€¼á€­á€™á€ºá€á€½á€±á€·á€á€²á€·á€á€œá€² count
- **Reward tensor** â€” $(s, a, s')$ triplet á average reward hold

---

### á€á€™á€­á€¯á€„á€ºá€¸á€¡á€€á€»á€‰á€ºá€¸ â€” Dyna-Q á á€™á€­á€á€ºá€†á€€á€º

Model-based RL ideas á€€á€­á€¯ researchers á€™á€»á€¬á€¸á€…á€½á€¬ credit á€•á€±á€¸á€‘á€¬á€¸á€•á€±á€™á€šá€·á€ºã€Dyna architecture á foundation á€–á€¼á€…á€ºá€á€²á€· papers áƒ á€á€¯ á€›á€¾á€­á€•á€«á€á€šá€º:

1. **1981** â€” Richard Sutton & Andrew Barto: "An Adaptive Network that Constructs and Uses an Internal Model of Its World"
2. **1990** â€” Richard Sutton: "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
3. **1991** â€” Richard Sutton: "Dyna, an Integrated Architecture for Learning, Planning, and Reacting" â€” Dyna-Q agent á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·

---

### Dyna-Q Model Learning Equations

**Transition function** á€€á€­á€¯ count-based approach á€–á€¼á€„á€·á€º learn:

$$\hat{T}(s'|s, a) = \frac{\text{count}(s, a, s')}{\sum_{s''} \text{count}(s, a, s'')}$$

> $(s, a, s')$ triplet á€€á€­á€¯ á€˜á€šá€ºá€”á€¾á€…á€ºá€€á€¼á€­á€™á€º á€á€½á€±á€·á€á€²á€·á€œá€² á€›á€±á€á€½á€€á€ºá€•á€¼á€®á€¸ probability á€€á€­á€¯ estimate

**Reward function** á€€á€­á€¯ incremental mean á€–á€¼á€„á€·á€º learn:

$$\hat{R}(s, a, s') \leftarrow \hat{R}(s, a, s') + \frac{r - \hat{R}(s, a, s')}{\text{count}(s, a, s')}$$

> Experience sample á€¡á€á€…á€ºá€›á€á€­á€¯á€„á€ºá€¸ reward estimate á€€á€­á€¯ incremental mean á€–á€¼á€„á€·á€º update

---

### Python Code â€” Dyna-Q Agent (1/3)

```python
def dyna_q(env,
           gamma=1.0,
           init_alpha=0.5,
           min_alpha=0.01,
           alpha_decay_ratio=0.5,
           init_epsilon=1.0,
           min_epsilon=0.1,
           epsilon_decay_ratio=0.9,
           n_planning=3,                # Planning phase á€›á€²á€· update á€¡á€›á€±á€¡á€á€½á€€á€º
           n_episodes=3000):
    nS, nA = env.observation_space.n, env.action_space.n
    pi_track = []
    Q = np.zeros((nS, nA), dtype=np.float64)              # Q-function
    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)
    T_count = np.zeros((nS, nA, nS), dtype=np.int)        # Transition count tensor
    R_model = np.zeros((nS, nA, nS), dtype=np.float64)    # Reward model tensor
    
    select_action = lambda state, Q, epsilon: \
        np.argmax(Q[state]) \
        if np.random.random() > epsilon \
        else np.random.randint(len(Q[state]))
    
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    epsilons = decay_schedule(
        init_epsilon, min_epsilon, 
        epsilon_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 2/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Dyna-Q á€á€Šá€º Q-learning agent á€”á€¾á€„á€·á€º á€†á€„á€ºá€á€°á€•á€±á€™á€šá€·á€ºã€environment model á€€á€­á€¯ á€á€„á€ºá€šá€°á€•á€¼á€®á€¸ á€’á€® model á€€á€­á€¯ estimates improve á€›á€¬á€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€º
2. `n_planning` â€” learned model á€€á€”á€± á€˜á€šá€ºá€”á€¾á€…á€ºá€€á€¼á€­á€™á€º planning update á€œá€¯á€•á€ºá€™á€šá€ºá€†á€­á€¯á€á€¬
3. `T_count` â€” transition function track ($3$D tensor: state Ã— action Ã— next\_state)
4. `R_model` â€” reward signal track ($3$D tensor)
5. á€€á€»á€”á€ºá€á€¬ Q-learning agent á€”á€¾á€„á€·á€º á€¡á€á€°á€á€°á€•á€«á€•á€²

---

### Python Code â€” Dyna-Q Agent (2/3)

```python
    for e in tqdm(range(n_episodes), leave=False):
        state, done = env.reset(), False
        while not done:
            action = select_action(state, Q, epsilons[e])
            next_state, reward, done, _ = env.step(action)
            
            # === Model Learning Phase ===
            # Transition count increment
            T_count[state][action][next_state] += 1
            
            # Reward model: incremental mean
            r_diff = reward - R_model[state][action][next_state]
            R_model[state][action][next_state] += \
                (r_diff / T_count[state][action][next_state])
            
            # === Model-free RL Phase (Q-learning) ===
            td_target = reward + gamma * Q[next_state].max() * (not done)
            td_error = td_target - Q[state][action]
            Q[state][action] = Q[state][action] + alphas[e] * td_error
            
            # next_state á€€á€­á€¯ backup (planning loop á€•á€¼á€®á€¸á€›á€„á€º restore á€œá€¯á€•á€ºá€–á€­á€¯á€·)
            backup_next_state = next_state
            
            # === Planning Phase ===
            for _ in range(n_planning):
                # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 3/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Episode loop á€‘á€²á€™á€¾á€¬ environment á€”á€¾á€„á€· interact á€–á€¼á€…á€ºá€•á€¼á€®á€¸ experience tuple á€›á€šá€°
2. **Model learning:** `T_count` á€€á€­á€¯ increment á€•á€¼á€®á€¸ `R_model` á€€á€­á€¯ incremental mean á€–á€¼á€„á€·á€º update â€” á€’á€«á€€ model á€€á€­á€¯ á€á€„á€ºá€šá€°á€”á€±á€á€¬!
3. **Model-free RL:** Q-learning style TD target (max á€á€¯á€¶á€¸, off-policy) á€–á€¼á€„á€·á€º Q-function update
4. `backup_next_state` â€” planning loop á€‘á€²á€™á€¾á€¬ state variable á€•á€¼á€±á€¬á€„á€ºá€¸á€á€½á€¬á€¸á€™á€¾á€¬á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º backup

---

### Python Code â€” Dyna-Q Agent (3/3)

```python
            # === Planning Phase ===
            for _ in range(n_planning):
                if Q.sum() == 0: break     # Q-function á€™á€¾á€¬ update á€™á€›á€¾á€­á€á€±á€¸á€›á€„á€º plan á€™á€œá€¯á€•á€º
                
                # á€šá€á€„á€º visit á€œá€¯á€•á€ºá€á€²á€·á€–á€°á€¸á€á€²á€· state á€€á€­á€¯ uniformly at random sample
                visited_states = np.where(
                    np.sum(T_count, axis=(1, 2)) > 0)[0]
                state = np.random.choice(visited_states)
                
                # á€’á€® state á€™á€¾á€¬ á€šá€á€„á€º á€šá€°á€á€²á€·á€–á€°á€¸á€á€²á€· action á€€á€­á€¯ random sample
                actions_taken = np.where(
                    np.sum(T_count[state], axis=1) > 0)[0]
                action = np.random.choice(actions_taken)
                
                # Learned model á€€á€”á€± next state á€€á€­á€¯ sample
                probs = T_count[state][action] / T_count[state][action].sum()
                next_state = np.random.choice(
                    np.arange(nS), size=1, p=probs)[0]
                reward = R_model[state][action][next_state]
                
                # Simulated experience á€–á€¼á€„á€·á€º Q-function update!
                td_target = reward + gamma * Q[next_state].max()
                td_error = td_target - Q[state][action]
                Q[state][action] = Q[state][action] + alphas[e] * td_error
            
            state = backup_next_state     # Planning loop á€•á€¼á€®á€¸á€›á€„á€º real next state á€•á€¼á€”á€ºá€šá€°
        
        Q_track[e] = Q
        pi_track.append(np.argmax(Q, axis=1))
    
    V = np.max(Q, axis=1)
    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return Q, V, pi, Q_track, pi_track
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Q-function á€™á€¾á€¬ update á€á€…á€ºá€á€¯á€™á€¾ á€™á€›á€¾á€­á€á€±á€¸á€›á€„á€º planning á€•á€á€¬á€™ á€™á€‘á€­á€™á€…á€¬á€¸á€•á€«
2. **State sampling:** visit á€œá€¯á€•á€ºá€á€²á€·á€–á€°á€¸á€á€²á€· states á€™á€»á€¬á€¸á€‘á€²á€€á€”á€± **uniformly at random** á€›á€½á€±á€¸ â€” á€’á€«á€€ Dyna-Q á€›á€²á€· planning strategy
3. **Action sampling:** á€’á€® state á€™á€¾á€¬ á€šá€°á€á€²á€·á€–á€°á€¸á€á€²á€· actions á€‘á€²á€€á€”á€± **random** á€›á€½á€±á€¸
4. **Model-based next state:** count matrix á€€á€”á€± probabilities á€á€½á€€á€ºá€•á€¼á€®á€¸ next state á€€á€­á€¯ sample
5. **Simulated Q update:** Real experience á€€á€²á€·á€á€­á€¯á€· Q-function á€€á€­á€¯ update â€” á€’á€«á€€ "planning" á€–á€¼á€…á€ºá€á€¬!
6. Planning loop á€•á€¼á€®á€¸á€›á€„á€º `state = backup_next_state` á€–á€¼á€„á€·á€º real interaction á€€á€­á€¯ á€†á€€á€ºá€œá€€á€º

> ğŸ’¡ Dyna-Q á€á€Šá€º state-action pairs á€€á€­á€¯ **uniformly at random** sample á€•á€«á€á€šá€ºá‹ Effective á€•á€±á€™á€šá€·á€º optimal sampling strategy á€™á€Ÿá€¯á€á€ºá€•á€«á‹ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€²á€· strategy á€›á€¾á€­á€•á€«á€á€±á€¸á€á€œá€¬á€¸?

---

### Model Learning á á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯

Dyna-Q á€€ episode á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ environment model á€€á€­á€¯ á€á€–á€¼á€Šá€ºá€¸á€–á€¼á€Šá€ºá€¸ improve á€•á€«á€á€šá€º:

- **Episode 1 á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:** Model á€™á€¾á€¬ obvious issues á€›á€¾á€­á€•á€«á€á€šá€ºá‹ Episode á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€”á€²á€· model á€€á€­á€¯ á€šá€¯á€¶á€€á€¼á€Šá€ºá€›á€„á€º bias á€›á€¾á€­á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º
- **Episodes 10 á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:** Model á€•á€¯á€¶á€á€á€¹á€á€¬á€”á€º á€–á€¼á€…á€ºá€œá€¬á€•á€«á€á€šá€º
- **Episodes 100 á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:** Transition probabilities á€™á€»á€¬á€¸á€€ real MDP á€”á€¾á€„á€· á€”á€®á€¸á€€á€•á€ºá€œá€¬á€•á€«á€á€šá€º
- **Episodes 1000+ á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:** MDP á€€á€­á€¯ correctly describe á€œá€¯á€•á€ºá€•á€«á€á€šá€º

> á€¥á€•á€™á€¬ â€” State 7 á€€á€”á€± right action á€šá€°á€›á€„á€º state 8 á€€á€­á€¯ ~50% probability á€–á€¼á€„á€·á€ºã€state 7 á€™á€¾á€¬ ~30% á€€á€»á€”á€ºã€state 6 á€€á€­á€¯ ~20% á€–á€¼á€„á€·á€º á€›á€±á€¬á€€á€ºá€›á€™á€¾á€¬ á€–á€¼á€…á€ºá€•á€¼á€®á€¸ã€learned model á€€ á€’á€«á€€á€­á€¯ correctly reflect á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

---

## á‡.á‰ â€” Trajectory Sampling: á€¡á€”á€¬á€‚á€á€ºá€¡á€á€½á€€á€º Plan á€á€»á€á€¼á€„á€ºá€¸

### Dyna-Q á á€€á€”á€·á€ºá€á€á€ºá€á€»á€€á€º

Dyna-Q á€™á€¾á€¬ planning phase á€á€½á€„á€º visited state-action pairs á€€á€”á€± **uniformly at random** sample á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º á€’á€«á€€ intuitively á€™á€¾á€”á€ºá€€á€”á€ºá€á€œá€¬á€¸? Random state á€€á€­á€¯ sample á€•á€¼á€®á€¸ plan á€œá€¯á€•á€ºá€”á€±á€á€¬á€€á€­á€¯!

á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€²á€· approach á€›á€¾á€­á€á€²á€·á€›á€„á€ºá€›á€±á€¬? Current episode á€™á€¾á€¬ encounter á€›á€”á€­á€¯á€„á€ºá€á€»á€±á€›á€¾á€­á€á€²á€· state á€€á€­á€¯ prioritize á€•á€¼á€®á€¸ plan á€á€»á€›á€„á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€™á€¾á€¬ á€™á€Ÿá€¯á€á€ºá€œá€¬á€¸?

**Analogy á€–á€¼á€„á€·á€º á€…á€‰á€ºá€¸á€…á€¬á€¸á€€á€¼á€Šá€·á€ºá€•á€«:**
- á€á€„á€ºá€Ÿá€¬ software engineer á€†á€­á€¯á€•á€«á€…á€­á€¯á€·
- **Dyna-Q approach:** Random career change (medicine á€€á€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸) á€€á€­á€¯ plan á€á€»á€™á€šá€º
- **Trajectory sampling approach:** á€’á€®á€¡á€•á€á€º programming book á€–á€á€ºá€á€¼á€„á€ºá€¸ã€side project á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ plan á€á€»á€™á€šá€º

**Immediate future á€€á€­á€¯ plan á€á€»á€á€¬á€€ smarter approach á€–á€¼á€…á€ºá€•á€«á€á€šá€º!**

### Trajectory Sampling á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²

Trajectory sampling á€á€Šá€º model-based RL method á€–á€¼á€…á€ºá€•á€¼á€®á€¸ Dyna-Q á improved version á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Dyna-Q á€€ learned MDP á€€á€­á€¯ uniformly at random sample á€•á€±á€™á€šá€·á€ºã€trajectory sampling á€€ **immediate future** á€™á€¾á€¬ encounter á€›á€”á€­á€¯á€„á€ºá€á€²á€· trajectories (transitions á€”á€¾á€„á€·á€º rewards) á€€á€­á€¯ sample á€•á€«á€á€šá€ºá‹ á€á€„á€·á€ºá€›á€²á€· á€¡á€•á€á€ºá€€á€­á€¯ plan á€á€»á€”á€±á€á€¬ á€–á€¼á€…á€ºá€•á€¼á€®á€¸ã€á€˜á€á€‘á€² random time á€€á€­á€¯ plan á€á€»á€”á€±á€á€¬ á€™á€Ÿá€¯á€á€ºá€•á€«á‹

Traditional trajectory sampling approach á€€á€á€±á€¬á€· initial state á€€á€”á€± terminal state á€‘á€­ on-policy trajectory á€–á€¼á€„á€·á€º sample á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º á€€á€”á€·á€ºá€á€á€º á€™á€‘á€¬á€¸á€á€„á€·á€ºá€•á€« â€” experiment á€œá€¯á€•á€ºá€€á€¼á€Šá€·á€ºá€á€„á€·á€ºá€•á€«á€á€šá€ºá‹

> Miguel á€›á€²á€· implementation á€™á€¾á€¬ initial state á€¡á€…á€¬á€¸ **current state** á€€á€”á€± start á€•á€¼á€®á€¸ã€preset step á€¡á€›á€±á€¡á€á€½á€€á€ºá€¡á€á€½á€„á€ºá€¸ **current estimates á€¡á€•á€±á€«á€º greedy policy** á€–á€¼á€„á€·á€º sample á€•á€«á€á€šá€ºá‹ Trajectory á€€á€­á€¯ sample á€”á€±á€á€›á€½á€±á€· trajectory sampling á€œá€­á€¯á€· á€á€±á€«á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    subgraph DYNA["Dyna-Q Planning"]
        D1["Random state á€›á€½á€±á€¸<br/>(uniformly at random)"]
        D1 --> D2["Random action á€›á€½á€±á€¸"]
        D2 --> D3["Broad but unfocused<br/>planning"]
    end
    
    subgraph TS["Trajectory Sampling"]
        T1["Current state á€€á€”á€± start"]
        T1 --> T2["Greedy action á€›á€½á€±á€¸"]
        T2 --> T3["Focused planning<br/>for immediate future"]
    end
    
    style D1 fill:#ff922b,color:#fff
    style T1 fill:#4CAF50,color:#fff
```

| Feature | Dyna-Q | Trajectory Sampling |
|---|---|---|
| **State sampling** | Visited states á€‘á€²á€€ uniformly random | Current state á€€á€”á€± greedy trajectory follow |
| **Action sampling** | Taken actions á€‘á€²á€€ random | Current Q value á€¡á€•á€±á€«á€º greedy |
| **Planning focus** | Broad, unfocused | Immediate future, focused |
| **Reward encounter** | MDP probabilities á€¡á€á€­á€¯á€„á€ºá€¸ | á€•á€­á€¯á€™á€»á€¬á€¸ (goal-directed) |

---

### Python Code â€” Trajectory Sampling Agent (1/3)

```python
def trajectory_sampling(env,
                        gamma=1.0,
                        init_alpha=0.5,
                        min_alpha=0.01,
                        alpha_decay_ratio=0.5,
                        init_epsilon=1.0,
                        min_epsilon=0.1,
                        epsilon_decay_ratio=0.9,
                        max_trajectory_depth=100,   # Trajectory length á€€á€”á€·á€ºá€á€á€º
                        n_episodes=3000):
    nS, nA = env.observation_space.n, env.action_space.n
    pi_track = []
    Q = np.zeros((nS, nA), dtype=np.float64)
    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)
    T_count = np.zeros((nS, nA, nS), dtype=np.int)         # Transition model
    R_model = np.zeros((nS, nA, nS), dtype=np.float64)     # Reward model
    
    select_action = lambda state, Q, epsilon: \
        np.argmax(Q[state]) \
        if np.random.random() > epsilon \
        else np.random.randint(len(Q[state]))
    
    alphas = decay_schedule(
        init_alpha, min_alpha, 
        alpha_decay_ratio, n_episodes)
    epsilons = decay_schedule(
        init_epsilon, min_epsilon, 
        epsilon_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 2/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Dyna-Q á€”á€¾á€„á€· á€¡á€™á€»á€¬á€¸á€…á€¯á€€ á€á€°á€•á€«á€á€šá€º â€” á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€¡á€”á€Šá€ºá€¸á€„á€šá€º
2. `n_planning` á€¡á€…á€¬á€¸ `max_trajectory_depth` á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€º â€” trajectory length á€€á€­á€¯ á€€á€”á€·á€ºá€á€á€ºá€–á€­á€¯á€·
3. Model variables (T\_count, R\_model) á€á€½á€±á€€ Dyna-Q á€”á€¾á€„á€· á€¡á€á€°á€á€°

---

### Python Code â€” Trajectory Sampling Agent (2/3)

```python
    for e in tqdm(range(n_episodes), leave=False):
        state, done = env.reset(), False
        while not done:
            action = select_action(state, Q, epsilons[e])
            next_state, reward, done, _ = env.step(action)
            
            # === Model Learning (Dyna-Q á€”á€¾á€„á€· á€¡á€á€°á€á€°) ===
            T_count[state][action][next_state] += 1
            r_diff = reward - R_model[state][action][next_state]
            R_model[state][action][next_state] += \
                (r_diff / T_count[state][action][next_state])
            
            # === Model-free RL Phase ===
            td_target = reward + gamma * Q[next_state].max() * (not done)
            td_error = td_target - Q[state][action]
            Q[state][action] = Q[state][action] + alphas[e] * td_error
            
            backup_next_state = next_state
            
            # === Trajectory Sampling Planning Phase ===
            for _ in range(max_trajectory_depth):
                # â¬‡ï¸ á€†á€€á€ºá€œá€€á€º 3/3 á€™á€¾á€¬...
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
- Real interaction, model learning, model-free update parts á€á€½á€±á€¡á€¬á€¸á€œá€¯á€¶á€¸ Dyna-Q á€”á€¾á€„á€· á€¡á€á€°á€á€°
- á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€ planning loop á€‘á€² á€–á€¼á€…á€ºá€•á€«á€á€šá€º

---

### Python Code â€” Trajectory Sampling Agent (3/3)

```python
            # === Trajectory Sampling Planning Phase ===
            for _ in range(max_trajectory_depth):
                if Q.sum() == 0: break
                
                # Greedy action selection (Dyna-Q á€€á€œá€­á€¯ random á€™á€Ÿá€¯á€á€º!)
                action = Q[state].argmax()
                
                # á€’á€® transition á€€á€­á€¯ experience á€™á€›á€¾á€­á€á€±á€¸á€›á€„á€º break
                if not T_count[state][action].sum(): break
                
                # Model á€€á€”á€± next state sample
                probs = T_count[state][action] / T_count[state][action].sum()
                next_state = np.random.choice(
                    np.arange(nS), size=1, p=probs)[0]
                reward = R_model[state][action][next_state]
                
                # Q-function update (simulated experience á€–á€¼á€„á€·á€º)
                td_target = reward + gamma * Q[next_state].max()
                td_error = td_target - Q[state][action]
                Q[state][action] = Q[state][action] + alphas[e] * td_error
                
                # Trajectory á€€á€­á€¯ follow! (Dyna-Q á€™á€¾á€¬ á€’á€® line á€™á€›á€¾á€­)
                state = next_state
            
            state = backup_next_state     # Planning á€•á€¼á€®á€¸á€›á€„á€º real state á€•á€¼á€”á€ºá€šá€°
        
        Q_track[e] = Q
        pi_track.append(np.argmax(Q, axis=1))
    
    V = np.max(Q, axis=1)
    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]
    return Q, V, pi, Q_track, pi_track
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º â€” Dyna-Q á€”á€¾á€„á€· á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€™á€»á€¬á€¸:**

1. **`action = Q[state].argmax()`** â€” Greedy action á€šá€°á€á€šá€º (Dyna-Q: random action)
2. **`if not T_count[state][action].sum(): break`** â€” á€’á€® transition á€€á€­á€¯ experience á€™á€›á€¾á€­á€á€±á€¸á€›á€„á€º plan á€á€»á€›á€„á€º mess á€–á€¼á€…á€ºá€™á€šá€ºá€˜á€² break
3. **`state = next_state`** â€” Trajectory á€€á€­á€¯ follow! Planning loop á€‘á€²á€™á€¾á€¬ state á€€á€­á€¯ update á€•á€¼á€®á€¸ greedy trajectory á€€á€­á€¯ á€†á€€á€ºá€œá€­á€¯á€€á€º
4. Planning loop á€•á€¼á€®á€¸á€™á€¾ `state = backup_next_state` á€–á€¼á€„á€·á€º real interaction á€•á€¼á€”á€ºá€†á€€á€º

> ğŸ’¡ **Key Insight:** Dyna-Q á€€ states á€€á€­á€¯á€›á€±á€¬ actions á€€á€­á€¯á€›á€±á€¬ uniformly at random sample á€•á€±á€™á€šá€·á€ºã€trajectory sampling á€€ **greedy trajectory** á€€á€­á€¯ follow á€•á€«á€á€šá€ºá‹ SWS environment á€™á€¾á€¬ goal state (state 8) á€†á€® skew á€–á€¼á€…á€ºá€•á€¼á€®á€¸ non-zero rewards á€€á€­á€¯ model á€€á€”á€± á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ encounter á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

### Dyna-Q vs. Trajectory Sampling: Sampling Strategy á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º

**Dyna-Q:**
- States á€€á€­á€¯á€›á€±á€¬ actions á€€á€­á€¯á€›á€±á€¬ uniformly at random sample
- Goal state á€›á€²á€· reward á€€á€­á€¯ MDP probabilities á€¡á€á€­á€¯á€„á€ºá€¸ encounter

**Trajectory Sampling:**
- Greedy trajectory follow á€•á€¼á€®á€¸ goal-directed states á€€á€­á€¯ á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ sample
- SWS á€™á€¾á€¬ right action á€€á€­á€¯ left action á€‘á€€á€º á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ sample
- State 7 (goal á á€˜á€šá€ºá€˜á€€á€º) á€€á€”á€± action á€šá€°á€›á€„á€º goal state á€€á€­á€¯ **á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ land** á€•á€¼á€®á€¸ non-zero rewards á€€á€­á€¯ **á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ experience**

---

## á‡.áá€ â€” Frozen Lake Environment

### Frozen Lake (FL) 4Ã—4

Chapter 2 á€™á€¾á€¬ develop á€œá€¯á€•á€ºá€á€²á€·á€á€²á€· **Frozen Lake (FL)** environment á€€á€­á€¯ á€•á€¼á€”á€ºá€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ FL á€á€Šá€º simple grid-world environment á€–á€¼á€…á€ºá€•á€¼á€®á€¸ discrete state and action spaces á€›á€¾á€­á€•á€«á€á€šá€º â€” **16 states**, **4 actions**ã€‚

**á€•á€”á€ºá€¸á€á€­á€¯á€„á€º:** Start location (state 0) á€€á€”á€± goal location (state 15) á€†á€® holes á€á€½á€± avoid á€•á€¼á€®á€¸ á€›á€±á€¬á€€á€ºá€›á€¾á€­á€á€¼á€„á€ºá€¸á‹ á€›á€±á€á€²á€•á€¼á€„á€ºá€•á€±á€«á€º á€á€¼á€±á€á€»á€±á€¬á€ºá€á€á€ºá€á€²á€· surface á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

```
 S   .   .   .
 .   H   .   H
 .   .   .   H
 H   .   .   G
```

| Feature | á€á€”á€ºá€–á€­á€¯á€¸ |
|---|---|
| **States** | 16 (4Ã—4 grid, state 0 = top-left, state 15 = bottom-right) |
| **Actions** | 4 (Up, Down, Left, Right) |
| **Initial state** | State 0 (START) |
| **Terminal states** | States 5, 7, 11, 12 (Holes) á€”á€¾á€„á€·á€º State 15 (GOAL) |
| **Rewards** | State 15 á€›á€±á€¬á€€á€ºá€›á€„á€º +1, á€€á€»á€”á€ºá€á€¬ 0 |
| **Slippery** | 33.3% intended, á€€á€»á€”á€º 66.7% orthogonal directions |

> Agent á€€ +1 transition á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€–á€­á€¯á€· á€€á€¼á€­á€¯á€¸á€…á€¬á€¸á€™á€šá€ºã€á€’á€«á€€ holes avoid á€á€¼á€„á€ºá€¸á€€á€­á€¯ entail á€•á€«á€á€šá€ºá‹ Actions á€›á€²á€· stochastic effects á€€á€¼á€±á€¬á€„á€·á€º agent á€€ á€›á€Šá€ºá€›á€½á€šá€ºá€‘á€¬á€¸á€á€œá€­á€¯ áƒ á€•á€¯á€¶ á á€•á€¯á€¶á€á€¬ á€›á€½á€¾á€±á€·á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

### FL Environment Hyperparameters

FL environment á€á€Šá€º SWS á€‘á€€á€º á€•á€­á€¯á€á€€á€ºá€á€²á€•á€«á€á€šá€ºá‹ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯á€€ **episode á€¡á€›á€±á€¡á€á€½á€€á€ºá€€á€­á€¯ 3,000 á€€á€”á€± 10,000 á€á€­á€¯á€· á€á€­á€¯á€¸á€á€¼á€„á€ºá€¸**á€•á€«á‹ á€’á€«á€–á€¼á€„á€·á€º alpha á€”á€¾á€„á€·á€º epsilon decay schedules á€€á€œá€Šá€ºá€¸ automatically adjust á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

- **Alpha:** 0.5 á€€á€”á€± 0.01 á€†á€® 50% episodes (5,000) á€¡á€á€½á€„á€ºá€¸ decay
- **Epsilon:** 1.0 á€€á€”á€± 0.1 á€†á€® 90% episodes (9,000) á€¡á€á€½á€„á€ºá€¸ decay
- **Gamma:** 0.99 (discounting)
- **Time limit:** OpenAI Gym wrapper á€–á€¼á€„á€·á€º episode á€€á€­á€¯ 100 steps á€‘á€­ á€€á€”á€·á€ºá€á€á€º

> âš ï¸ **á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€»á€€á€º:** Gamma á€”á€¾á€„á€·á€º time wrapper á á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸á€€ optimal policy á€”á€¾á€„á€· value function á€€á€­á€¯ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€…á€±á€•á€«á€á€šá€ºá‹ FL environment notebook á€á€½á€„á€º gamma á€€á€­á€¯ different values (1, 0.5, 0) á€–á€¼á€„á€·á€º experiment á€œá€¯á€•á€ºá€€á€¼á€Šá€·á€ºá€•á€¼á€®á€¸ time wrapper á€€á€­á€¯ `env = env.unwrapped` á€–á€¼á€„á€·á€º á€–á€šá€ºá€›á€¾á€¬á€¸á€€á€¼á€Šá€·á€ºá€•á€«á‹

---

### Frozen Lake 8Ã—8 (FL8Ã—8)

á€•á€­á€¯á€á€€á€ºá€á€²á€á€²á€· environment á€–á€¼á€„á€·á€º test á€€á€¼á€Šá€·á€ºá€€á€¼á€™á€šá€º!

FL8Ã—8 á€á€Šá€º 8Ã—8 grid world á€–á€¼á€…á€ºá€•á€¼á€®á€¸ FL á€”á€¾á€„á€· properties á€†á€„á€ºá€á€°á€•á€«á€á€šá€º:

```
 S   .   .   .   .   .   .   .
 .   .   .   .   .   .   .   .
 .   .   .   H   .   .   .   .
 .   .   .   .   .   H   .   .
 .   .   .   H   .   .   .   .
 .   H   H   .   .   H   .   .
 .   H   .   .   H   .   H   .
 .   .   .   H   .   .   .   G
```

| Feature | á€á€”á€ºá€–á€­á€¯á€¸ |
|---|---|
| **States** | 64 (8Ã—8 grid) |
| **Initial state** | State 0 (top-left) |
| **Goal state** | State 63 (bottom-right) |
| **Holes** | States 19, 29, 35, 41, 42, 46, 49, 52, 54, 59 (á€…á€¯á€…á€¯á€•á€±á€«á€„á€ºá€¸ **10 holes!**) |
| **Slippery** | 33.3% intended |
| **Time limit** | 200 steps |

### FL8Ã—8 Hyperparameters

FL8Ã—8 á€á€Šá€º á€’á€®á€…á€¬á€¡á€¯á€•á€ºá€™á€¾á€¬ á€¡á€á€€á€ºá€á€²á€†á€¯á€¶á€¸ discrete environment á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

- **64 states** â€” á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ state space
- **Single non-zero reward** â€” agent á€€ GOAL á€€á€­á€¯ á€•á€‘á€™á€†á€¯á€¶á€¸á€¡á€€á€¼á€­á€™á€º randomly á€›á€±á€¬á€€á€ºá€™á€¾ reward á€›á€™á€šá€º
- **Episode count:** **30,000** episodes (SWS: 3,000, FL: 10,000)
- **Lambda:** 0.5 (trajectory á á€‘á€€á€ºá€á€€á€ºá€á€”á€·á€º propagate)
- **Alpha decay:** 0.5 â†’ 0.01 (15,000 episodes á€¡á€á€½á€„á€ºá€¸)
- **Epsilon decay:** 1.0 â†’ 0.1 (27,000 episodes á€¡á€á€½á€„á€ºá€¸)

> ğŸ’¡ **SARSA/Q-learning (vanilla versions)** á€á€Šá€º reward á€€á€­á€¯ á€•á€‘á€™á€†á€¯á€¶á€¸ á€›á€á€»á€­á€”á€ºá€™á€¾á€¬ GOAL á€›á€²á€· **previous state** á€€á€­á€¯á€á€¬ update á€•á€«á€á€šá€º (one-step back)á‹ á€’á€® value á€€á€­á€¯ á€‘á€•á€ºá€†á€„á€·á€º propagate á€›á€”á€º á€’á€¯á€á€­á€š-to-final state á€€á€­á€¯ randomly á€•á€¼á€”á€ºá€›á€±á€¬á€€á€ºá€›á€•á€«á€™á€šá€ºá‹ **SARSA(Î»)** á€”á€¾á€„á€·á€º **Q(Î»)** á€†á€­á€¯á€›á€„á€ºá€á€±á€¬á€· propagation depth á€€ lambda value á€•á€±á€«á€º á€™á€°á€á€Šá€º â€” Î»=0.5 á€†á€­á€¯á€›á€„á€º trajectory á á€‘á€€á€ºá€á€€á€ºá€á€”á€·á€º propagate á€•á€«á€á€šá€ºá‹

| Environment | Episodes | Î³ | Alpha decay | Epsilon decay |
|---|---|---|---|---|
| **SWS** | 3,000 | 1.0 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |
| **FL 4Ã—4** | 10,000 | 0.99 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |
| **FL 8Ã—8** | 30,000 | 0.99 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |

---

## á‡.áá â€” Experimental Results

### FL 4Ã—4 á€›á€œá€’á€ºá€™á€»á€¬á€¸

**State-value function estimation:**
1. **SARSA(Î»):** Optimal state-value function á€€á€­á€¯ estimate á€›á€¬á€á€½á€„á€º struggle á€–á€¼á€…á€º â€” on-policy algorithm á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º á€á€€á€ºá€á€²
2. **Q(Î»):** Off-policy á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º estimates á€™á€»á€¬á€¸á€€ optimal values á€†á€® á€›á€½á€¾á€±á€·á€›á€¬á€á€½á€„á€º SARSA(Î») á€‘á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á‹ SARSA(Î») á€€ episodes á€•á€­á€¯á€›á€›á€„á€º converge á€–á€¼á€…á€ºá€•á€«á€œá€­á€™á€·á€ºá€™á€šá€º
3. **Dyna-Q:** Q(Î») á€‘á€€á€º á€•á€­á€¯á€™á€¼á€”á€ºá€•á€±á€™á€šá€·á€º training á€¡á€…á€±á€¬á€•á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ **large error spike** á€›á€¾á€­ â€” early model incorrect á€–á€¼á€…á€ºá€•á€¼á€®á€¸ random states sample á€á€²á€·á€¡á€á€½á€€á€º
4. **Trajectory Sampling:** Greedy trajectory sampling á€–á€¼á€„á€·á€º encounter á€›á€”á€­á€¯á€„á€ºá€á€»á€±á€á€²á€· states á€€á€­á€¯ sample â†’ **á€•á€­á€¯á€™á€­á€¯ stable** estimates

**Success rate á€”á€¾á€„á€· policy performance:**
- SARSA(Î») á€™á€¾á€œá€½á€²á algorithms á€¡á€¬á€¸á€œá€¯á€¶á€¸ optimal policy á success rate á€›á€›á€¾á€­
- Model-based methods á€•á€­á€¯á€™á€¼á€”á€ºá€›á€±á€¬á€€á€ºá€•á€±á€™á€šá€·á€º differ á€á€¼á€„á€ºá€¸ á€™á€™á€»á€¬á€¸
- Model-based methods á‚ á€á€¯á€œá€¯á€¶á€¸á€™á€¾á€¬ initial error spike á€›á€¾á€­ â€” trajectory sampling á€•á€­á€¯á€…á€±á€¬á€…á€±á€¬á€á€Šá€ºá€„á€¼á€­á€™á€º

### FL 8Ã—8 á€›á€œá€’á€ºá€™á€»á€¬á€¸

FL8Ã—8 á€™á€¾á€¬ trends á€á€½á€± á€•á€­á€¯á€‘á€„á€ºá€›á€¾á€¬á€¸á€œá€¬á€•á€«á€á€šá€º:

1. **SARSA(Î»):** Estimates **optimal values á€”á€²á€· á€”á€®á€¸á€€á€•á€ºá€™á€¾á€¯ á€™á€›á€¾á€­**ã€€ â€” on-policy algorithm á€–á€¼á€„á€· á€á€€á€ºá€á€²á€œá€½á€”á€ºá€¸
2. **Q(Î»):** Estimates á€™á€»á€¬á€¸ optimal values reflect á€–á€¼á€…á€º (time limit caveat á€›á€¾á€­)
3. **Dyna-Q:** Model-based advantage á€€á€¼á€®á€¸á€™á€¬á€¸ â€” reward á€•á€‘á€™á€†á€¯á€¶á€¸á€›á€›á€„á€º planning phase á€€ values á€€á€­á€¯ **á€¡á€™á€¼á€”á€ºá€†á€¯á€¶á€¸ propagate**
4. **Trajectory Sampling:** Estimates á€™á€»á€¬á€¸ optimal values á€€á€­á€¯ track á€•á€¼á€®á€¸ **spike á€™á€›á€¾á€­ á€•á€­á€¯á€™á€­á€¯ stable curve** á€•á€±á€¸

**Error Analysis:**
- Dyna-Q á estimated expected return error á€€á€¼á€®á€¸á€™á€¬á€¸ (unstable)
- Trajectory sampling á€”á€¾á€„á€· Q(Î») agents á€•á€­á€¯á€™á€­á€¯ stable
- Dyna-Q á action-value function error **á€¡á€”á€­á€™á€·á€ºá€†á€¯á€¶á€¸** â€” uniformly random sampling á€€á€¼á€±á€¬á€„á€·á€º state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ update á€›
- Trajectory sampling (greedy only) á€€á€á€±á€¬á€· á€¡á€á€»á€­á€¯á€· states á€™á€»á€¬á€¸á€€á€­á€¯ update á€™á€› (greedy trajectory á€•á€±á€«á€º á€™á€›á€¾á€­)

### Results Summary Table

| Method | SWS | FL 4Ã—4 | FL 8Ã—8 | Sample Efficiency |
|---|---|---|---|---|
| **SARSA(Î»)** | OK | Slow | âŒ Too slow | Low |
| **Q(Î»)** | Good | Good | âœ… Converges | Medium-High |
| **Dyna-Q** | Good | Fast | âœ… Fast but spiky | High |
| **Trajectory Sampling** | Good | Fast | âœ… Fast & stable | Highest |

> ğŸ’¡ **Model-based methods** (Dyna-Q, Trajectory Sampling) á€á€Šá€º model-free methods á€‘á€€á€º **sample efficient** á€•á€­á€¯á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Dyna-Q á€™á€¾á€¬ initial error spike á€›á€¾á€­á€”á€­á€¯á€„á€ºá€•á€±á€™á€šá€·á€º trajectory sampling á€™á€¾á€¬ á€•á€­á€¯á€™á€­á€¯ stable á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## á‡.áá‚ â€” Key Equations Summary

| Equation | Formula |
|---|---|
| **SARSA(Î») trace** | $E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{1}[s=S_t, a=A_t]$ |
| **SARSA(Î») Q update** | $Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a)$ |
| **SARSA(Î») TD error** | $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$ |
| **Q(Î») trace (greedy)** | $E_t = \gamma \lambda E_{t-1}$ |
| **Q(Î») trace (exploratory)** | $E_t = 0$ (reset all) |
| **Dyna-Q transition model** | $\hat{T}(s'\|s,a) = \frac{\text{count}(s,a,s')}{\sum_{s''}\text{count}(s,a,s'')}$ |
| **Dyna-Q reward model** | $\hat{R}(s,a,s') \leftarrow \hat{R} + \frac{r - \hat{R}}{\text{count}(s,a,s')}$ |

---

## á‡.ááƒ â€” á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Summary)

á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ RL á€€á€­á€¯ **á€•á€­á€¯á€™á€­á€¯á€‘á€­á€›á€±á€¬á€€á€º** (effective) á€›á€±á€¬ **á€•á€­á€¯á€™á€­á€¯á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­** (efficient) á€›á€±á€¬ á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€á€„á€ºá€šá€°á€á€²á€·á€•á€«á€á€šá€º:

### Effective á€†á€­á€¯á€á€¬:
á€’á€®á€¡á€á€”á€ºá€¸á€›á€²á€· agents á€á€½á€±á€€ interaction **á€€á€”á€·á€ºá€á€á€ºá€‘á€¬á€¸á€á€²á€· episodes** á€¡á€á€½á€„á€ºá€¸á€™á€¾á€¬ environment á€€á€­á€¯ solve á€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Vanilla SARSA, Q-learning, Monte Carlo control á€á€­á€¯á€· FL8Ã—8 á€€á€­á€¯ 30,000 episodes á€–á€¼á€„á€·á€º solve á€›á€¬á€á€½á€„á€º á€•á€¼á€¿á€”á€¬á€›á€¾á€­á€•á€«á€œá€­á€™á€·á€ºá€™á€šá€ºá‹

### Efficient á€†á€­á€¯á€á€¬:
**Data-efficient** â€” data á€á€°á€á€°á€”á€²á€· á€•á€­á€¯á€™á€»á€¬á€¸á€™á€»á€¬á€¸ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸:

- **SARSA(Î») á€”á€¾á€„á€·á€º Q(Î»):** Vanilla counterparts á€‘á€€á€º rewards á€€á€­á€¯ value-function estimates á€†á€® **á€•á€­á€¯á€™á€¼á€”á€ºá€†á€”á€ºá€…á€½á€¬ propagate** á€”á€­á€¯á€„á€ºá‹ Î» parameter á€€á€­á€¯ adjust á€•á€¼á€®á€¸ episode á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ credit assign á€”á€­á€¯á€„á€º (Î»=1 á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸ á€™á€Ÿá€¯á€á€ºá€•á€±á€™á€šá€·á€º option á€›á€¾á€­)
- **Dyna-Q á€”á€¾á€„á€·á€º trajectory sampling:** Samples á€€á€­á€¯ model á€á€„á€ºá€šá€°á€›á€”á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á‹ Agent á€€ samples 1M á€á€¯ state s' á€€á€­á€¯ 100% land á€–á€¼á€…á€ºá€›á€„á€º á€’á€® information á€€á€­á€¯ value functions/policies improve á€›á€¬á€á€½á€„á€º á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º á€™á€á€¯á€¶á€¸á€›á€™á€œá€²?

### Advanced Model-based Deep RL:
Experience samples á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€›á€á€€á€ºá€á€²á€· domains (robotics, high-speed simulation á€™á€›á€¾á€­, hardware á€€á€¼á€®á€¸á€™á€¬á€¸á€á€²á€· resources) á€™á€¾á€¬ model-based methods á€¡á€œá€½á€”á€ºá€¡á€›á€±á€¸á€€á€¼á€®á€¸á€•á€«á€á€šá€ºá‹

### á€›á€¾á€±á€·á€†á€€á€ºá€›á€™á€Šá€·á€º á€œá€™á€ºá€¸:
á€’á€®á€…á€¬á€¡á€¯á€•á€ºá á€€á€»á€”á€º chapters á€á€½á€±á€™á€¾á€¬ **non-linear function approximation** (neural networks) á€€á€­á€¯ RL á€”á€¾á€„á€· á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€™á€šá€ºá‹ á€šá€á€¯á€‘á€­ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€›á€¬á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€ á€†á€€á€ºá€œá€€á€ºá€á€€á€ºá€†á€­á€¯á€„á€ºá€•á€«á€á€šá€º â€” á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€á€á€±á€¬á€· vectors/matrices á€¡á€…á€¬á€¸ **supervised learning** á€”á€¾á€„á€· **function approximation** á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ DRL á€™á€¾á€¬ agents á€á€½á€±á€€ **sequential** (one-shot á€™á€Ÿá€¯á€á€º), **evaluative** (supervised á€™á€Ÿá€¯á€á€º), **sampled** (exhaustive á€™á€Ÿá€¯á€á€º) feedback á€€á€­á€¯ handle á€›á€•á€«á€á€šá€ºá‹ "Sampled" part á€€á€­á€¯ á€’á€®á€¡á€á€»á€­á€”á€ºá€‘á€­ á€™á€‘á€­á€á€²á€·á€˜á€°á€¸ â€” agents á€á€½á€± states/state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ visit á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€²á€·á€á€šá€ºá‹ Chapter 8 á€€á€”á€±á€…á€•á€¼á€®á€¸ **exhaustively sample á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º**á€á€²á€· problems á€€á€­á€¯ á€¡á€¬á€›á€¯á€¶á€…á€­á€¯á€€á€ºá€•á€«á€™á€šá€ºá‹

```mermaid
graph TD
    CH7["Chapter 7:<br/>More Effective & Efficient"] --> TRACES["Eligibility Traces"]
    CH7 --> MBRL["Model-based RL"]
    
    TRACES --> SL["SARSA(Î»)<br/>On-policy"]
    TRACES --> QLam["Q(Î»)<br/>Off-policy"]
    
    SL --> ACC["Accumulating Traces"]
    SL --> REP["Replacing Traces"]
    QLam --> ACC
    QLam --> REP
    
    MBRL --> DQ["Dyna-Q<br/>Random planning"]
    MBRL --> TS["Trajectory Sampling<br/>Focused planning"]
    
    style CH7 fill:#ffd43b,color:#000
    style SL fill:#2196F3,color:#fff
    style QLam fill:#4CAF50,color:#fff
    style DQ fill:#9C27B0,color:#fff
    style TS fill:#ef5350,color:#fff
```

á€’á€®á€¡á€á€”á€ºá€¸á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:

- âœ… RL agents á€€á€­á€¯ á€•á€”á€ºá€¸á€á€­á€¯á€„á€ºá€›á€±á€¬á€€á€ºá€›á€¬á€á€½á€„á€º **á€•á€­á€¯á€‘á€­á€›á€±á€¬á€€á€ºá€¡á€±á€¬á€„á€º** develop á€œá€¯á€•á€ºá€á€á€ºá€•á€¼á€®
- âœ… RL agents á€€á€­á€¯ **sample efficient** á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€œá€¯á€•á€ºá€á€á€ºá€•á€¼á€®
- âœ… **Sequential á€›á€±á€¬ evaluative á€›á€±á€¬** á€–á€¼á€…á€ºá€á€²á€· feedback á€€á€­á€¯ handle á€œá€¯á€•á€ºá€á€á€ºá€•á€¼á€®

> ğŸ’¡ Chapter 8 á€€á€”á€± tabular RL (discrete states/actions) á€€á€”á€± **deep RL** (continuous/high-dimensional states) á€†á€® á€€á€°á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ function approximation (neural networks) á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹
