# Chapter 5: Evaluating Agents' Behaviors - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ **sequential + evaluative** feedback á€”á€¾á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€á€Šá€ºá€¸ á€€á€­á€¯á€„á€ºá€á€½á€šá€ºá€•á€¼á€®á€¸ agent á€›á€²á€· policy á€€á€­á€¯ **evaluate** (prediction problem) á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€„á€ºá€šá€°á€•á€«á€á€šá€ºá‹ MDP á€€á€­á€¯ **á€™á€á€­á€˜á€²** experience á€€á€”á€± state-value function $V^\pi(s)$ á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€›á€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph CH3["ğŸ“š Chapter 3: Planning"]
        P3["MDP known<br/>PI / VI algorithms"]
    end
    
    subgraph CH4["ğŸ“˜ Chapter 4: Bandits"]
        P4["MDP unknown<br/>Evaluative feedback<br/>Single-state (isolation)"]
    end
    
    subgraph CH5["ğŸ“— Chapter 5: Prediction"]
        P5["MDP unknown<br/>Sequential + Evaluative<br/>Multi-state environments"]
    end
    
    CH3 -->|"model-free<br/>á€–á€¼á€…á€ºá€œá€¬"| CH4
    CH4 -->|"sequential<br/>á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€º"| CH5
    
    style CH3 fill:#4CAF50,color:#fff
    style CH4 fill:#2196F3,color:#fff
    style CH5 fill:#9C27B0,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Reward vs Return vs Value function** á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º
2. **Monte Carlo (MC) prediction** â€” episode á€•á€¼á€®á€¸á€™á€¾ update
3. **Temporal-Difference (TD) learning** â€” step á€á€­á€¯á€„á€ºá€¸ update
4. **N-step TD** â€” MC á€”á€¾á€„á€·á€º TD á€€á€¼á€¬á€¸á€™á€¾á€¬ á€›á€¾á€­á€á€²á€· spectrum
5. **TD(Î»)** â€” eligibility traces á€–á€¼á€„á€·á€º MC á€”á€¾á€„á€·á€º TD á€€á€­á€¯ unify

---

## 2. Reward, Return, Value Function á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º

> ğŸ’¡ RL agent á€€ reward á€€á€­á€¯ maximize á€œá€¯á€•á€ºá€”á€±á€á€¬ **á€™á€Ÿá€¯á€á€ºá€•á€«**! Value function á€€á€­á€¯ maximize á€œá€¯á€•á€ºá€”á€±á€á€¬á€•á€«á‹

| Concept | Definition | Intuition |
|---|---|---|
| **Reward** $R_{t+1}$ | One-step reward signal | Immediate satisfaction â€” candy á€€á€­á€¯ á€šá€á€¯ á€…á€¬á€¸ |
| **Return** $G_t$ | Total discounted rewards (single episode) | Episode á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€›á€²á€· reward sum â€” á€’á€®á€”á€±á€· á€›á€›á€¾á€­á€á€²á€·á€¡á€€á€»á€­á€¯á€¸ |
| **Value function** $V^\pi(s)$ | Expectation of returns | Returns á€›á€²á€· average  â€” **long-term expected outcome** |

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

$$V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$$

```mermaid
graph LR
    R["ğŸ¬ Reward<br/>Immediate, greedy"] -->|"sum over time"| G["ğŸ“Š Return<br/>Episode total"]
    G -->|"average over episodes"| V["ğŸ“ˆ Value Function<br/>Expected return"]
    
    R -.->|"maximize? âŒ"| WRONG["Greedy, short-sighted"]
    G -.->|"maximize? âŒ"| NOISY["Noisy, one-shot"]
    V -.->|"maximize? âœ…"| CORRECT["Stable, long-term"]
    
    style R fill:#ff6b6b,color:#fff
    style G fill:#ff922b,color:#fff
    style V fill:#4CAF50,color:#fff
    style CORRECT fill:#51cf66,color:#fff
```

> ğŸ’¡ **Life analogy:** Reward â†’ á€á€»á€€á€ºá€á€¼á€„á€ºá€¸ á€•á€»á€±á€¬á€ºá€›á€½á€¾á€„á€ºá€™á€¾á€¯ (candy á€…á€¬á€¸)á‹ Return â†’ á€’á€®á€”á€±á€· á€›á€›á€¾á€­á€á€²á€· á€¡á€€á€»á€­á€¯á€¸á€¡á€•á€¼á€Šá€·á€º (candy á€…á€¬á€¸á€•á€¼á€®á€¸ á€á€™á€ºá€¸á€—á€­á€¯á€€á€ºá€”á€¬)á‹ Value function â†’ á€›á€±á€›á€¾á€Šá€ºá€™á€»á€¾á€±á€¬á€ºá€™á€¾á€”á€ºá€¸ á€á€¶á€…á€¬á€¸á€á€»á€€á€º (candy á€¡á€…á€¬á€¸ á€€á€»á€”á€ºá€¸á€™á€¬á€›á€±á€¸ á€›á€½á€±á€¸á€•á€¼á€®á€¸ long-term happiness)á‹

---

## 3. Random Walk (RW) Environment

Chapter 5 á€™á€¾á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€²á€· primary environment â€” five non-terminal states á€›á€¾á€­á€á€²á€· single-row grid world:

```
Terminal(0) -- State(1) -- State(2) -- State(3) -- State(4) -- State(5) -- Terminal(6)
    R=0                                                                        R=+1
```

- Agent á€€ **left/right** á€Š 50% probability á€–á€¼á€„á€·á€º move á€•á€«á€á€šá€º (random walk)
- Policy evaluation á€›á€²á€· target: $V^\pi(s)$ á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

```mermaid
graph LR
    T0["Terminal<br/>R=0"] --- S1["State 1"]
    S1 --- S2["State 2"]
    S2 --- S3["State 3"]
    S3 --- S4["State 4"]
    S4 --- S5["State 5"]
    S5 --- T6["Terminal<br/>R=+1"]
    
    style T0 fill:#ff6b6b,color:#fff
    style T6 fill:#51cf66,color:#fff
    style S3 fill:#ffd43b,color:#000
```

> ğŸ’¡ RL framework (optimal control) á€€á€­á€¯ control á€™á€›á€”á€­á€¯á€„á€ºá€á€²á€· environment á€™á€¾á€¬ study á€œá€¯á€•á€ºá€•á€¼á€®á€†á€­á€¯á€á€±á€¬á€· â€” deterministic transition + stochastic policy (50% left, 50% right) á€œá€­á€¯á€· á€…á€‰á€ºá€¸á€…á€¬á€¸á€œá€­á€¯á€· á€›á€•á€«á€á€šá€ºá‹ á€›á€œá€’á€ºá€á€°á€•á€«á€á€šá€ºá‹

---

## 4. Monte Carlo (MC) Prediction

### 4.1 MC á Core Idea

Policy $\pi$ á€€á€­á€¯ run á€•á€¼á€®á€¸ episodes á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸á€›á€²á€· **actual returns** á€€á€”á€± average á€šá€°á€•á€¼á€®á€¸ $V^\pi(s)$ á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ â€” straightforward approach:

1. Policy $\pi$ á€–á€¼á€„á€·á€º **full episode** (trajectory) generate
2. Trajectory á€‘á€²á€€ state á€á€­á€¯á€„á€ºá€¸á€¡á€á€½á€€á€º **return** $G_t$ á€€á€­á€¯ calculate
3. Returns á€€á€­á€¯ **average** á€šá€°á€•á€¼á€®á€¸ $V(s)$ á€€á€­á€¯ update

```mermaid
graph TD
    subgraph MC["ğŸ“Š Monte Carlo Prediction"]
        GEN["1. Generate trajectory<br/>Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚‚, ... Sâ‚œ"] 
        CALC["2. Calculate returns<br/>Gâ‚œ for each state visited"]
        UPD["3. Update V(s)<br/>using average returns"]
    end
    
    GEN --> CALC --> UPD
    UPD -->|"repeat for<br/>many episodes"| GEN
    
    style GEN fill:#2196F3,color:#fff
    style CALC fill:#ff922b,color:#fff
    style UPD fill:#4CAF50,color:#fff
```

### 4.2 MC Math

$$V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$$

$$G_{t:T} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$$

Incremental update form â€” episode á€•á€¼á€®á€¸á€á€­á€¯á€„á€ºá€¸:

$$V(s) \leftarrow V(s) + \alpha \left[ G_{t:T} - V(s) \right]$$

- $G_{t:T}$ â€” state $s$ á€€á€”á€± episode á€¡á€†á€¯á€¶á€¸ $T$ á€‘á€­ **actual return** (MC target)
- $\alpha$ â€” learning rate
- $G_{t:T} - V(s)$ â€” **MC error** (actual return - current estimate)

> ğŸ’¡ MC update á€€á€­á€¯ episode **á€¡á€†á€¯á€¶á€¸** $T$ á€™á€¾á€¬á€•á€² á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º â€” $G_{t:T}$ á€€á€­á€¯ calculate á€–á€­á€¯á€· full trajectory á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹

### 4.3 First-Visit vs Every-Visit MC

Episode á€á€…á€ºá€á€¯á€™á€¾á€¬ state $s$ á€€á€­á€¯ **multiple times** visit á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º:

| Method | Rule | IID? |
|---|---|---|
| **First-Visit MC (FVMC)** | State $s$ á€›á€²á€· **á€•á€‘á€™á€†á€¯á€¶á€¸** visit á€€á€”á€± return á€€á€­á€¯á€•á€² á€á€¯á€¶á€¸ | âœ… IID samples |
| **Every-Visit MC (EVMC)** | State $s$ á€›á€²á€· visit **á€¡á€¬á€¸á€œá€¯á€¶á€¸** á€€á€”á€± returns á€á€¯á€¶á€¸ | âŒ Not IID, but still converges |

```mermaid
graph TD
    subgraph FVMC_ex["First-Visit MC Example"]
        direction LR
        T1["Sâ‚€=A"] --> T2["Sâ‚=B"] --> T3["Sâ‚‚=A"] --> T4["Sâ‚ƒ=C"] --> T5["Sâ‚œ=Terminal"]
    end
    
    FVMC_ex -->|"FVMC"| FV["State A: use Gâ‚€ only<br/>(first visit at t=0)"]
    FVMC_ex -->|"EVMC"| EV["State A: use Gâ‚€ AND Gâ‚‚<br/>(both visits)"]
    
    style FV fill:#4CAF50,color:#fff
    style EV fill:#2196F3,color:#fff
```

### 4.4 MC Python Code

```python
def generate_trajectory(pi, env, max_steps=20):
    done, trajectory = False, []
    while not done:
        state = env.reset()
        for t in count():
            action = pi(state)
            next_state, reward, done, _ = env.step(action)
            experience = (state, action, reward, next_state, done)
            trajectory.append(experience)
            if done:
                break
            if t >= max_steps - 1:
                trajectory = []
                break
            state = next_state
    return np.array(trajectory, np.object)

def mc_prediction(pi, env, gamma=1.0, init_alpha=0.5, min_alpha=0.01,
                  alpha_decay_ratio=0.3, n_episodes=500, 
                  max_steps=100, first_visit=True):
    nS = env.observation_space.n
    discounts = np.logspace(0, max_steps, num=max_steps, 
                            base=gamma, endpoint=False)
    alphas = decay_schedule(init_alpha, min_alpha, 
                            alpha_decay_ratio, n_episodes)
    V = np.zeros(nS)
    
    for e in tqdm(range(n_episodes), leave=False):
        trajectory = generate_trajectory(pi, env, max_steps)
        visited = np.zeros(nS, dtype=np.bool)
        for t, (state, _, reward, _, _) in enumerate(trajectory):
            if visited[state] and first_visit:
                continue
            visited[state] = True
            n_steps = len(trajectory[t:])
            G = np.sum(discounts[:n_steps] * trajectory[t:, 2])
            V[state] = V[state] + alphas[e] * (G - V[state])
    return V
```

### 4.5 MC á€›á€²á€· Properties

| Property | Description |
|---|---|
| **Unbiased** âœ… | Actual return $G_{t:T}$ á€€á€­á€¯ target á€¡á€”á€±á€”á€²á€· á€á€¯á€¶á€¸ â€” true value á€€á€­á€¯ á€‘á€­á€™á€¼á€”á€º |
| **High variance** âŒ | $G_{t:T}$ á€‘á€²á€™á€¾á€¬ random events á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ accumulate (actions, transitions, rewards) |
| **Sample inefficient** âŒ | Variance á€€á€¼á€®á€¸á€á€²á€·á€¡á€á€½á€€á€º data á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ á€œá€­á€¯á€¡á€•á€º |
| **Episode-end update** âŒ | Terminal state á€›á€±á€¬á€€á€ºá€™á€¾ update á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º |

---

## 5. Temporal-Difference (TD) Learning

### 5.1 TD á Core Idea

MC á€›á€²á€· main drawback â€” episode á€•á€¼á€®á€¸á€™á€¾ update â€” á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€–á€­á€¯á€· **estimated return** á€€á€­á€¯ á€á€¯á€¶á€¸:

- MC: actual return $G_{t:T}$ á€€á€­á€¯ á€á€¯á€¶á€¸ (episode á€•á€¼á€®á€¸á€¡á€±á€¬á€„á€º á€…á€±á€¬á€„á€·á€º)
- TD: **one-step reward** $R_{t+1}$ + **estimated return** $\gamma V(S_{t+1})$ á€€á€­á€¯ á€á€¯á€¶á€¸ (step á€á€­á€¯á€„á€ºá€¸ update)

$$\underbrace{R_{t+1} + \gamma V(S_{t+1})}_{\text{TD target}} \quad \text{vs} \quad \underbrace{G_{t:T}}_{\text{MC target}}$$

```mermaid
graph TD
    subgraph MC_way["ğŸ“Š MC: Wait until end"]
        MC_S["State Sâ‚œ"] -->|"Aâ‚€"| MC_R1["Râ‚"]
        MC_R1 -->|"Aâ‚"| MC_R2["Râ‚‚"]
        MC_R2 -->|"..."| MC_RT["Râ‚œ"]
        MC_RT --> MC_TERM["Terminal âœ‹<br/>Now calculate Gâ‚œ"]
        MC_TERM -->|"Update V(Sâ‚œ)"| MC_UPD["V(Sâ‚œ) += Î±(Gâ‚œ - V(Sâ‚œ))"]
    end
    
    subgraph TD_way["âš¡ TD: Update every step"]
        TD_S["State Sâ‚œ"] -->|"Aâ‚œ"| TD_R["Râ‚œâ‚Šâ‚, Sâ‚œâ‚Šâ‚"]
        TD_R -->|"Immediately!"| TD_UPD["V(Sâ‚œ) += Î±(Râ‚œâ‚Šâ‚ + Î³V(Sâ‚œâ‚Šâ‚) - V(Sâ‚œ))"]
    end
    
    style MC_TERM fill:#ff6b6b,color:#fff
    style TD_UPD fill:#51cf66,color:#fff
```

### 5.2 TD Math

State-value function á€€á€­á€¯ recursively á€›á€±á€¸á€”á€­á€¯á€„á€º:

$$V^\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s]$$

TD update (every time step):

$$V(S_t) \leftarrow V(S_t) + \alpha \left[ \underbrace{R_{t+1} + \gamma V(S_{t+1})}_{\text{TD target}} - V(S_t) \right]$$

**TD error** (**temporal-difference error**):

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$

á€’á€«á€€á€¼á€±á€¬á€„á€·á€º:

$$V(S_t) \leftarrow V(S_t) + \alpha \cdot \delta_t$$

> ğŸ’¡ **Bootstrapping** â€” estimate á€€á€”á€± estimate á€€á€­á€¯ update á€œá€¯á€•á€ºá€á€¬á€€á€­á€¯ bootstrapping á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹ $V(S_{t+1})$ á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€ºá€€ estimate á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º $R_{t+1}$ (real reward) á€€ "reality á€€á€­á€¯ inject" á€œá€¯á€•á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹

### 5.3 TD Python Code

```python
def td(pi, env, gamma=1.0, init_alpha=0.5, min_alpha=0.01,
       alpha_decay_ratio=0.3, n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS)
    alphas = decay_schedule(init_alpha, min_alpha, 
                            alpha_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        state, done = env.reset(), False
        while not done:
            action = pi(state)
            next_state, reward, done, _ = env.step(action)
            td_target = reward + gamma * V[next_state] * (not done)
            td_error = td_target - V[state]
            V[state] = V[state] + alphas[e] * td_error
            state = next_state
    return V
```

### 5.4 TD á€›á€²á€· Properties

| Property | Description |
|---|---|
| **Biased** âŒ | Estimated return á€á€¯á€¶á€¸ â€” $V(S_{t+1})$ á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€ºá€€ estimate |
| **Low variance** âœ… | Single action, single transition, single reward á€•á€² depend |
| **Sample efficient** âœ… | Variance á€”á€Šá€ºá€¸á€á€²á€·á€¡á€á€½á€€á€º data á€”á€Šá€ºá€¸á€”á€²á€·á€œá€Šá€ºá€¸ learn á€”á€­á€¯á€„á€º |
| **Per-step update** âœ… | Step á€á€­á€¯á€„á€ºá€¸ update â€” episode á€•á€¼á€®á€¸á€¡á€±á€¬á€„á€º á€…á€±á€¬á€„á€·á€ºá€…á€›á€¬ á€™á€œá€­á€¯ |

---

## 6. MC vs TD: Bias-Variance Trade-off

### Core Comparison

| Feature | MC | TD |
|---|---|---|
| **Target** | $G_{t:T}$ (actual return) | $R_{t+1} + \gamma V(S_{t+1})$ (estimated return) |
| **Bias** | Unbiased âœ… | Biased âŒ (bootstrapping) |
| **Variance** | High âŒ (many random events) | Low âœ… (single step) |
| **Update timing** | Episode end | Every step |
| **Bootstrapping** | No | Yes |
| **Need terminal state?** | Yes | No (continuing tasks OK) |

```mermaid
graph LR
    subgraph Spectrum["âš–ï¸ Bias-Variance Trade-off"]
        MC_end["MC<br/>Unbiased<br/>High Variance<br/>Episode-end update"]
        TD_end["TD<br/>Biased<br/>Low Variance<br/>Per-step update"]
    end
    
    MC_end <-->|"Trade-off"| TD_end
    
    style MC_end fill:#ff922b,color:#fff
    style TD_end fill:#2196F3,color:#fff
```

### MC Targets vs TD Targets (RW Environment)

**MC targets** â€” RW environment á€™á€¾á€¬ return á€€ 0 (left terminal) á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º 1 (right terminal) á€•á€² á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º â†’ **high variance** (0 á€”á€¾á€„á€·á€º 1 á€€á€¼á€¬á€¸á€™á€¾á€¬ á€á€¯á€”á€º)

**TD targets** â€” $R_{t+1} + \gamma V(S_{t+1})$ â†’ range á€•á€­á€¯á€á€±á€¸á€•á€±á€™á€šá€·á€º $V(S_{t+1})$ estimate á€€á€¼á€±á€¬á€„á€·á€º **biased** (off-target)

```mermaid
graph TD
    subgraph MC_tgt["MC Targets"]
        MC1["Episode 1: G = 1 (right terminal)"]
        MC2["Episode 2: G = 0 (left terminal)"]
        MC3["Episode 3: G = 1"]
        MC4["Episode 4: G = 0"]
        MC_R["Jumps between 0 and 1!<br/>True value â‰ˆ 0.5<br/>HIGH VARIANCE"]
    end
    
    subgraph TD_tgt["TD Targets"]
        TD1["Step 1: R + Î³V(S') â‰ˆ 0.45"]
        TD2["Step 2: R + Î³V(S') â‰ˆ 0.52"]
        TD3["Step 3: R + Î³V(S') â‰ˆ 0.48"]
        TD_R["Smooth but off-center<br/>BIASED but LOW VARIANCE"]
    end
    
    style MC_R fill:#ff6b6b,color:#fff
    style TD_R fill:#ff922b,color:#fff
```

> ğŸ’¡ MC estimates â†’ **noisy but centered** (true value á€•á€á€ºá€œá€Šá€º á€á€¯á€”á€º)á‹ TD estimates â†’ **smooth but off-target** (initially biased, slowly converge)á‹

---

## 7. N-step TD: MC á€”á€¾á€„á€·á€º TD á€€á€¼á€¬á€¸á€™á€¾á€¬

### 7.1 Motivation

MC â†’ infinite-step (episode á€€á€¯á€”á€ºá€¡á€±á€¬á€„á€º)
TD â†’ 1-step (one interaction)
**N-step TD** â†’ n steps interact á€•á€¼á€®á€¸á€™á€¾ bootstrap

```mermaid
graph LR
    TD1["TD (1-step)<br/>Râ‚ + Î³V(Sâ‚)"] 
    NTD["N-step TD (n steps)<br/>Râ‚ + Î³Râ‚‚ + ... + Î³â¿â»Â¹Râ‚™ + Î³â¿V(Sâ‚™)"]
    MC["MC (âˆ-step)<br/>Râ‚ + Î³Râ‚‚ + ... + Î³áµ€â»Â¹Râ‚œ"]
    
    TD1 -->|"n=1"| NTD
    NTD -->|"n=âˆ"| MC
    
    style TD1 fill:#2196F3,color:#fff
    style NTD fill:#9C27B0,color:#fff
    style MC fill:#ff922b,color:#fff
```

### 7.2 N-step TD Math

N-step return:

$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})$$

N-step TD update:

$$V(S_t) \leftarrow V(S_t) + \alpha \left[ G_{t:t+n} - V(S_t) \right]$$

- $n = 1$ â†’ TD (one-step bootstrapping)
- $n = \infty$ â†’ MC (no bootstrapping, actual return)
- $1 < n < \infty$ â†’ intermediate, often **best performance**

> ğŸ’¡ **Extremist á€™á€–á€¼á€…á€ºá€•á€«á€”á€²á€·!** Intermediate $n$ value á€€ á€¡á€™á€»á€¬á€¸á€¡á€¬á€¸á€–á€¼á€„á€·á€º extremes (TD, MC) á€‘á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

### 7.3 N-step TD Python Code

```python
def ntd(pi, env, gamma=1.0, init_alpha=0.5, min_alpha=0.01,
        alpha_decay_ratio=0.5, n_step=3, n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS)
    alphas = decay_schedule(init_alpha, min_alpha, 
                            alpha_decay_ratio, n_episodes)
    discounts = np.logspace(0, n_step+1, num=n_step+1, 
                            base=gamma, endpoint=False)
    
    for e in tqdm(range(n_episodes), leave=False):
        state, done, path = env.reset(), False, []
        while not done or path is not None:
            path = path[1:]
            while not done and len(path) < n_step:
                action = pi(state)
                next_state, reward, done, _ = env.step(action)
                experience = (state, reward, next_state, done)
                path.append(experience)
                state = next_state
                if done:
                    break
            n = len(path)
            est_state = path[0][0]
            rewards = np.array(path)[:,1]
            partial_return = discounts[:n] * rewards
            bs_val = discounts[-1] * V[next_state] * (not done)
            ntd_target = np.sum(np.append(partial_return, bs_val))
            ntd_error = ntd_target - V[est_state]
            V[est_state] = V[est_state] + alphas[e] * ntd_error
            if len(path) == 1 and path[0][3]:
                path = None
    return V
```

---

## 8. Forward-View TD(Î»): N-step á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸

### 8.1 Motivation

N-step TD á€™á€¾á€¬ $n$ á€˜á€šá€ºá€œá€±á€¬á€€á€ºá€›á€½á€±á€¸á€›á€™á€œá€²? â†’ **á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ weighted combination** á€–á€¼á€„á€·á€º á€•á€±á€«á€„á€ºá€¸á€á€¯á€¶á€¸á€œá€­á€¯á€€á€º!

### 8.2 Lambda Return (Î»-return)

$$G_t^\lambda = (1-\lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1} G_{t:T}$$

$$V(S_t) \leftarrow V(S_t) + \alpha \left[ G_t^\lambda - V(S_t) \right]$$

```mermaid
graph TD
    subgraph FV_TD["ğŸ”€ Forward-View TD(Î») â€” Weighted Combination"]
        W1["1-step return G(t:t+1)<br/>weight: (1-Î»)"]
        W2["2-step return G(t:t+2)<br/>weight: (1-Î»)Î»"]
        W3["3-step return G(t:t+3)<br/>weight: (1-Î»)Î»Â²"]
        WN["n-step return G(t:t+n)<br/>weight: (1-Î»)Î»â¿â»Â¹"]
        WMC["MC return G(t:T)<br/>weight: Î»áµ€â»áµ—â»Â¹"]
    end
    
    W1 --> SUM["Weighted sum = Î»-return Gâ‚œáµ˜"]
    W2 --> SUM
    W3 --> SUM
    WN --> SUM
    WMC --> SUM
    
    style W1 fill:#2196F3,color:#fff
    style W2 fill:#42A5F5,color:#fff
    style W3 fill:#64B5F6,color:#fff
    style WMC fill:#ff922b,color:#fff
    style SUM fill:#4CAF50,color:#fff
```

**Î» á€›á€²á€· Special Cases:**
- $\lambda = 0$ â†’ 1-step return á€€á€­á€¯á€•á€² á€á€¯á€¶á€¸ â†’ **TD(0)** = TD
- $\lambda = 1$ â†’ MC return weight á€¡á€€á€¼á€®á€¸á€†á€¯á€¶á€¸ á€–á€¼á€…á€º â†’ **TD(1)** â‰ˆ MC
- $0 < \lambda < 1$ â†’ **intermediate** â€” all n-step returns á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€º

```mermaid
graph LR
    L0["Î» = 0<br/>TD(0) = TD"] 
    LM["0 < Î» < 1<br/>Weighted mix"]
    L1["Î» = 1<br/>TD(1) â‰ˆ MC"]
    
    L0 --- LM --- L1
    
    style L0 fill:#2196F3,color:#fff
    style LM fill:#9C27B0,color:#fff
    style L1 fill:#ff922b,color:#fff
```

> ğŸ’¡ Forward-view TD(Î») á€›á€²á€· **á€•á€¼á€¿á€”á€¬** â€” episode á€•á€¼á€®á€¸á€™á€¾ update á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º (MC á€œá€­á€¯ á€•á€²) â€” á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯ all n-step returns á€€á€­á€¯ calculate á€–á€­á€¯á€· full trajectory á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹

---

## 9. Backward-View TD(Î»): Eligibility Traces

### 9.1 Core Idea

Forward-view TD(Î») á€›á€²á€· episode-end update á€•á€¼á€¿á€”á€¬á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€–á€­á€¯á€· **eligibility traces** á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯ â€” **every step** update á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º!

**Eligibility trace** $E(s)$ = memory vector â€” recently visited states á€€á€­á€¯ track á€•á€¼á€®á€¸ "á€˜á€šá€º state á€€ update á€›á€–á€­á€¯á€· eligible á€–á€¼á€…á€ºá€á€œá€²" á€†á€¯á€¶á€¸á€–á€¼á€á€º:

```mermaid
graph TD
    subgraph ET["ğŸ“ Eligibility Traces"]
        INIT["Episode start:<br/>E = [0, 0, 0, 0, 0]"]
        VISIT["Visit state 2:<br/>E = [0, 0, 1, 0, 0]"]
        DECAY["Decay by Î³Î»:<br/>E = [0, 0, Î³Î», 0, 0]"]
        VISIT2["Visit state 4:<br/>E = [0, 0, (Î³Î»)Â², 0, 1]"]
    end
    
    INIT --> VISIT --> DECAY --> VISIT2
    
    NOTE["TD error Î´â‚œ á€€á€­á€¯ E vector á€–á€¼á€„á€·á€º multiply á€•á€¼á€®á€¸<br/>eligible states á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ update!"]
    
    style INIT fill:#e0e0e0,color:#000
    style VISIT fill:#4CAF50,color:#fff
    style VISIT2 fill:#2196F3,color:#fff
```

### 9.2 TD(Î») Math

Every episode start:

$$E(s) = 0 \quad \forall s$$

Every time step:

$$E(S_t) \leftarrow E(S_t) + 1$$

$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$

$$V \leftarrow V + \alpha \cdot \delta_t \cdot E$$

$$E \leftarrow \gamma \lambda \cdot E$$

- $E(S_t) + 1$ â€” state á€€á€­á€¯ visit á€œá€¯á€•á€ºá€á€­á€¯á€„á€ºá€¸ eligibility á€€á€­á€¯ increment
- $\delta_t \cdot E$ â€” TD error á€€á€­á€¯ **eligible states á€¡á€¬á€¸á€œá€¯á€¶á€¸** á€€á€­á€¯ apply
- $\gamma \lambda \cdot E$ â€” traces á€€á€­á€¯ decay á€•á€¼á€®á€¸ recent states á€€á€­á€¯ á€•á€­á€¯á€™á€»á€¬á€¸ credit á€•á€±á€¸

> ğŸ’¡ **Core insight:** Recent states â†’ high eligibility â†’ more creditá‹ Old states â†’ decayed eligibility â†’ less creditá‹ á€’á€«á€€ **temporal credit assignment** á€•á€¼á€¿á€”á€¬á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

### 9.3 TD(Î») Python Code

```python
def td_lambda(pi, env, gamma=1.0, init_alpha=0.5, min_alpha=0.01,
              alpha_decay_ratio=0.3, lambda_=0.3, n_episodes=500):
    nS = env.observation_space.n
    V = np.zeros(nS)
    E = np.zeros(nS)
    alphas = decay_schedule(init_alpha, min_alpha, 
                            alpha_decay_ratio, n_episodes)
    
    for e in tqdm(range(n_episodes), leave=False):
        E.fill(0)
        state, done = env.reset(), False
        while not done:
            action = pi(state)
            next_state, reward, done, _ = env.step(action)
            td_target = reward + gamma * V[next_state] * (not done)
            td_error = td_target - V[state]
            E[state] = E[state] + 1
            V = V + alphas[e] * td_error * E
            E = gamma * lambda_ * E
            state = next_state
    return V
```

### 9.4 TD(Î») á Properties

| Î» Value | Equivalent | Behavior |
|---|---|---|
| $\lambda = 0$ | TD(0) = TD | One-step update only; no trace propagation |
| $\lambda = 1$ | TD(1) â‰ˆ MC | All visited states get equal credit (offline) |
| $0 < \lambda < 1$ | Mix | Recent states get more credit; exponential decay |

---

## 10. Algorithm Comparison Summary

### 10.1 Method Spectrum

```mermaid
graph LR
    subgraph Methods["ğŸ“Š Prediction Methods Spectrum"]
        TD["TD(0)<br/>1-step<br/>Biased, Low Var"]
        NTD["N-step TD<br/>n steps<br/>Balanced"]
        TDL["TD(Î»)<br/>Weighted mix<br/>Tunable"]
        MC["MC<br/>âˆ-step<br/>Unbiased, High Var"]
    end
    
    TD -->|"n â†‘"| NTD -->|"generalize"| TDL -->|"Î»â†’1"| MC
    
    style TD fill:#2196F3,color:#fff
    style NTD fill:#9C27B0,color:#fff
    style TDL fill:#4CAF50,color:#fff
    style MC fill:#ff922b,color:#fff
```

### 10.2 Feature Comparison Table

| Feature | MC | TD | N-step TD | TD(Î») |
|---|---|---|---|---|
| **Target** | $G_{t:T}$ | $R_{t+1} + \gamma V(S_{t+1})$ | $G_{t:t+n}$ | $G_t^\lambda$ (weighted) |
| **Bias** | Unbiased | Biased | Moderate | Tunable |
| **Variance** | High | Low | Moderate | Tunable |
| **Update timing** | Episode end | Every step | Every n steps | Every step (backward) |
| **Bootstrapping** | No | Yes | Partial | Tunable |
| **Visual trend** | Noisy, centered | Smooth, off-target | Noisy-ish, centered | Smooth-ish, fast |

### 10.3 Experimental Observations (RW & RNG Environments)

**MC estimates:**
- Running estimates â†’ true value á€•á€á€ºá€œá€Šá€º **noisy jumps** (high variance)
- Targets â†’ exact 0 or 1 (RW environment) â†’ massive spread

**TD estimates:**
- Running estimates â†’ **smooth but off-center** (biased)
- Targets â†’ narrow range, depend on $V(S_{t+1})$ estimate

**N-step TD estimates:**
- MC-like: **noisy and centered**

**TD(Î») estimates:**
- TD-like: **smooth**, but converges faster than TD

```mermaid
graph TD
    subgraph Trends["ğŸ“ˆ Running Estimate Trends"]
        MC_T["MC: Jagged â†•ï¸<br/>Centered around true value"]
        TD_T["TD: Smooth â¡ï¸<br/>Slowly approaches true value"]
        NTD_T["N-step: Moderate jagged<br/>Centered, less noise than MC"]
        TDL_T["TD(Î»): Smooth<br/>Faster than TD, less noise"]
    end
    
    style MC_T fill:#ff922b,color:#fff
    style TD_T fill:#2196F3,color:#fff
    style NTD_T fill:#9C27B0,color:#fff
    style TDL_T fill:#4CAF50,color:#fff
```

---

## 11. Key Equations Summary

| Concept | Equation |
|---|---|
| Return | $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ |
| State-value function | $V^\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$ |
| MC update | $V(S_t) \leftarrow V(S_t) + \alpha [G_{t:T} - V(S_t)]$ |
| TD target | $R_{t+1} + \gamma V(S_{t+1})$ |
| TD error | $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ |
| TD update | $V(S_t) \leftarrow V(S_t) + \alpha \cdot \delta_t$ |
| N-step return | $G_{t:t+n} = \sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n V(S_{t+n})$ |
| Î»-return | $G_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_{t:T}$ |
| Eligibility trace | $E \leftarrow \gamma \lambda E; \quad E(S_t) \leftarrow E(S_t) + 1$ |
| TD(Î») update | $V \leftarrow V + \alpha \cdot \delta_t \cdot E$ |

---

## 12. Russell and Norvig's Gridworld (RNG) Environment

Chapter 5 á€™á€¾á€¬ RW á€¡á€•á€¼á€„á€º RNG environment á€€á€­á€¯á€œá€Šá€ºá€¸ test á€•á€«á€á€šá€º:

```
 0    1    2    3(G +1)
 4    5   [W]   7(H -1)
 8(S) 9   10   11
```

- S = Start (state 8), G = Goal (+1), H = Hole (â€“1), W = Wall
- Transition: 80% success, 10% left-slip, 10% right-slip
- Living penalty: â€“0.04

```mermaid
graph TD
    subgraph RNG["ğŸ—ï¸ Russell & Norvig Gridworld"]
        S0["0"] --- S1["1"] --- S2["2"] --- S3["3 ğŸ¯<br/>+1"]
        S4["4"] --- S5["5"] --- SW["Wall ğŸ§±"] --- S7["7 ğŸ’€<br/>-1"]
        S8["8 ğŸš€<br/>Start"] --- S9["9"] --- S10["10"] --- S11["11"]
    end
    
    S0 --- S4
    S1 --- S5
    S4 --- S8
    S5 --- S9
    S10 --- S2
    S11 --- S7
    
    style S3 fill:#51cf66,color:#fff
    style S7 fill:#ff6b6b,color:#fff
    style S8 fill:#ffd43b,color:#000
    style SW fill:#868e96,color:#fff
```

> ğŸ’¡ RNG environment results: TD á€”á€¾á€„á€·á€º TD(Î») â†’ smoothest curvesá‹ MC á€”á€¾á€„á€·á€º n-step TD â†’ most centered trendsá‹ TD(Î») â†’ TD á€›á€²á€· smoothness + MC á€‘á€€á€º faster convergence!

---

## 13. RL Terminology Recap

| Term | Meaning |
|---|---|
| **Incremental** | Estimates á€€á€­á€¯ iteratively improve (DP, bandits, RL) |
| **Sequential** | Multi-state environment (DP, RL â€” bandits á€™á€Ÿá€¯á€á€º) |
| **Trial-and-error** | Environment interaction á€€á€”á€± learn (bandits, RL â€” DP á€™á€Ÿá€¯á€á€º) |
| **Experience tuple** | $(S_t, A_t, R_{t+1}, S_{t+1})$ |
| **Trajectory** | Sequence of experience tuples (one episode) |
| **Bootstrapping** | Estimate á€€á€”á€± estimate á€€á€­á€¯ update (TD, DP) |
| **Prediction problem** | Policy á€›á€²á€· value function á€€á€­á€¯ estimate (this chapter) |
| **Control problem** | Optimal policy á€€á€­á€¯ á€›á€¾á€¬ (next chapter) |

---

## 14. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

```mermaid
graph TB
    subgraph Summary["ğŸ“š Chapter 5 Summary"]
        PROB["ğŸ”´ Problem<br/>MDP unknown + Sequential + Evaluative<br/>â†’ Prediction problem"]
        MC_S["ğŸ“Š MC<br/>Actual returns<br/>Unbiased, high variance"]
        TD_S["âš¡ TD<br/>Estimated returns<br/>Biased, low variance"]
        NSTEP["ğŸ”€ N-step TD<br/>n steps then bootstrap<br/>Intermediate"]
        TDL_S["ğŸ¯ TD(Î»)<br/>Eligibility traces<br/>Unify MC & TD"]
    end
    
    PROB --> MC_S
    PROB --> TD_S
    MC_S --> NSTEP
    TD_S --> NSTEP
    NSTEP --> TDL_S
    
    style PROB fill:#ff6b6b,color:#fff
    style MC_S fill:#ff922b,color:#fff
    style TD_S fill:#2196F3,color:#fff
    style NSTEP fill:#9C27B0,color:#fff
    style TDL_S fill:#4CAF50,color:#fff
```

### á€¡á€“á€­á€€ á€á€­á€›á€™á€šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. **MC prediction** â€” actual returns á€€á€­á€¯ average á€šá€°á€•á€¼á€®á€¸ $V^\pi(s)$ estimate; unbiased but high variance
2. **TD learning** â€” bootstrapping á€–á€¼á€„á€·á€º step á€á€­á€¯á€„á€ºá€¸ update; biased but low variance and sample efficient
3. **N-step TD** â€” MC á€”á€¾á€„á€·á€º TD á€€á€¼á€¬á€¸á€™á€¾á€¬ spectrum; intermediate $n$ á€€ often best
4. **TD(Î»)** â€” eligibility traces á€–á€¼á€„á€·á€º all n-step returns á€€á€­á€¯ weighted combination; Î»=0 â†’ TD, Î»=1 â†’ MC
5. **Bias-variance trade-off** â€” MC (unbiased, high var) â†” TD (biased, low var) á€€á€­á€¯ tuning á€–á€¼á€„á€·á€º balance
6. **Prediction problem** â€” policy á€›á€²á€· value á€€á€­á€¯ estimate (control problem = next chapter)

> **Chapter 5 â†’ Chapter 6:** Chapter 5 á€™á€¾á€¬ policy value á€€á€­á€¯ **estimate** (prediction) á€œá€¯á€•á€ºá€á€á€ºá€•á€¼á€® â€” Chapter 6 á€™á€¾á€¬ policy á€€á€­á€¯á€•á€² **improve** (control) á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€„á€ºá€šá€°á€•á€«á€™á€šá€ºá‹ MC + TD methods á€€á€­á€¯ policy improvement á€”á€¾á€„á€·á€º á€•á€±á€«á€„á€ºá€¸á€•á€¼á€®á€¸ optimal behavior á€€á€­á€¯ á€›á€¾á€¬á€•á€«á€™á€šá€º!

---

*Summary created from: Miguel Morales - Grokking Deep Reinforcement Learning (2020, Manning Publications) â€” Chapter 5*
