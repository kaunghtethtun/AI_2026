# Chapter 4: Balancing the Gathering and Use of Information - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ **evaluative feedback** á€›á€²á€· challenges á€€á€­á€¯ isolation á€‘á€²á€™á€¾á€¬ á€œá€±á€·á€œá€¬á€•á€«á€á€šá€ºá‹ Agent á€€ MDP á€›á€²á€· dynamics (transition function, reward function) á€€á€­á€¯ **á€™á€á€­**á€˜á€² á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º experience á€€á€”á€± optimal action á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€›á€•á€«á€á€šá€ºá‹ á€’á€«á€€á€­á€¯ **trial-and-error learning** á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph CH3["ğŸ“š Chapter 3: Planning"]
        P3["Agent á€á€Šá€º MDP á€€á€­á€¯ á€á€­á€á€Šá€º<br/>T(s,a,s'), R(s,a,s') known"]
        P3 --> ALG3["PI / VI Algorithms"]
    end
    
    subgraph CH4["ğŸ“˜ Chapter 4: Learning from Evaluative Feedback"]
        P4["Agent á€á€Šá€º MDP á€€á€­á€¯ á€™á€á€­<br/>T, R unknown"]
        P4 --> ALG4["Exploration Strategies<br/>Trial-and-error"]
    end
    
    CH3 -->|"á€’á€® Chapter á€™á€¾á€¬<br/>feedback á€€á€”á€± á€á€„á€ºá€šá€°"| CH4
    
    style CH3 fill:#4CAF50,color:#fff
    style CH4 fill:#2196F3,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Evaluative feedback** á€›á€²á€· challenges á€”á€¾á€„á€·á€º exploration-exploitation trade-off
2. **Multi-armed bandits (MABs)** â€” single-state, single-step environments
3. **Exploration strategies** â€” greedy, epsilon-greedy, decaying, optimistic, softmax, UCB, Thompson sampling

---

## 2. Evaluative Feedback á€›á€²á€· Challenge

### Planning vs Trial-and-Error Learning

| Feature | Chapter 3 (Planning) | Chapter 4 (Trial-and-Error) |
|---|---|---|
| **MDP knowledge** | Agent á€€ T, R á€€á€­á€¯ á€á€­á€á€Šá€º | Agent á€€ T, R á€€á€­á€¯ á€™á€á€­ |
| **Learning method** | Bellman equation á€–á€¼á€„á€·á€º compute | Environment á€”á€¾á€„á€·á€º interact á€•á€¼á€®á€¸ learn |
| **Feedback type** | Sequential | Evaluative (one-shot) |
| **Environment** | Frozen Lake (multi-state) | Multi-armed bandits (single-state) |

### Core Intuition

> Evaluative feedback (+1, +1.345, â€“100 ...) á€›á€á€²á€·á€¡á€á€« agent á€€ underlying MDP á€€á€­á€¯ á€™á€á€­á€á€²á€·á€¡á€á€½á€€á€º maximum reward á€˜á€šá€ºá€œá€±á€¬á€€á€ºá€›á€”á€­á€¯á€„á€ºá€™á€œá€² á€†á€­á€¯á€á€¬ á€™á€á€­á€•á€«á‹ "+1 á€›á€á€šá€º... á€’á€«á€•á€±á€™á€šá€·á€º +100 á€›á€”á€­á€¯á€„á€ºá€á€¬á€œá€Šá€ºá€¸ á€›á€¾á€­á€”á€­á€¯á€„á€ºá€á€šá€º" á€†á€­á€¯á€á€²á€· uncertainty á€€á€¼á€±á€¬á€„á€·á€º agent á€€ **explore** á€œá€¯á€•á€ºá€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€ºá‹

$$\text{Exploration} \xrightarrow{\text{builds}} \text{Knowledge} \xrightarrow{\text{enables}} \text{Effective Exploitation}$$

```mermaid
graph LR
    EXP["ğŸ” Exploration<br/>Information gathering"] -->|"builds"| K["ğŸ“š Knowledge<br/>Better estimates"]
    K -->|"enables"| EXL["ğŸ’° Exploitation<br/>Maximize reward"]
    EXL -.->|"but too much<br/>exploitation = stuck"| EXP
    
    style EXP fill:#ff922b,color:#fff
    style K fill:#4CAF50,color:#fff
    style EXL fill:#2196F3,color:#fff
```

---

## 3. Multi-Armed Bandits (MABs)

### MAB á€†á€­á€¯á€á€¬á€˜á€¬á€œá€²

Multi-armed bandit (MAB) á€†á€­á€¯á€á€¬ RL problem á€›á€²á€· special case á€–á€¼á€…á€ºá€•á€¼á€®á€¸:
- **State space size = 1** (single non-terminal state)
- **Horizon = 1** (single time step per episode)
- **Actions = multiple** (many options, single choice)

```mermaid
graph TD
    subgraph MAB["ğŸ° Multi-Armed Bandit"]
        S["Single State sâ‚€"] -->|"action aâ‚€"| R0["Reward ~ P(R|aâ‚€)"]
        S -->|"action aâ‚"| R1["Reward ~ P(R|aâ‚)"]
        S -->|"action aâ‚‚"| R2["Reward ~ P(R|aâ‚‚)"]
        S -->|"..."| RN["Reward ~ P(R|aâ‚™)"]
    end
    
    style S fill:#ffd43b,color:#000
    style R0 fill:#64B5F6,color:#fff
    style R1 fill:#64B5F6,color:#fff
    style R2 fill:#64B5F6,color:#fff
    style RN fill:#64B5F6,color:#fff
```

### MAB á Math Formulation

$$Q^*(a) = \mathbb{E}[R | A = a]$$

$$V^* = \max_a Q^*(a)$$

$$a^* = \arg\max_a Q^*(a)$$

- $Q^*(a)$ â€” action $a$ á€›á€²á€· true expected reward
- $V^*$ â€” optimal value (best action á€›á€²á€· expected reward)
- $a^*$ â€” optimal action

### MAB Applications
- **Advertising** â€” á€˜á€šá€º ad á€€á€­á€¯ á€•á€¼á€™á€œá€² (click-through rate optimize)
- **Website design** â€” á€˜á€šá€º layout á€€ donations/sales á€•á€­á€¯á€›á€™á€œá€²
- **Medical trials** â€” á€˜á€šá€ºá€†á€±á€¸ á€€ á€•á€­á€¯á€‘á€­á€›á€±á€¬á€€á€ºá€™á€œá€²
- **Recommender systems** â€” á€˜á€šá€º product á€€á€­á€¯ recommend á€œá€¯á€•á€ºá€™á€œá€²

---

## 4. Regret: Exploration á€›á€²á€· Cost

### Total Regret

$$T_{\text{regret}} = \sum_{e=1}^{E} \left[ V^* - Q^*(a_e) \right]$$

- $V^*$ â€” optimal action á€›á€²á€· true expected reward
- $Q^*(a_e)$ â€” episode $e$ á€™á€¾á€¬ á€›á€½á€±á€¸á€œá€­á€¯á€€á€ºá€á€²á€· action á€›á€²á€· true expected reward
- Regret = optimal action á€”á€¾á€„á€·á€º á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€•á€±á€«á€„á€ºá€¸á€œá€’á€º

> ğŸ’¡ Regret á€€á€­á€¯ compute á€œá€¯á€•á€ºá€–á€­á€¯á€· MDP á€€á€­á€¯ á€á€­á€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€ºá‹ Agent á€¡á€á€½á€€á€º á€™á€Ÿá€¯á€á€ºá€˜á€² **strategies á€€á€­á€¯ compare** á€œá€¯á€•á€ºá€–á€­á€¯á€· á€¡á€á€½á€€á€ºá€•á€² á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    subgraph Regret["ğŸ“Š Total Regret"]
        OPT["V* = optimal value"] 
        ACT["Q*(aâ‚‘) = selected action value"]
        DIFF["V* - Q*(aâ‚‘) = per-episode regret"]
    end
    OPT --> DIFF
    ACT --> DIFF
    DIFF -->|"sum over E episodes"| TOTAL["T_regret"]
    
    style OPT fill:#51cf66,color:#fff
    style ACT fill:#ff922b,color:#fff
    style TOTAL fill:#ff6b6b,color:#fff
```

---

## 5. Q-Function Estimation in MABs

MAB environments á€™á€¾á€¬ Q-function estimation á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€º â€” per-action average reward á€•á€² á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

$$Q(a) = \frac{\text{Total reward from action } a}{\text{Number of times action } a \text{ selected}}$$

Incremental update form:

$$Q(a) \leftarrow Q(a) + \frac{1}{N(a)} \left[ R - Q(a) \right]$$

> ğŸ’¡ **Strategy á€¡á€¬á€¸á€œá€¯á€¶á€¸á€™á€¾á€¬ Q-function estimation á€á€°á€á€°á€•á€² á€–á€¼á€…á€ºá€•á€«á€á€šá€º**á‹ á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€ºá€€ Q-function estimates á€€á€­á€¯ **action selection** á€¡á€á€½á€€á€º á€˜á€šá€ºá€œá€­á€¯ á€á€¯á€¶á€¸á€á€œá€² á€†á€­á€¯á€á€¬á€•á€² á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 6. Exploration Strategies

### Strategy á€™á€»á€¬á€¸á Classification

```mermaid
graph TD
    subgraph Strategies["ğŸ¯ Exploration Strategy Families"]
        R["Random Exploration<br/>Randomness á€‘á€Šá€·á€ºá€•á€¼á€®á€¸ explore"]
        O["Optimistic Exploration<br/>Uncertainty á€€á€­á€¯ optimistic á€šá€°á€†"]
        I["Information State-Space<br/>Uncertainty á€€á€­á€¯ state á€‘á€²á€‘á€Šá€·á€º"]
    end
    
    R --> EG["Epsilon-Greedy"]
    R --> DEG["Decaying Îµ-Greedy"]
    R --> SM["Softmax"]
    O --> OI["Optimistic Initialization"]
    O --> UCB["UCB"]
    O --> TS["Thompson Sampling"]
    
    style R fill:#2196F3,color:#fff
    style O fill:#4CAF50,color:#fff
    style I fill:#ff9800,color:#fff
```

---

### 6.1 Pure Exploitation (Greedy Baseline)

á€¡á€™á€¼á€²á€á€™á€ºá€¸ estimated value á€¡á€™á€¼á€„á€·á€ºá€†á€¯á€¶á€¸ action á€€á€­á€¯á€•á€² á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸ â€” exploration **á€œá€¯á€¶á€¸á€á€™á€›á€¾á€­**á‹

$$a = \arg\max_a Q(a)$$

**á€•á€¼á€¿á€”á€¬:** Q-table á€€á€­á€¯ zero á€–á€¼á€„á€·á€º initialize á€œá€¯á€•á€ºá€›á€„á€º á€•á€‘á€™á€†á€¯á€¶á€¸ action á€™á€¾á€¬ stuck á€–á€¼á€…á€ºá€á€½á€¬á€¸á€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    Q0["Q = [0, 0]"] -->|"argmax â†’ aâ‚€ (ties)"| A0["Action 0 selected"]
    A0 -->|"Reward = +1"| Q1["Q = [1, 0]"]
    Q1 -->|"argmax â†’ aâ‚€ always"| A0_again["Action 0 forever! âŒ"]
    
    style A0_again fill:#ff6b6b,color:#fff
```

```python
def pure_exploitation(env, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        action = np.argmax(Q)                    # Always greedy
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ Time horizon á€”á€Šá€ºá€¸á€›á€„á€º (episode 1 á€á€¯á€•á€² á€€á€»á€”á€ºá€›á€„á€º) greedy strategy á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º long-term á€™á€¾á€¬ information gather á€™á€œá€¯á€•á€ºá€›á€„á€º suboptimal á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

### 6.2 Pure Exploration (Random Baseline)

á€¡á€™á€¼á€²á€á€™á€ºá€¸ random action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸ â€” exploitation **á€œá€¯á€¶á€¸á€á€™á€›á€¾á€­**á‹

$$a = \text{random}(\{a_0, a_1, \ldots, a_{n-1}\})$$

```python
def pure_exploration(env, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        action = np.random.randint(len(Q))       # Always random
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ **Exploitation** á€”á€Šá€ºá€¸á€œá€™á€ºá€¸ á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€á€¬ á€›á€¾á€­á€á€Šá€º (best action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸)á‹ **Exploration** á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€€á€á€±á€¬á€· á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ á€›á€¾á€­á€•á€«á€á€šá€º â€” random, count-based, variance-based, uncertainty-based ...

---

### 6.3 Epsilon-Greedy Strategy

Exploit most of the time, explore randomly with probability $\epsilon$:

$$a = \begin{cases} \arg\max_a Q(a) & \text{with probability } 1 - \epsilon \\ \text{random action} & \text{with probability } \epsilon \end{cases}$$

```mermaid
graph TD
    RAND["Random number r ~ U(0,1)"] -->|"r > Îµ"| EXPLOIT["ğŸ¯ Exploit<br/>argmax Q(a)"]
    RAND -->|"r â‰¤ Îµ"| EXPLORE["ğŸ” Explore<br/>random action"]
    
    style EXPLOIT fill:#4CAF50,color:#fff
    style EXPLORE fill:#ff922b,color:#fff
```

```python
def epsilon_greedy(env, epsilon=0.01, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        if np.random.random() > epsilon:
            action = np.argmax(Q)                # Exploit
        else:
            action = np.random.randint(len(Q))   # Explore (includes greedy!)
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ **á€á€á€­á€‘á€¬á€¸á€›á€”á€º:** Exploration step á€™á€¾á€¬ greedy action á€•á€« á€•á€«á€á€„á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ Îµ = 0.5, actions = 2 á€†á€­á€¯á€›á€„á€º non-greedy action á€›á€½á€±á€¸á€–á€­á€¯á€· probability â‰ˆ 25% á€•á€² á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

### 6.4 Decaying Epsilon-Greedy

Early episodes á€™á€¾á€¬ explore á€•á€­á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ á€€á€»á€”á€ºá€á€¬á€™á€¾á€¬ exploit á€•á€­á€¯á€œá€¯á€•á€ºá€–á€­á€¯á€· epsilon á€€á€­á€¯ decay á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á‹

#### Linearly Decaying

```python
def lin_dec_epsilon_greedy(env, init_epsilon=1.0, min_epsilon=0.01, 
                            decay_ratio=0.05, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        decay_episodes = n_episodes * decay_ratio
        epsilon = 1 - e / decay_episodes
        epsilon *= init_epsilon - min_epsilon
        epsilon += min_epsilon
        epsilon = np.clip(epsilon, min_epsilon, init_epsilon)
        if np.random.random() > epsilon:
            action = np.argmax(Q)
        else:
            action = np.random.randint(len(Q))
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

#### Exponentially Decaying

```python
def exp_dec_epsilon_greedy(env, init_epsilon=1.0, min_epsilon=0.01, 
                            decay_ratio=0.1, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    decay_episodes = int(n_episodes * decay_ratio)
    rem_episodes = n_episodes - decay_episodes
    epsilons = 0.01
    epsilons /= np.logspace(-2, 0, decay_episodes)
    epsilons *= init_epsilon - min_epsilon
    epsilons += min_epsilon
    epsilons = np.pad(epsilons, (0, rem_episodes), 'edge')
    for e in range(n_episodes):
        if np.random.random() > epsilons[e]:
            action = np.argmax(Q)
        else:
            action = np.random.randint(len(Q))
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

```mermaid
graph LR
    subgraph Decay["Îµ Decay Strategies"]
        LIN["ğŸ“‰ Linear Decay<br/>Îµ = 1 â†’ 0.01<br/>(straight line)"]
        EXP["ğŸ“‰ Exponential Decay<br/>Îµ = 1 â†’ 0.01<br/>(fast early drop)"]
    end
    
    style LIN fill:#4CAF50,color:#fff
    style EXP fill:#2196F3,color:#fff
```

> ğŸ’¡ **Key Idea:** Early episodes â†’ value estimates á€™á€¾á€”á€ºá€€á€”á€ºá€–á€­á€¯á€· explore á€•á€­á€¯á€œá€¯á€•á€ºá‹ Later episodes â†’ estimates á€€á€±á€¬á€„á€ºá€¸á€œá€¬á€•á€¼á€® á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º exploit á€•á€­á€¯á€œá€¯á€•á€ºá‹

---

### 6.5 Optimistic Initialization

Q-function á€€á€­á€¯ **high value** á€–á€¼á€„á€·á€º initialize á€œá€¯á€•á€ºá€•á€¼á€®á€¸ greedy action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸á‹ Agent á€€ "paradise á€™á€¾á€¬ á€›á€¾á€­á€á€šá€º" á€œá€­á€¯á€· á€šá€¯á€¶á€€á€¼á€Šá€ºá€•á€¼á€®á€¸ explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

$$Q(a) \leftarrow \text{optimistic\_estimate} \quad \forall a$$
$$N(a) \leftarrow \text{initial\_count} \quad \forall a$$

```mermaid
graph TD
    INIT["Q = [1, 1], N = [10, 10]<br/>Optimistic!"] -->|"argmax â†’ aâ‚€"| A0["Action 0 â†’ Reward = 0"]
    A0 -->|"Qâ‚€ = 10/11 â‰ˆ 0.91"| Q1["Q = [0.91, 1]"]
    Q1 -->|"argmax â†’ aâ‚"| A1["Action 1 â†’ Reward = 0"]
    A1 -->|"Qâ‚ = 10/11 â‰ˆ 0.91"| Q2["Q = [0.91, 0.91]"]
    Q2 -->|"both explored!<br/>estimates converge â†“"| CONV["Converge to true values âœ…"]
    
    style INIT fill:#ffd43b,color:#000
    style CONV fill:#51cf66,color:#fff
```

```python
def optimistic_initialization(env, optimistic_estimate=1.0, 
                               initial_count=100, n_episodes=5000):
    Q = np.full((env.action_space.n), optimistic_estimate, dtype=np.float64)
    N = np.full((env.action_space.n), initial_count, dtype=np.float64)
    for e in range(n_episodes):
        action = np.argmax(Q)                    # Always greedy!
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

**Optimism in the face of uncertainty** â€” á€™á€á€­á€á€¬á€€á€­á€¯ á€¡á€€á€±á€¬á€„á€ºá€¸á€†á€¯á€¶á€¸á€œá€­á€¯á€· á€šá€¯á€¶á€€á€¼á€Šá€ºá€á€¼á€„á€ºá€¸ á€–á€¼á€„á€·á€º explore á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Experience á€›á€œá€¬á€á€¬á€”á€²á€· estimates á€€á€»á€†á€„á€ºá€¸á€•á€¼á€®á€¸ true values á€†á€® converge á€•á€«á€á€šá€ºá‹

**á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸:**
1. Maximum reward á€€á€­á€¯ á€€á€¼á€­á€¯á€á€­á€›á€”á€º á€œá€­á€¯á€¡á€•á€ºá€á€¼á€„á€ºá€¸ â€” optimistic value á€€á€­á€¯ environment á€›á€²á€· max reward á€‘á€€á€º á€™á€»á€¬á€¸á€…á€½á€¬ á€™á€¼á€„á€·á€ºá€›á€„á€º converge á€–á€­á€¯á€· á€€á€¼á€¬á€•á€«á€á€šá€º
2. `initial_count` hyperparameter tuning á€œá€­á€¯á€¡á€•á€ºá€á€¼á€„á€ºá€¸

---

### 6.6 Softmax Strategy

Q-value estimates á€€á€­á€¯ probability distribution á€¡á€–á€¼á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸ â€” higher estimated value á€›á€¾á€­á€á€²á€· actions á€€á€­á€¯ á€•á€­á€¯á€›á€½á€±á€¸á€•á€«á€á€šá€º:

$$P(a) = \frac{e^{Q(a)/\tau}}{\sum_{a'} e^{Q(a')/\tau}}$$

| Temperature $\tau$ | Behavior |
|---|---|
| $\tau \to \infty$ | Uniform random (pure exploration) |
| $\tau \to 0$ | Greedy (pure exploitation) |
| Moderate $\tau$ | Balanced exploration-exploitation |

```mermaid
graph TD
    subgraph Softmax["ğŸ² Softmax Strategy"]
        Q["Q-values"] -->|"Ã· Ï„"| SCALED["Scaled Q/Ï„"]
        SCALED -->|"exp()"| EXP["e^(Q/Ï„)"]
        EXP -->|"normalize"| PROB["P(a) = probability distribution"]
        PROB -->|"sample"| ACTION["Selected action"]
    end
    
    TAU_HIGH["Ï„ â†’ âˆ<br/>Uniform"] -.-> PROB
    TAU_LOW["Ï„ â†’ 0<br/>Greedy"] -.-> PROB
    
    style Q fill:#4CAF50,color:#fff
    style ACTION fill:#ffd43b,color:#000
```

```python
def softmax(env, init_temp=1000.0, min_temp=0.01, 
            decay_ratio=0.04, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        decay_episodes = n_episodes * decay_ratio
        temp = 1 - e / decay_episodes
        temp *= init_temp - min_temp
        temp += min_temp
        temp = np.clip(temp, min_temp, init_temp)
        
        scaled_Q = Q / temp
        norm_Q = scaled_Q - np.max(scaled_Q)     # Numeric stability
        exp_Q = np.exp(norm_Q)
        probs = exp_Q / np.sum(exp_Q)
        
        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ Epsilon-greedy á€”á€¾á€„á€·á€º **á€€á€½á€¬á€á€¼á€¬á€¸á€á€»á€€á€º**: epsilon-greedy á€€ explore á€œá€¯á€•á€ºá€›á€„á€º uniform random á€•á€² á€›á€½á€±á€¸á€•á€«á€á€šá€ºá‹ Softmax á€€á€á€±á€¬á€· Q-value estimates á€€á€­á€¯ proportion á€œá€­á€¯á€€á€ºá€•á€¼á€®á€¸ action á€›á€½á€±á€¸á€•á€«á€á€šá€ºá‹ Low-value actions á€€á€­á€¯ explore á€œá€¯á€•á€ºá€–á€­á€¯á€· chance á€”á€Šá€ºá€¸á€•á€«á€á€šá€ºá‹

---

### 6.7 Upper Confidence Bound (UCB)

Q-value estimate + **uncertainty bonus** á€•á€±á€«á€„á€ºá€¸á€•á€¼á€®á€¸ action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸ â€” uncertain actions á€€á€­á€¯ explore á€–á€­á€¯á€· encourage:

$$a_e = \arg\max_a \left[ Q(a) + U_e(a) \right]$$

$$U_e(a) = \sqrt{\frac{c \cdot \ln(e)}{N(a)}}$$

- $Q(a)$ â€” estimated value (exploitation term)
- $U_e(a)$ â€” uncertainty bonus (exploration term)
- $c$ â€” exploration scale hyperparameter
- $N(a)$ â€” action $a$ á€€á€­á€¯ select á€œá€¯á€•á€ºá€á€²á€·á€á€²á€· á€¡á€€á€¼á€­á€™á€ºá€›á€±

```mermaid
graph LR
    subgraph UCB_eq["UCB Action Selection"]
        Q_term["Q(a)<br/>Estimated value<br/>(Exploitation)"]
        U_term["U(a) = âˆš(cÂ·ln(e)/N(a))<br/>Uncertainty bonus<br/>(Exploration)"]
        SUM["Q(a) + U(a)"]
        ACTION["argmax â†’ Selected action"]
    end
    Q_term --> SUM
    U_term --> SUM
    SUM --> ACTION
    
    style Q_term fill:#4CAF50,color:#fff
    style U_term fill:#ff922b,color:#fff
    style ACTION fill:#ffd43b,color:#000
```

**UCB á Intuition:**
- $N(a)$ **á€”á€Šá€ºá€¸á€›á€„á€º** (action á€€á€­á€¯ á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸á€•á€² try á€á€²á€·á€›á€„á€º) â†’ $U(a)$ **á€€á€¼á€®á€¸á€•á€«á€á€šá€º** â†’ explore á€–á€­á€¯á€· encourage
- $N(a)$ **á€™á€»á€¬á€¸á€›á€„á€º** (action á€€á€­á€¯ á€¡á€™á€»á€¬á€¸á€€á€¼á€®á€¸ try á€á€²á€·á€›á€„á€º) â†’ $U(a)$ **á€„á€šá€ºá€•á€«á€á€šá€º** â†’ exploit á€–á€­á€¯á€· encourage
- Attempts á€€á€½á€¬á€»á€á€¬á€¸á€™á€¾á€¯ (0 vs 100) â–º early episodes á€™á€¾á€¬ bonus á€•á€­á€¯á€€á€¼á€®á€¸; (100 vs 200) â–º later episodes á€™á€¾á€¬ bonus á€„á€šá€ºá€œá€¬

```python
def upper_confidence_bound(env, c=2, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        if e < len(Q):
            action = e                           # Try each action once first
        else:
            U = np.sqrt(c * np.log(e) / N)       # Uncertainty bonus
            action = np.argmax(Q + U)             # Value + exploration bonus
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ **Optimistic initialization vs UCB:** Optimistic initialization á€€ blindly optimistic á€–á€¼á€…á€ºá€•á€¼á€®á€¸ max reward á€€á€­á€¯ á€€á€¼á€­á€¯á€á€­á€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€ºá‹ UCB á€€á€á€±á€¬á€· **realistic optimism** â€” uncertainty á€€á€­á€¯ statistical technique á€–á€¼á€„á€·á€º measure á€•á€¼á€®á€¸ exploration bonus á€¡á€–á€¼á€…á€º á€•á€±á€«á€„á€ºá€¸á€‘á€Šá€·á€ºá€•á€«á€á€šá€ºá‹

---

### 6.8 Thompson Sampling

Bayesian approach â€” Q-value á€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€®á€€á€­á€¯ **probability distribution** (Gaussian) á€¡á€–á€¼á€…á€º model á€œá€¯á€•á€ºá€•á€¼á€®á€¸ sample á€šá€°á€€á€¬ action á€›á€½á€±á€¸á€á€¼á€„á€ºá€¸:

$$\tilde{Q}(a) \sim \mathcal{N}\left(Q(a), \frac{\alpha}{\sqrt{N(a)} + \beta}\right)$$

$$a = \arg\max_a \tilde{Q}(a)$$

```mermaid
graph TD
    subgraph TS["ğŸ² Thompson Sampling"]
        G0["Action 0<br/>ğ’©(Î¼â‚€, Ïƒâ‚€)<br/>mean=0.6, wide"] 
        G1["Action 1<br/>ğ’©(Î¼â‚, Ïƒâ‚)<br/>mean=0.3, narrow"]
    end
    
    G0 -->|"sample sâ‚€"| COMP["Compare samples"]
    G1 -->|"sample sâ‚"| COMP
    COMP -->|"argmax"| SEL["Selected action"]
    
    NOTE["Wide Ïƒ â†’ uncertain â†’ more varied samples<br/>Narrow Ïƒ â†’ confident â†’ samples near mean"]
    
    style G0 fill:#ff922b,color:#fff
    style G1 fill:#4CAF50,color:#fff
    style SEL fill:#ffd43b,color:#000
```

**Thompson Sampling á Intuition:**
- **Uncertain action** (Ïƒ á€€á€¼á€®á€¸á€á€Šá€º) â†’ samples á€€ varied á€–á€¼á€…á€ºá€•á€¼á€®á€¸ highest sample á€–á€¼á€…á€ºá€–á€­á€¯á€· chance á€›á€¾á€­ â†’ explore
- **Confident action** (Ïƒ á€„á€šá€ºá€á€Šá€º) â†’ samples á€€ mean á€”á€¬á€¸á€™á€¾á€¬ â†’ mean á€€á€¼á€®á€¸á€›á€„á€º exploit

```python
def thompson_sampling(env, alpha=1, beta=0, n_episodes=5000):
    Q = np.zeros((env.action_space.n))
    N = np.zeros((env.action_space.n))
    for e in range(n_episodes):
        samples = np.random.normal(
            loc=Q, 
            scale=alpha / (np.sqrt(N) + beta))   # Sample from Gaussians
        action = np.argmax(samples)               # Pick highest sample
        _, reward, _, _ = env.step(action)
        N[action] += 1
        Q[action] = Q[action] + (reward - Q[action]) / N[action]
    return Q
```

> ğŸ’¡ **UCB** á€€ **frequentist** approach (minimal assumptions)áŠ **Thompson Sampling** á€€ **Bayesian** approach (prior distributions á€¡á€á€¯á€¶á€¸á€•á€¼á€¯)á‹ Beta distribution á€€á€­á€¯á€œá€Šá€ºá€¸ prior á€¡á€–á€¼á€…á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

## 7. Bandit Environments

### 7.1 Bandit Slippery Walk (BSW)

Chapter 3 á€€á€”á€± á€•á€¼á€”á€ºá€œá€¬á€á€²á€· environment â€” single-state, two-armed bandit:

```
H(0) -- S(1) -- G(2)
```

- Action 0 â†’ reward +1 probability 0.2
- Action 1 â†’ reward +1 probability 0.8
- Agent á€€ á€’á€® probabilities á€€á€­á€¯ á€™á€á€­á€•á€«

```mermaid
graph LR
    H["H (0)<br/>Reward = 0"] 
    S["S (1)<br/>Start"]
    G["G (2)<br/>Reward = +1"]
    
    S -->|"aâ‚€: p=0.8 â†’ H, p=0.2 â†’ G"| H
    S -->|"aâ‚: p=0.2 â†’ H, p=0.8 â†’ G"| G
    
    style H fill:#ff6b6b,color:#fff
    style S fill:#ffd43b,color:#000
    style G fill:#51cf66,color:#fff
```

### 7.2 Two-Armed Bernoulli Bandits

$$R(a_0) \sim \text{Bernoulli}(\alpha) \quad (+1 \text{ with prob } \alpha, \text{ else } 0)$$
$$R(a_1) \sim \text{Bernoulli}(\beta) \quad (+1 \text{ with prob } \beta, \text{ else } 0)$$

- $\alpha$ á€”á€¾á€„á€·á€º $\beta$ independent probabilities á€–á€¼á€…á€ºá€•á€«á€á€šá€º (BSW á€”á€¾á€„á€·á€º á€€á€½á€¬á€•á€«á€á€šá€º)

### 7.3 10-Armed Gaussian Bandits

$$q^*(a_k) \sim \mathcal{N}(0, 1) \quad \text{for } k = 0, 1, \ldots, 9$$

$$R_e(a_k) \sim \mathcal{N}(q^*(a_k), 1)$$

- Arm á€á€­á€¯á€„á€ºá€¸ reward á€•á€±á€¸á€•á€«á€á€šá€º (Bernoulli á€”á€¾á€„á€·á€º á€€á€½á€¬á€•á€«á€á€šá€º)
- Reward á€€ Gaussian distribution á€€á€”á€± sample á€šá€°á€•á€«á€á€šá€º

```mermaid
graph TD
    subgraph GB["ğŸ° 10-Armed Gaussian Bandit"]
        A0["Arm 0<br/>q*(aâ‚€) ~ ğ’©(0,1)"]
        A1["Arm 1<br/>q*(aâ‚) ~ ğ’©(0,1)"]
        A2["..."]
        A9["Arm 9<br/>q*(aâ‚‰) ~ ğ’©(0,1)"]
    end
    
    A0 -->|"R ~ ğ’©(q*(aâ‚€), 1)"| R0["Reward varies!"]
    A9 -->|"R ~ ğ’©(q*(aâ‚‰), 1)"| R9["Reward varies!"]
    
    style A0 fill:#4CAF50,color:#fff
    style A9 fill:#2196F3,color:#fff
```

---

## 8. Experimental Results

### Two-Armed Bernoulli Bandits Results

| Strategy | Performance |
|---|---|
| **Optimistic 1.0, count=10** | Highest mean episode reward |
| **Exp Îµ-greedy 1.0** | Low total regret |
| **Softmax âˆ** | Best across all experiments |
| *Pure exploitation* | *Linear total regret (bad!)* |
| *Pure exploration* | *Linear total regret (bad!)* |

### 10-Armed Gaussian Bandits Results

| Strategy | Performance |
|---|---|
| **UCB 0.2, 0.5** | Top performers, lowest regret |
| **Thompson 0.5** | Competitive with UCB |
| **Lin Îµ-greedy 1.0** | Best among simple strategies |
| **Softmax âˆ** | Less effective in 10-arm setting |

```mermaid
graph TB
    subgraph Results["ğŸ“Š Strategy Comparison"]
        direction LR
        SIMPLE["Simple Strategies<br/>Îµ-greedy, decaying<br/>optimistic"] 
        ADVANCED["Advanced Strategies<br/>softmax, UCB,<br/>Thompson sampling"]
    end
    
    SIMPLE -->|"2-arm bandits"| R1["Both perform well"]
    ADVANCED -->|"10-arm bandits"| R2["Advanced strategies<br/>clearly better!"]
    
    style SIMPLE fill:#ff922b,color:#fff
    style ADVANCED fill:#4CAF50,color:#fff
    style R2 fill:#51cf66,color:#fff
```

> ğŸ’¡ **Key Finding:** Advanced strategies (UCB, Thompson) á€€ environment complexity (more arms) á€á€­á€¯á€¸á€œá€¬á€á€¬á€”á€²á€· simple strategies á€‘á€€á€º á€•á€¼á€á€ºá€á€¬á€¸á€…á€½á€¬ outperform á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

---

## 9. Strategy Comparison Summary

| Strategy | Type | Explore | Exploit | Key Feature |
|---|---|---|---|---|
| **Pure Exploitation** | Baseline | âŒ | âœ… Always | First action á€™á€¾á€¬ stuck |
| **Pure Exploration** | Baseline | âœ… Always | âŒ | Q converges but never used |
| **Îµ-Greedy** | Random | Îµ prob | 1-Îµ prob | Simple, effective |
| **Decaying Îµ-Greedy** | Random | Highâ†’Low | Lowâ†’High | Time-aware exploration |
| **Optimistic Init** | Optimistic | Implicit | Greedy | Needs max reward knowledge |
| **Softmax** | Random | Proportional | Proportional | Q-value based preference |
| **UCB** | Optimistic | Uncertainty bonus | Q-value | Statistical uncertainty |
| **Thompson Sampling** | Bayesian | Distribution sampling | Mean value | Full posterior maintenance |

---

## 10. Key Equations Summary

| Concept | Equation |
|---|---|
| MAB Q-function | $Q^*(a) = \mathbb{E}[R \mid A = a]$ |
| Optimal value | $V^* = \max_a Q^*(a)$ |
| Q-update (incremental) | $Q(a) \leftarrow Q(a) + \frac{1}{N(a)}[R - Q(a)]$ |
| Total regret | $T = \sum_{e=1}^{E} [V^* - Q^*(a_e)]$ |
| Îµ-Greedy | $a = \arg\max Q$ w.p. $1 - \epsilon$; random w.p. $\epsilon$ |
| Softmax | $P(a) = \frac{e^{Q(a)/\tau}}{\sum_{a'} e^{Q(a')/\tau}}$ |
| UCB | $a = \arg\max_a \left[ Q(a) + \sqrt{\frac{c \ln e}{N(a)}} \right]$ |
| Thompson Sampling | $\tilde{Q}(a) \sim \mathcal{N}\left(Q(a), \frac{\alpha}{\sqrt{N(a)} + \beta}\right)$ |

---

## 11. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

```mermaid
graph TB
    subgraph Summary["ğŸ“š Chapter 4 Summary"]
        PROBLEM["ğŸ”´ Problem<br/>MDP unknown â†’ evaluative feedback only"]
        TRADEOFF["âš–ï¸ Trade-off<br/>Exploration vs Exploitation"]
        STRATEGIES["ğŸ¯ Strategies<br/>Îµ-greedy, softmax, UCB,<br/>Thompson sampling"]
        ENV["ğŸ° Environments<br/>BSW, Bernoulli, Gaussian bandits"]
    end
    
    PROBLEM --> TRADEOFF
    TRADEOFF --> STRATEGIES
    STRATEGIES --> ENV
    
    style PROBLEM fill:#ff6b6b,color:#fff
    style TRADEOFF fill:#ff922b,color:#fff
    style STRATEGIES fill:#4CAF50,color:#fff
    style ENV fill:#2196F3,color:#fff
```

### á€¡á€“á€­á€€ á€á€­á€›á€™á€šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸:

1. **Evaluative feedback** â€” Agent á€€ MDP á€€á€­á€¯ á€™á€á€­á€á€²á€·á€¡á€á€½á€€á€º á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º experience á€€á€”á€± learn á€›á€•á€«á€á€šá€º
2. **Exploration-exploitation trade-off** â€” Information gathering á€”á€¾á€„á€·á€º reward maximization á€€á€­á€¯ balance á€œá€¯á€•á€ºá€›á€•á€«á€á€šá€º
3. **MABs** â€” Single-state environments á€–á€¼á€…á€ºá€•á€¼á€®á€¸ evaluative feedback challenge á€€á€­á€¯ isolate á€œá€¯á€•á€ºá€•á€«á€á€šá€º
4. **Regret** â€” Exploration á€›á€²á€· cost á€€á€­á€¯ measure á€•á€¼á€®á€¸ strategies á€€á€­á€¯ compare á€œá€¯á€•á€ºá€•á€«á€á€šá€º
5. **Random strategies** â€” Îµ-greedy, decaying Îµ-greedy, softmax
6. **Optimistic strategies** â€” Optimistic initialization, UCB
7. **Bayesian strategies** â€” Thompson sampling
8. **Advanced strategies** (UCB, Thompson) á€€ complex environments á€™á€¾á€¬ simple strategies á€‘á€€á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€º

> **Chapter 4 vs Chapter 5:** Chapter 4 á€™á€¾á€¬ evaluative feedback challenge á€€á€­á€¯ single-state (bandit) environments á€™á€¾á€¬ isolate á€•á€¼á€®á€¸ study á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ Chapter 5 á€™á€¾á€¬ sequential + evaluative feedback á€•á€±á€«á€„á€ºá€¸á€•á€¼á€®á€¸ multi-state environments á€™á€¾á€¬ agents á€›á€²á€· behaviors á€€á€­á€¯ evaluate á€œá€¯á€•á€ºá€–á€­á€¯á€· á€á€„á€ºá€šá€°á€•á€«á€™á€šá€ºá‹
