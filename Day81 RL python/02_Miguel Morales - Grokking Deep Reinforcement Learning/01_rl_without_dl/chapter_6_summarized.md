# Chapter 6: Improving Agents' Behaviors - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ **control problem** á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€–á€­á€¯á€· agents á€á€½á€±á€€á€­á€¯ optimal policies á€›á€¾á€¬á€–á€½á€±á€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º á€á€„á€ºá€€á€¼á€¬á€¸á€•á€«á€á€šá€ºá‹ Chapter 5 á€™á€¾á€¬ prediction problem (value function estimation) á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€²á€·á€•á€¼á€®á€¸, á€’á€® Chapter á€™á€¾á€¬á€á€±á€¬á€· agent á€á€½á€±á€€ trial-and-error learning á€–á€¼á€„á€·á€º arbitrary policies á€€á€”á€± optimal policies á€†á€® á€›á€±á€¬á€€á€ºá€¡á€±á€¬á€„á€º á€á€á€ºá€™á€¼á€±á€¬á€€á€ºá€•á€«á€á€šá€ºá‹
á€€á€½á€”á€ºá€–á€¼á€°á€¸á€›á€¾á€•á€ºá€›á€²á€· á€…á€€á€¬á€¸
```bash
"á€•á€”á€ºá€¸á€á€­á€¯á€„á€ºá€€á€­á€¯ á€™á€›á€±á€¬á€€á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬á€·á€˜á€°á€¸á€†á€­á€¯á€á€¬ á€‘á€„á€ºá€›á€¾á€¬á€¸á€”á€±á€á€²á€·á€¡á€á€«áŠ á€•á€”á€ºá€¸á€á€­á€¯á€„á€ºá€€á€­á€¯ á€™á€•á€¼á€„á€ºá€•á€«á€”á€²á€·áŠ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€›á€™á€šá€·á€º á€¡á€†á€„á€·á€ºá€á€½á€±á€€á€­á€¯á€•á€² á€•á€¼á€„á€ºá€†á€„á€ºá€•á€«"á‹
```

```mermaid
graph TD
    subgraph CH5["ğŸ“˜ Chapter 5: Prediction Problem"]
        P5["Value Function Estimation<br/>V(s) or Q(s,a) á€€á€­á€¯ estimate"]
        P5 --> M5["MC / TD / n-step / TD(Î»)"]
    end
    
    subgraph CH6["ğŸ“— Chapter 6: Control Problem"]
        P6["Policy Optimization<br/>Optimal policy Ï€* á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±"]
        P6 --> M6["MC Control / SARSA<br/>Q-learning / Double Q-learning"]
    end
    
    CH5 -->|"prediction + improvement<br/>= GPI pattern"| CH6
    
    style CH5 fill:#2196F3,color:#fff
    style CH6 fill:#4CAF50,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Generalized Policy Iteration (GPI)** pattern
2. **MC Control** â€” episode á€•á€¼á€®á€¸á€™á€¾ policy improve
3. **SARSA** â€” step á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ on-policy improvement
4. **Q-learning** â€” off-policy optimal policy learning
5. **Double Q-learning** â€” maximization bias á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸

---

## 2. Prediction Problem vs Control Problem

### Terminology Clarification

| Term | Meaning |
|---|---|
| **Prediction Problem** | Policy á€›á€²á€· value function á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ |
| **Control Problem** | Optimal policy á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€á€¼á€„á€ºá€¸ |
| **Policy Evaluation** | Prediction problem á€€á€­á€¯ solve á€œá€¯á€•á€ºá€á€±á€¬ algorithms |
| **Policy Improvement** | Policy á€€á€­á€¯ greedier á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º improve á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ |

> ğŸ’¡ Control problem á€€á€­á€¯ solve á€œá€¯á€•á€ºá€–á€­á€¯á€· policy evaluation + policy improvement á€€á€­á€¯ **combine** á€œá€¯á€•á€ºá€›á€•á€«á€á€šá€ºá‹ Policy improvement á€á€…á€ºá€á€¯á€á€Šá€ºá€¸ á€”á€²á€·á€™á€›á€•á€«á‹

---

## 3. Generalized Policy Iteration (GPI)

### GPI Pattern

GPI á€†á€­á€¯á€á€¬ policy evaluation á€”á€¾á€„á€·á€º policy improvement á€€á€­á€¯ **á€¡á€•á€¼á€”á€ºá€¡á€œá€¾á€”á€º** interact á€œá€¯á€•á€ºá€•á€¼á€®á€¸ progressively optimal policy á€†á€® á€›á€½á€±á€·á€á€½á€¬á€¸á€á€²á€· pattern á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    PE["ğŸ“Š Policy Evaluation<br/>Q â‰ˆ qÏ€"] -->|"estimates value<br/>of current policy"| PI["ğŸ¯ Policy Improvement<br/>Ï€ = Îµ-greedy(Q)"]
    PI -->|"creates better<br/>policy"| PE
    
    PE -.->|"iteratively<br/>converge"| OPT["â­ Optimal<br/>Ï€* , q*"]
    PI -.->|"iteratively<br/>converge"| OPT
    
    style PE fill:#ff922b,color:#fff
    style PI fill:#2196F3,color:#fff
    style OPT fill:#4CAF50,color:#fff
```

### GPI á á€¡á€“á€­á€€ Insight

$$\text{Policy Evaluation} \xrightarrow{\text{makes V consistent with } \pi} \text{Policy Improvement} \xrightarrow{\text{makes } \pi \text{ greedy w.r.t. V}} \text{Better Policy}$$

- **Policy evaluation** â€” current policy á€›á€²á€· value function á€€á€­á€¯ accurate á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º estimate
- **Policy improvement** â€” estimated value function á€€á€­á€¯ á€¡á€á€¼á€±á€á€¶á policy á€€á€­á€¯ greedier á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º update
- á€’á€® two processes á€€á€­á€¯ repeatedly alternate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º optimal policy á€†á€®á€›á€±á€¬á€€á€ºá€•á€«á€á€šá€º

---

## 4. MC Control vs SARSA vs Q-learning Comparison

### Key Changes from Prediction to Control

Control problem á€€á€­á€¯ solve á€œá€¯á€•á€ºá€–á€­á€¯á€· changes á‚ á€á€¯ á€œá€­á€¯á€•á€«á€á€šá€º:

1. **V(s) á€¡á€…á€¬á€¸ Q(s,a) á€€á€­á€¯ estimate á€œá€¯á€•á€ºá€›á€™á€šá€º** â€” V-function á€”á€²á€· MDP á€™á€›á€¾á€­á€˜á€² best action á€˜á€šá€ºá€Ÿá€¬á€œá€² á€†á€­á€¯á€á€¬ á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€œá€­á€¯á€·á€™á€›á€•á€«
2. **Exploration á€œá€¯á€•á€ºá€›á€™á€šá€º** â€” greedy policy á€á€¬ follow á€á€²á€·á€›á€„á€º better actions á€€á€­á€¯ discover á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«

```mermaid
graph TD
    subgraph MC_CONTROL["MC Control"]
        MC1["Monte Carlo Prediction<br/>(full episode)"] --> MC2["Îµ-greedy Improvement"]
        MC2 -->|"next episode"| MC1
    end
    
    subgraph SARSA_CTRL["SARSA"]
        S1["TD Prediction<br/>(single step, on-policy)"] --> S2["Îµ-greedy Improvement"]
        S2 -->|"next step"| S1
    end
    
    subgraph Q_CTRL["Q-learning"]
        Q1["TD Prediction<br/>(single step, off-policy)"] --> Q2["Îµ-greedy Improvement"]
        Q2 -->|"next step"| Q1
    end
    
    style MC1 fill:#ff922b,color:#fff
    style S1 fill:#2196F3,color:#fff
    style Q1 fill:#4CAF50,color:#fff
```

### Algorithms Comparison Table

| Feature | MC Control | SARSA | Q-learning | Double Q-learning |
|---|---|---|---|---|
| **Policy Evaluation** | MC (full episode) | TD (one-step) | TD (one-step) | TD (one-step) |
| **Update Timing** | Episode á€•á€¼á€®á€¸á€™á€¾ | Step á€á€­á€¯á€„á€ºá€¸ | Step á€á€­á€¯á€„á€ºá€¸ | Step á€á€­á€¯á€„á€ºá€¸ |
| **On/Off-policy** | On-policy | On-policy | Off-policy | Off-policy |
| **Bootstrapping** | No | Yes | Yes | Yes |
| **Overestimation** | Low | Low | High | Mitigated |

---

## 5. Slippery Walk Seven (SWS) Environment

á€’á€® Chapter á€›á€²á€· experiments á€á€½á€±á€¡á€á€½á€€á€º **Slippery Walk Seven (SWS)** environment á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    T0["â˜ ï¸<br/>Terminal<br/>State 0"] --- S1["S1"] --- S2["S2"] --- S3["S3"] --- S4["S4"] --- S5["S5"] --- S6["S6"] --- S7["S7"] --- T8["ğŸ†<br/>Terminal<br/>State 8<br/>+1"]
    
    style T0 fill:#ef5350,color:#fff
    style T8 fill:#4CAF50,color:#fff
    style S1 fill:#64B5F6,color:#fff
    style S2 fill:#64B5F6,color:#fff
    style S3 fill:#64B5F6,color:#fff
    style S4 fill:#64B5F6,color:#fff
    style S5 fill:#64B5F6,color:#fff
    style S6 fill:#64B5F6,color:#fff
    style S7 fill:#64B5F6,color:#fff
```

**SWS Environment Properties:**
- Non-terminal states: 7 (states 1-7)
- Terminal states: 0 (left) á€”á€¾á€„á€·á€º 8 (right, reward +1)
- Actions: Left (0), Right (1)
- **Slippery**: 50% intended direction, 33% stay, 17% opposite direction
- Agent á€€ state IDs á€”á€¾á€„á€·á€º action numbers á€€á€­á€¯á€á€¬ á€™á€¼á€„á€ºá€›á€•á€¼á€®á€¸ environment á€›á€²á€· structure á€€á€­á€¯ á€™á€á€­á€•á€«

---

## 6. Monte Carlo Control

### Algorithm Overview

MC Control á€á€Šá€º MC prediction á€€á€­á€¯ policy evaluation á€¡á€á€½á€€á€º á€á€¯á€¶á€¸á€•á€¼á€®á€¸ decaying Îµ-greedy á€€á€­á€¯ policy improvement á€¡á€á€½á€€á€º á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ Episode á€á€…á€ºá€á€¯á€•á€¼á€®á€¸á€á€­á€¯á€„á€ºá€¸ policy á€€á€­á€¯ improve á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

### MC Control Update Rule

$$Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ G_t - Q(s, a) \Big]$$

where $G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$ (full return from time step $t$)

### Key Components

```python
# Decay Schedule (exponential)
values = np.logspace(log_start, 0, decay_steps, base=log_base)[::-1]
values = (values - values.min()) / (values.max() - values.min())
values = (init_value - min_value) * values + min_value

# Epsilon-greedy action selection
select_action = lambda state, Q, epsilon: \
    np.argmax(Q[state]) if np.random.random() > epsilon \
    else np.random.randint(len(Q[state]))

# MC Control Update (inside episode loop)
G = np.sum(discounts[:n_steps] * trajectory[t:, 2])
Q[state][action] += alphas[e] * (G - Q[state][action])
```

> ğŸ’¡ MC Control á€á€Šá€º episode á€•á€¼á€®á€¸á€™á€¾á€á€¬ update á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€²á€·á€¡á€á€½á€€á€º **offline (episode-to-episode)** method á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Variance á€™á€¼á€„á€·á€ºá€•á€±á€™á€šá€·á€º bias á€”á€Šá€ºá€¸á€•á€«á€á€šá€ºá‹

---

## 7. SARSA (State-Action-Reward-State-Action)

### Algorithm Overview

SARSA á€á€Šá€º TD prediction á€€á€­á€¯ policy evaluation á€¡á€á€½á€€á€º á€á€¯á€¶á€¸á€•á€¼á€®á€¸ Îµ-greedy á€€á€­á€¯ improvement á€¡á€á€½á€€á€º á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ **On-policy** method á€–á€¼á€…á€ºá€•á€¼á€®á€¸ every step á€™á€¾á€¬ update á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

### SARSA Update Rule

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \Big]$$

- $A_{t+1}$ â€” agent á€€ **actually á€šá€°á€™á€šá€·á€º** action (Îµ-greedy á€€á€”á€± select)
- TD target: $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$

### SARSA Implementation Key

```python
# SARSA: next action á€€á€­á€¯ Îµ-greedy á€”á€²á€· select
action = select_action(state, Q, epsilons[e])
next_state, reward, done, _ = env.step(action)
next_action = select_action(next_state, Q, epsilons[e])

# TD target uses the ACTUAL next action
td_target = reward + gamma * Q[next_state][next_action] * (not done)
td_error = td_target - Q[state][action]
Q[state][action] += alphas[e] * td_error

state, action = next_state, next_action
```

> ğŸ’¡ SARSA á€›á€²á€· name á€›á€²á€· origin â€” **(S**tate, **A**ction, **R**eward, next **S**tate, next **A**ction) â€” tuple á€€á€­á€¯á€¡á€á€¼á€±á€á€¶á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

---

## 8. Q-learning

### Algorithm Overview

Q-learning á€á€Šá€º **off-policy** method á€–á€¼á€…á€ºá€•á€¼á€®á€¸ behavior policy (Îµ-greedy) á€”á€²á€· target policy (greedy) á€€á€­á€¯ á€á€½á€²á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Agent á€€ randomly explore á€œá€¯á€•á€ºá€”á€±á€œá€Šá€ºá€¸ optimal Q-function á€€á€­á€¯ approximate á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

### Q-learning Update Rule

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \Big]$$

### SARSA vs Q-learning â€” Key Difference

```mermaid
graph TD
    subgraph SARSA_UPD["SARSA (On-policy)"]
        SA["TD target =<br/>R + Î³ Q(S', A')"]
        SA_NOTE["A' = Îµ-greedy action<br/>(actual next action)"]
    end
    
    subgraph Q_UPD["Q-learning (Off-policy)"]
        QA["TD target =<br/>R + Î³ max_a' Q(S', a')"]
        QA_NOTE["max_a' = greedy action<br/>(best estimated action)"]
    end
    
    style SA fill:#ff922b,color:#fff
    style QA fill:#4CAF50,color:#fff
```

| | SARSA | Q-learning |
|---|---|---|
| **Next action in target** | $Q(S_{t+1}, A_{t+1})$ â€” actually taken action | $\max_{a'} Q(S_{t+1}, a')$ â€” max over all actions |
| **Policy type** | On-policy | Off-policy |
| **Learning about** | Behavioral policy itself | Optimal policy (greedy) |

### Q-learning Implementation Key

```python
# Q-learning: action selection inside the step loop
action = select_action(state, Q, epsilons[e])
next_state, reward, done, _ = env.step(action)

# TD target uses MAX over next state (not actual next action!)
td_target = reward + gamma * Q[next_state].max() * (not done)
td_error = td_target - Q[state][action]
Q[state][action] += alphas[e] * td_error

state = next_state  # no need to track next_action
```

---

## 9. On-policy vs Off-policy Learning

```mermaid
graph TD
    subgraph ON["On-policy Learning"]
        ON1["Single Policy Ï€"]
        ON1 -->|"generate data"| ON2["Experience"]
        ON2 -->|"evaluate & improve"| ON1
        ON_EX["Examples: MC Control, SARSA"]
    end
    
    subgraph OFF["Off-policy Learning"]
        OFF1["Behavior Policy Î¼<br/>(Îµ-greedy, exploratory)"]
        OFF1 -->|"generate data"| OFF2["Experience"]
        OFF2 -->|"learn about"| OFF3["Target Policy Ï€<br/>(greedy, optimal)"]
        OFF_EX["Examples: Q-learning, Double Q-learning"]
    end
    
    style ON1 fill:#2196F3,color:#fff
    style OFF1 fill:#ff922b,color:#fff
    style OFF3 fill:#4CAF50,color:#fff
```

### Convergence Requirements

**GLIE (Greedy in the Limit with Infinite Exploration):**

On-policy algorithms (MC control, SARSA) á€¡á€á€½á€€á€º:
1. State-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ infinitely often explore á€œá€¯á€•á€ºá€›á€™á€Šá€º
2. Policy á€á€Šá€º greedy policy á€†á€® converge á€–á€¼á€…á€ºá€›á€™á€Šá€º

**Off-policy algorithms (Q-learning)** á€¡á€á€½á€€á€º:
- State-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ sufficiently update á€œá€¯á€•á€ºá€›á€™á€Šá€º (condition 1 only)

**Stochastic Approximation Theory (learning rate requirements):**

$$\sum_{t=1}^{\infty} \alpha_t = \infty, \quad \sum_{t=1}^{\infty} \alpha_t^2 < \infty$$

> ğŸ’¡ Practice á€™á€¾á€¬ small constant learning rate á€€á€­á€¯ common á€–á€¼á€„á€·á€º á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ Non-stationary environments á€¡á€á€½á€€á€ºá€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

---

## 10. Double Q-learning

### Maximization Bias Problem

Q-learning á€á€Šá€º value function á€€á€­á€¯ **overestimate** á€œá€¯á€•á€ºá€á€á€ºá€•á€«á€á€šá€ºá‹ Max over **estimates** á€€á€­á€¯ **estimate of max** á€¡á€–á€¼á€…á€º á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸á€€á€¼á€±á€¬á€„á€·á€º positive bias á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

$$\max_a \hat{Q}(s, a) \geq \max_a Q^*(s, a)$$

> ğŸ’¡ Estimates á€á€½á€±á€™á€¾á€¬ bias á€›á€¾á€­á€á€šá€º (positive/negative)á‹ Max á€šá€°á€á€¼á€„á€ºá€¸á€€ always positive bias á€€á€­á€¯ favor á€œá€¯á€•á€ºá€•á€¼á€®á€¸ compounding errors á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

### Double Learning Solution

Q1 á€”á€²á€· Q2 â€” two separate Q-functions á€€á€­á€¯ track á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

```mermaid
graph LR
    COIN["ğŸª™ Flip coin"] -->|"Heads"| UPD1["Update Q1"]
    COIN -->|"Tails"| UPD2["Update Q2"]
    
    UPD1 --> SEL1["Q1 selects best action<br/>a* = argmax Q1(s')"]
    SEL1 --> VAL1["Q2 evaluates it<br/>target uses Q2(s', a*)"]
    
    UPD2 --> SEL2["Q2 selects best action<br/>a* = argmax Q2(s')"]
    SEL2 --> VAL2["Q1 evaluates it<br/>target uses Q1(s', a*)"]
    
    style COIN fill:#ffd43b,color:#000
    style UPD1 fill:#2196F3,color:#fff
    style UPD2 fill:#4CAF50,color:#fff
```

### Double Q-learning Update Equations

If updating $Q_1$:

$$a^* = \arg\max_a Q_1(S_{t+1}, a)$$

$$Q_1(S_t, A_t) \leftarrow Q_1(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q_2(S_{t+1}, a^*) - Q_1(S_t, A_t) \Big]$$

If updating $Q_2$ (mirror):

$$a^* = \arg\max_a Q_2(S_{t+1}, a)$$

$$Q_2(S_t, A_t) \leftarrow Q_2(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q_1(S_{t+1}, a^*) - Q_2(S_t, A_t) \Big]$$

**Action selection:** $Q_1 + Q_2$ á€›á€²á€· average á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€º:

$$\pi(s) = \arg\max_a \frac{Q_1(s, a) + Q_2(s, a)}{2}$$

---

## 11. Experimental Results (SWS Environment)

### Performance Comparison

| Metric | MC Control | SARSA | Q-learning | Double Q-learning |
|---|---|---|---|---|
| **Convergence speed** | Moderate | Moderate | Fast | Slightly slower than Q |
| **Variance** | High | Lower | Moderate | Low |
| **Overestimation** | Low | Low | High | Controlled |
| **Stability** | Moderate | Good | Jumpy | Best |
| **Optimal policy success** | Slow | Slow | Fast but overshoots | Fastest to 100% |

> ğŸ’¡ **Double Q-learning** á€á€Šá€º Q-learning á€‘á€€á€º stable á€–á€¼á€…á€ºá€•á€¼á€®á€¸ optimal policy á€€á€­á€¯ faster á€›á€±á€¬á€€á€ºá€•á€«á€á€šá€ºá‹ Overestimation á€€á€­á€¯ effectively mitigate á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

---

## 12. Key Equations Summary

| Equation | Formula |
|---|---|
| **MC Return** | $G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$ |
| **MC Control Update** | $Q(s,a) \leftarrow Q(s,a) + \alpha [G_t - Q(s,a)]$ |
| **SARSA Update** | $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]$ |
| **Q-learning Update** | $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_t,A_t)]$ |
| **Double Q Update (Q1)** | $Q_1(S_t,A_t) \leftarrow Q_1 + \alpha [R_{t+1} + \gamma Q_2(S_{t+1}, \arg\max_a Q_1(S_{t+1},a)) - Q_1(S_t,A_t)]$ |
| **GLIE epsilon decay** | $\epsilon \to 0$ as $t \to \infty$ |
| **Î± requirements** | $\sum \alpha_t = \infty, \; \sum \alpha_t^2 < \infty$ |

---

## 13. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **GPI pattern** â€” policy evaluation + improvement á€€á€­á€¯ alternate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º optimal policy á€€á€­á€¯á€›á€¾á€¬á€•á€«á€á€šá€º
2. **MC Control** â€” episode á€•á€¼á€®á€¸á€™á€¾ Q estimates update, high variance but unbiased
3. **SARSA** â€” on-policy TD method, step-by-step update, stable
4. **Q-learning** â€” off-policy TD method, learns optimal policy regardless of behavior policy
5. **Double Q-learning** â€” maximization bias á€€á€­á€¯ mitigate, more stable convergence
6. **On-policy vs Off-policy** â€” each has pros and cons; off-policy á€€á€­á€¯ bootstrapping + function approximation á€”á€²á€· combine á€›á€„á€º divergence á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º

```mermaid
graph TD
    GPI["ğŸ”„ GPI Pattern"] --> MC["MC Control<br/>Episode-based<br/>On-policy"]
    GPI --> SARSA["SARSA<br/>Step-based<br/>On-policy"]
    GPI --> QL["Q-learning<br/>Step-based<br/>Off-policy"]
    QL --> DQL["Double Q-learning<br/>Reduces overestimation"]
    
    MC -.->|"offline updates"| NOTE1["High variance<br/>No bias"]
    SARSA -.->|"online updates"| NOTE2["Lower variance<br/>Some bias"]
    QL -.->|"online updates"| NOTE3["Overestimates<br/>But fast"]
    DQL -.->|"online updates"| NOTE4["More stable<br/>Best overall"]
    
    style GPI fill:#ffd43b,color:#000
    style MC fill:#ff922b,color:#fff
    style SARSA fill:#2196F3,color:#fff
    style QL fill:#4CAF50,color:#fff
    style DQL fill:#9C27B0,color:#fff
```
