# Chapter 7: Achieving Goals More Effectively and Efficiently - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ Chapter 6 á€›á€²á€· control methods á€á€½á€±á€€á€­á€¯ **á€•á€­á€¯á€™á€­á€¯ effective** (á€›á€œá€’á€ºá€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸) á€”á€¾á€„á€·á€º **á€•á€­á€¯á€™á€­á€¯ efficient** (data á€•á€­á€¯á€”á€Šá€ºá€¸á€á€¯á€¶á€¸) á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º improve á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Eligibility traces á€–á€¼á€„á€·á€º credit assignment á€€á€­á€¯ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¯á€•á€ºá€•á€¼á€®á€¸ model-based methods á€–á€¼á€„á€·á€º sample efficiency á€€á€­á€¯ á€™á€¼á€¾á€„á€·á€ºá€á€„á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph CH6["ğŸ“— Chapter 6: Basic Control"]
        C6["MC Control / SARSA / Q-learning"]
        C6 --> L6["Single-step credit assignment"]
    end
    
    subgraph CH7["ğŸ“˜ Chapter 7: Advanced Control"]
        C7A["SARSA(Î») / Q(Î»)<br/>Eligibility Traces"]
        C7B["Dyna-Q / Trajectory Sampling<br/>Model-based RL"]
    end
    
    CH6 -->|"multi-step credit<br/>assignment"| C7A
    CH6 -->|"learn & use<br/>environment model"| C7B
    
    style CH6 fill:#2196F3,color:#fff
    style C7A fill:#4CAF50,color:#fff
    style C7B fill:#9C27B0,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **SARSA(Î»)** â€” on-policy control with eligibility traces
2. **Watkins's Q(Î»)** â€” off-policy control with eligibility traces
3. **Accumulating vs Replacing traces**
4. **Dyna-Q** â€” model-free + model-based RL á€•á€±á€«á€„á€ºá€¸á€…á€•á€º
5. **Trajectory Sampling** â€” smarter planning strategy

---

## 2. SARSA(Î») â€” Eligibility Traces for On-policy Control

### SARSA(Î») Overview

SARSA(Î») á€á€Šá€º SARSA + TD(Î») á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Chapter 5 á€™á€¾á€¬ prediction á€¡á€á€½á€€á€º á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· eligibility traces á€€á€­á€¯ **control** (Q-function learning) á€¡á€á€½á€€á€º apply á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    SARSA["SARSA<br/>(one-step TD control)"] -->|"+ eligibility traces<br/>+ Î» parameter"| SARSA_L["SARSA(Î»)<br/>(multi-step credit)"]
    
    TDL["TD(Î»)<br/>(prediction with traces)"] -->|"+ Q-function<br/>+ Îµ-greedy"| SARSA_L
    
    style SARSA fill:#ff922b,color:#fff
    style TDL fill:#2196F3,color:#fff
    style SARSA_L fill:#4CAF50,color:#fff
```

### SARSA(Î») Update Equations

**Eligibility Trace Update:**

$$E_t(s, a) = \begin{cases} E_{t-1}(s,a) + 1 & \text{if } s = S_t, a = A_t \\ \gamma \lambda \, E_{t-1}(s,a) & \text{otherwise} \end{cases}$$

**Q-function Update (all state-action pairs at once):**

$$Q(s, a) \leftarrow Q(s, a) + \alpha \, \delta_t \, E_t(s, a), \quad \forall s, a$$

where TD error: $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$

### SARSA(Î») Implementation Key

```python
# Initialize eligibility traces
E = np.zeros((nS, nA), dtype=np.float64)

# Inside episode loop
E.fill(0)  # reset traces every episode
while not done:
    next_action = select_action(next_state, Q, epsilons[e])
    td_target = reward + gamma * Q[next_state][next_action] * (not done)
    td_error = td_target - Q[state][action]
    
    E[state][action] += 1                    # increment trace
    if replacing_traces: E.clip(0, 1, out=E) # clip to 1 if replacing
    
    Q = Q + alphas[e] * td_error * E         # update ALL eligible pairs
    E = gamma * lambda_ * E                  # decay traces
```

---

## 3. Accumulating Traces vs Replacing Traces

### Key Difference

```mermaid
graph TD
    subgraph ACC["Accumulating Traces"]
        A1["E(s,a) += 1 every visit"]
        A2["Trace value > 1 possible"]
        A3["Frequency heuristic: <br/>self-loop states á€€á€­á€¯<br/>extra credit á€•á€±á€¸"]
    end
    
    subgraph REP["Replacing Traces"]
        R1["E(s,a) = min(E+1, 1)"]
        R2["Trace value â‰¤ 1 always"]
        R3["Recency heuristic: <br/>recent events á€€á€­á€¯<br/>á€•á€­á€¯á€á€¬ recognize"]
    end
    
    style A1 fill:#ff922b,color:#fff
    style R1 fill:#4CAF50,color:#fff
```

| Feature | Accumulating Traces | Replacing Traces |
|---|---|---|
| **Multiple visits** | Trace value increases > 1 | Trace value clipped to 1 |
| **Heuristic** | Frequency + Recency | Recency dominant |
| **Loop environments** | Frequently visited states á€€á€­á€¯ over-credit | More balanced credit |
| **When to use** | Frequency matters | When loops/repeated visits occur |

> ğŸ’¡ Accumulating traces á€á€Šá€º frequency á€€á€­á€¯ emphasize á€œá€¯á€•á€ºá€•á€¼á€®á€¸ replacing traces á€á€Šá€º recency á€€á€­á€¯ moderate á€•á€¼á€”á€ºá€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Loops á€›á€¾á€­á€á€²á€· environments á€™á€¾á€¬ replacing traces á€€á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€á€ºá€•á€«á€á€šá€ºá‹

---

## 4. Watkins's Q(Î») â€” Off-policy Control with Traces

### Q(Î») Overview

Q(Î») á€á€Šá€º Q-learning + eligibility traces á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Off-policy method á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º trace management á€€á€­á€¯ **careful** á€–á€¼á€…á€ºá€›á€•á€«á€á€šá€ºá‹

### Key Difference from SARSA(Î»)

```mermaid
graph TD
    subgraph SL["SARSA(Î»)"]
        SL1["On-policy"]
        SL2["Traces always decay:<br/>E = Î³Î»E"]
        SL3["Follows Îµ-greedy policy"]
    end
    
    subgraph QL["Watkins's Q(Î»)"]
        QL1["Off-policy"]
        QL2["Traces conditional:<br/>If next action is greedy â†’ E = Î³Î»E<br/>If exploratory â†’ E = 0 (reset!)"]
        QL3["Learns about greedy policy"]
    end
    
    style SL1 fill:#2196F3,color:#fff
    style QL1 fill:#4CAF50,color:#fff
```

### Q(Î») Trace Reset Logic

$$E_t = \begin{cases} \gamma \lambda \, E_{t-1} & \text{if } A_{t+1} = \arg\max_a Q(S_{t+1}, a) \\ 0 & \text{otherwise (exploratory action)} \end{cases}$$

> ğŸ’¡ Exploratory action á€šá€°á€á€²á€·á€¡á€á€« traces á€€á€­á€¯ **reset** á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬á€· off-policy learning á€™á€¾á€¬ greedy policy á€€á€­á€¯ learn á€”á€±á€á€¬á€–á€¼á€…á€ºá€•á€¼á€®á€¸ exploratory action á€šá€°á€•á€¼á€®á€¸á€›á€„á€º greedy trajectory á€€á€”á€± á€‘á€½á€€á€ºá€á€½á€¬á€¸á€á€¬á€™á€­á€¯á€· traces á€›á€²á€· validity á€€á€»á€á€½á€¬á€¸á€•á€«á€á€šá€ºá‹

### Q(Î») Implementation Key

```python
# Q(Î»): conditional trace update
if replacing_traces: E[state].fill(0)  # zero out all actions of current state
E[state][action] += 1
Q = Q + alphas[e] * td_error * E

# Check if next action is greedy
next_action_is_greedy = (Q[next_state][next_action] == Q[next_state].max())
if next_action_is_greedy:
    E = gamma * lambda_ * E   # normal decay
else:
    E.fill(0)                  # reset all traces!
```

---

## 5. Model-based Reinforcement Learning

### Planning vs Model-free vs Model-based

```mermaid
graph TD
    subgraph PLAN["Planning Methods"]
        P["VI / PI"]
        P -->|"require"| MDP1["MDP given in advance"]
    end
    
    subgraph MF["Model-free RL"]
        MF1["SARSA / Q-learning"]
        MF1 -->|"don't need"| MDP2["No MDP needed"]
    end
    
    subgraph MB["Model-based RL"]
        MB1["Dyna-Q / Trajectory Sampling"]
        MB1 -->|"learn"| MDP3["Learn MDP from interaction"]
        MB1 -->|"and use it for"| PLAN2["Planning / Simulation"]
    end
    
    style P fill:#ef5350,color:#fff
    style MF1 fill:#2196F3,color:#fff
    style MB1 fill:#4CAF50,color:#fff
```

| Method Type | MDP Required? | MDP Learned? | Sample Efficiency |
|---|---|---|---|
| **Planning** (VI, PI) | Yes (given) | No | N/A (computes directly) |
| **Model-free** (SARSA, Q) | No | No | Low (only real experience) |
| **Model-based** (Dyna-Q) | No | Yes (learns from data) | High (real + simulated) |

---

## 6. Dyna-Q

### Dyna-Q Architecture

Dyna-Q á€á€Šá€º Q-learning + model learning + planning á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ Real experience á€€á€”á€± Q-function á€€á€­á€¯ update á€œá€¯á€•á€ºá€›á€¯á€¶á€á€¬á€™á€€ environment model á€€á€­á€¯á€œá€Šá€ºá€¸ learn á€•á€¼á€®á€¸ simulated experience á€€á€”á€±á€œá€Šá€ºá€¸ Q-function á€€á€­á€¯ á€‘á€•á€ºá€™á€¶ update á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    VF["Value Function Q"] -->|"1. Select action<br/>(Îµ-greedy)"| ACT["Action"]
    ACT -->|"2. Interact with<br/>environment"| EXP["Experience<br/>(s, a, r, s')"]
    EXP -->|"3a. Update Q<br/>(model-free RL)"| VF
    EXP -->|"3b. Learn model<br/>(T, R functions)"| MODEL["Environment Model<br/>T_count(s,a,s'), R_model(s,a,s')"]
    MODEL -->|"4. Simulated<br/>experience"| PLAN["Planning Updates<br/>(n_planning steps)"]
    PLAN -->|"5. Further<br/>improve Q"| VF
    
    style VF fill:#2196F3,color:#fff
    style MODEL fill:#4CAF50,color:#fff
    style PLAN fill:#9C27B0,color:#fff
```

### Dyna-Q Model Learning

**Transition function** á€€á€­á€¯ count-based approach á€–á€¼á€„á€·á€º learn:

$$\hat{T}(s'|s, a) = \frac{\text{count}(s, a, s')}{\sum_{s''} \text{count}(s, a, s'')}$$

**Reward function** á€€á€­á€¯ incremental mean á€–á€¼á€„á€·á€º learn:

$$\hat{R}(s, a, s') \leftarrow \hat{R}(s, a, s') + \frac{r - \hat{R}(s, a, s')}{\text{count}(s, a, s')}$$

### Dyna-Q Planning Phase

```python
# Planning: sample from learned model
for _ in range(n_planning):
    if Q.sum() == 0: break
    # Sample previously visited state
    visited_states = np.where(np.sum(T_count, axis=(1,2)) > 0)[0]
    state = np.random.choice(visited_states)
    # Sample previously taken action
    actions_taken = np.where(np.sum(T_count[state], axis=1) > 0)[0]
    action = np.random.choice(actions_taken)
    # Sample next state from learned model
    probs = T_count[state][action] / T_count[state][action].sum()
    next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]
    reward = R_model[state][action][next_state]
    # Q-learning update with simulated experience
    td_target = reward + gamma * Q[next_state].max()
    Q[state][action] += alphas[e] * (td_target - Q[state][action])
```

> ğŸ’¡ Dyna-Q á€á€Šá€º state-action pairs á€€á€­á€¯ **uniformly at random** sample á€•á€«á€á€šá€ºá‹ á€’á€«á€€ effective á€•á€±á€™á€šá€·á€º optimal sampling strategy á€™á€Ÿá€¯á€á€ºá€•á€«á‹

---

## 7. Trajectory Sampling

### Trajectory Sampling vs Dyna-Q

Trajectory Sampling á€á€Šá€º Dyna-Q á€›á€²á€· improved version á€–á€¼á€…á€ºá€•á€¼á€®á€¸ **immediate future** á€¡á€á€½á€€á€º plan á€•á€«á€á€šá€ºá‹ Random state á€€á€­á€¯ sample á€œá€¯á€•á€ºá€™á€Šá€·á€ºá€¡á€…á€¬á€¸ current greedy trajectory á€€á€­á€¯ sample á€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    subgraph DYNA["Dyna-Q Planning"]
        D1["Random state selection<br/>(uniformly at random)"]
        D1 --> D2["Random action"]
        D2 --> D3["Broad but unfocused<br/>planning"]
    end
    
    subgraph TS["Trajectory Sampling"]
        T1["Current state á€€á€”á€± start"]
        T1 --> T2["Greedy action selection"]
        T2 --> T3["Focused planning<br/>for immediate future"]
    end
    
    style D1 fill:#ff922b,color:#fff
    style T1 fill:#4CAF50,color:#fff
```

| Feature | Dyna-Q | Trajectory Sampling |
|---|---|---|
| **State sampling** | Uniformly random from visited | Follow greedy trajectory from current |
| **Action sampling** | Random from taken actions | Greedy w.r.t. current Q |
| **Planning focus** | Broad, unfocused | Immediate future, focused |
| **Reward encounter** | Proportional to MDP | More frequent (goal-directed) |

### Trajectory Sampling Planning Phase

```python
for _ in range(max_trajectory_depth):
    if Q.sum() == 0: break
    action = Q[state].argmax()                    # greedy action
    if not T_count[state][action].sum(): break    # no experience for this
    probs = T_count[state][action] / T_count[state][action].sum()
    next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]
    reward = R_model[state][action][next_state]
    td_target = reward + gamma * Q[next_state].max()
    Q[state][action] += alphas[e] * (td_target - Q[state][action])
    state = next_state  # follow the trajectory!
```

---

## 8. Frozen Lake Environments

### Test Environments

á€’á€® Chapter á€™á€¾á€¬ SWS á€¡á€•á€¼á€„á€º **Frozen Lake (FL)** á€”á€¾á€„á€·á€º **Frozen Lake 8Ã—8 (FL8Ã—8)** environments á€€á€­á€¯á€œá€Šá€ºá€¸ test á€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph FL["Frozen Lake 4Ã—4"]
        FL1["16 states, 4 actions"]
        FL2["Slippery (33% intended)"]
        FL3["Single +1 reward at GOAL"]
        FL4["Holes = terminal states"]
    end
    
    subgraph FL8["Frozen Lake 8Ã—8"]
        FL81["64 states, 4 actions"]
        FL82["10 holes"]
        FL83["Much harder â€” sparse reward"]
        FL84["Need 30,000 episodes"]
    end
    
    FL -->|"scaled up"| FL8
    
    style FL1 fill:#2196F3,color:#fff
    style FL81 fill:#ef5350,color:#fff
```

### Environment Hyperparameters

| Environment | Episodes | Î³ | Alpha decay | Epsilon decay |
|---|---|---|---|---|
| SWS | 3,000 | 1.0 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |
| FL 4Ã—4 | 10,000 | 0.99 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |
| FL 8Ã—8 | 30,000 | 0.99 | 0.5â†’0.01 (50%) | 1.0â†’0.1 (90%) |

---

## 9. Experimental Results

### Key Findings

| Method | SWS | FL 4Ã—4 | FL 8Ã—8 | Sample Efficiency |
|---|---|---|---|---|
| **SARSA(Î»)** | OK | Slow | âŒ Too slow | Low |
| **Q(Î»)** | Good | Good | âœ… Converges | Medium-High |
| **Dyna-Q** | Good | Fast | âœ… Fast but spiky | High |
| **Trajectory Sampling** | Good | Fast | âœ… Fast & stable | Highest |

> ğŸ’¡ **Model-based methods** (Dyna-Q, Trajectory Sampling) á€á€Šá€º model-free methods á€‘á€€á€º sample efficient á€•á€­á€¯á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Dyna-Q á€™á€¾á€¬ initial error spike á€›á€¾á€­á€”á€­á€¯á€„á€ºá€•á€±á€™á€šá€·á€º trajectory sampling á€™á€¾á€¬ á€•á€­á€¯á€™á€­á€¯ stable á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 10. Key Equations Summary

| Equation | Formula |
|---|---|
| **SARSA(Î») trace** | $E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{1}[s=S_t, a=A_t]$ |
| **SARSA(Î») Q update** | $Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a)$ |
| **SARSA(Î») TD error** | $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$ |
| **Q(Î») trace (greedy)** | $E_t = \gamma \lambda E_{t-1}$ |
| **Q(Î») trace (exploratory)** | $E_t = 0$ (reset all) |
| **Dyna-Q transition model** | $\hat{T}(s'\|s,a) = \frac{\text{count}(s,a,s')}{\sum_{s''}\text{count}(s,a,s'')}$ |
| **Dyna-Q reward model** | $\hat{R}(s,a,s') \leftarrow \hat{R} + \frac{r - \hat{R}}{\text{count}(s,a,s')}$ |

---

## 11. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **SARSA(Î»)** â€” eligibility traces á€–á€¼á€„á€·á€º on-policy control, Î» parameter á€–á€¼á€„á€·á€º credit assignment depth á€€á€­á€¯ control
2. **Q(Î»)** â€” off-policy Î»-return method, exploratory action á€šá€°á€›á€„á€º traces reset
3. **Accumulating vs Replacing traces** â€” frequency vs recency trade-off
4. **Dyna-Q** â€” model-free RL + model learning + planning á€•á€±á€«á€„á€ºá€¸á€…á€•á€º, sample efficiency boost
5. **Trajectory Sampling** â€” greedy trajectory á€–á€¼á€„á€·á€º focused planning, Dyna-Q á€‘á€€á€º stable
6. **Model-based RL** â€” experience samples á€€á€­á€¯ á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­á€†á€¯á€¶á€¸ á€¡á€á€¯á€¶á€¸á€á€»á€”á€­á€¯á€„á€º, complex environments á€™á€¾á€¬ á€•á€­á€¯á€¡á€›á€±á€¸á€€á€¼á€®á€¸

```mermaid
graph TD
    CH7["Chapter 7:<br/>More Effective & Efficient"] --> TRACES["Eligibility Traces"]
    CH7 --> MBRL["Model-based RL"]
    
    TRACES --> SL["SARSA(Î»)<br/>On-policy"]
    TRACES --> QLam["Q(Î»)<br/>Off-policy"]
    
    SL --> ACC["Accumulating Traces"]
    SL --> REP["Replacing Traces"]
    QLam --> ACC
    QLam --> REP
    
    MBRL --> DQ["Dyna-Q<br/>Random planning"]
    MBRL --> TS["Trajectory Sampling<br/>Focused planning"]
    
    style CH7 fill:#ffd43b,color:#000
    style SL fill:#2196F3,color:#fff
    style QLam fill:#4CAF50,color:#fff
    style DQ fill:#9C27B0,color:#fff
    style TS fill:#ef5350,color:#fff
```

> ğŸ’¡ á€’á€® Chapter á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º tabular RL (discrete states/actions) á€€á€”á€± **deep RL** (continuous/high-dimensional states) á€†á€® á€€á€°á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€«á€™á€šá€ºá‹ Chapter 8 á€€á€”á€±á€…á€•á€¼á€®á€¸ function approximation (neural networks) á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹
