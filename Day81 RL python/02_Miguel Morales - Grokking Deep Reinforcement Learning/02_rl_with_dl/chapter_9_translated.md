# á€¡á€á€”á€ºá€¸ á‰ â€” á€•á€­á€¯á€á€Šá€ºá€„á€¼á€­á€™á€ºá€á€±á€¬ Value-based Methods (More Stable Value-Based Methods)

> *"á€á€„á€·á€ºá€á€¼á€±á€œá€¾á€™á€ºá€¸á€á€Šá€º á€”á€¾á€±á€¸á€•á€¼á€®á€¸ á€™á€¼á€²á€™á€¼á€¶á€•á€«á€…á€± â€” á€™á€™á€­á€¯á€€á€ºá€”á€­á€¯á€„á€ºá€–á€­á€¯á€·á€–á€¼á€„á€ºá€á€±á€¬á€º"*
> â€” Tokugawa Ieyasu
> (Tokugawa shogunate á founder áŠ Japan á€€á€­á€¯ á€•á€¼á€Šá€ºá€‘á€±á€¬á€„á€ºá€€á€¼á€®á€¸ áƒ á€¦á€¸ á€‘á€²á€™á€¾ á€á€¦á€¸)

## á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ á€á€„á€ºá€šá€°á€›á€™á€Šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

- á€šá€á€„á€º chapter á€™á€¾ methods á€™á€»á€¬á€¸á€€á€­á€¯ á€•á€­á€¯á€á€Šá€ºá€„á€¼á€­á€™á€ºá€¡á€±á€¬á€„á€º á€†á€±á€¬á€„á€ºá€›á€½á€€á€ºá€™á€Šá€º (divergence á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ á€œá€»á€±á€¬á€·á€á€»)
- Advanced value-based deep RL methods á€”á€¾á€„á€·á€º value-based methods á€€á€­á€¯ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€…á€±á€á€±á€¬ components á€™á€»á€¬á€¸á€€á€­á€¯ explore á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º
- Cart-pole environment á€€á€­á€¯ á€šá€á€„á€ºá€‘á€€á€º samples á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸á€”á€¾á€„á€·á€º á€•á€­á€¯á€á€±á€á€»á€¬á€•á€¼á€®á€¸ consistent results á€–á€¼á€„á€·á€º solve á€œá€¯á€•á€ºá€™á€Šá€º

---

## á‰.á â€” Chapter 8 á€€á€”á€± á€’á€®á€”á€±á€· â€” á€˜á€¬á€á€½á€± á€–á€¼á€…á€ºá€•á€»á€€á€ºá€”á€±á€á€œá€²

Chapter 8 á€á€½á€„á€º value-based deep RL á€€á€­á€¯ introduce á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ **NFQ (Neural Fitted Q-Iteration)** algorithm á€€á€­á€¯ develop á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ NFQ á€á€Šá€º **IID assumption violation** á€”á€¾á€„á€·á€º **non-stationary targets** á€á€­á€¯á€· á€–á€¼á€…á€ºá€á€±á€¬ á‚ á€€á€¼á€®á€¸á€€á€»á€•á€ºá€†á€¯á€¶á€¸á€á€±á€¬ problems á€€á€­á€¯ batching á€–á€¼á€„á€·á€º address á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ á€€á€±á€¬á€„á€ºá€¸á€á€²á€· results á€•á€¼á€á€”á€­á€¯á€„á€ºá€á€²á€·á€•á€«á€á€šá€ºâ€”á€’á€«á€•á€±á€™á€šá€·á€º **á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€±á€¸á€á€šá€º**!

á€’á€® chapter á€™á€¾á€¬ á€’á€® problems á€€á€­á€¯ fundamental á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€±á€¬ techniques á€–á€¼á€„á€·á€º address:
1. **Experience Replay** â€” IID violation á€–á€¼á€±á€›á€¾á€„á€ºá€¸
2. **Target Networks** â€” Non-stationary targets á€–á€¼á€±á€›á€¾á€„á€ºá€¸
3. **Double Learning** â€” Overestimation bias á€–á€¼á€±á€›á€¾á€„á€ºá€¸

á€’á€® áƒ á€á€¯á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€•á€¼á€®á€¸ **DQN (Deep Q-Network)** á€”á€¾á€„á€·á€º **DDQN (Double DQN)** á€–á€”á€ºá€á€®á€¸á€™á€Šá€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º!

```mermaid
graph TD
    NFQ["NFQ (Ch 8)"] -->|"Problem 1: Data NOT IID"| ER["+ Experience Replay"]
    NFQ -->|"Problem 2: Non-stationary targets"| TN["+ Target Network"]
    ER --> DQN["DQN<br/>(Nature DQN, 2015)"]
    TN --> DQN
    DQN -->|"Problem 3: Overestimation bias"| DDQN["DDQN<br/>(Double DQN, 2015)"]
    
    style NFQ fill:#ff922b,color:#fff
    style ER fill:#2196F3,color:#fff
    style TN fill:#4CAF50,color:#fff
    style DQN fill:#9C27B0,color:#fff
    style DDQN fill:#ef5350,color:#fff
```

---

## á‰.á‚ â€” Value-based Deep RL á Common Problems (á€•á€¼á€”á€ºá€á€¯á€¶á€¸á€á€•á€ºá€á€¼á€„á€ºá€¸)

DQN á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€–á€­á€¯á€· á€¦á€¸á€…á€½á€¬ problems á‚ á€á€¯á€€á€­á€¯ á€•á€¼á€”á€ºá€€á€¼á€Šá€·á€ºá€›á€•á€«á€™á€Šá€º:

### á‰.á‚.á â€” Problem 1: Data á€á€Šá€º IID á€™á€Ÿá€¯á€á€ºá€•á€«

Supervised learning á€™á€¾á€¬ dataset á€€á€­á€¯ advance á€†á€„á€ºá€‘á€¬á€¸á€•á€¼á€®á€¸ shuffle á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

```
Supervised Learning Dataset:
[sample_5, sample_2, sample_8, sample_1, ...] â† shuffled, IID âœ…
```

RL á€™á€¾á€¬ data á€€á€­á€¯ **online** collect á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º â€” sequential trajectory á€á€…á€ºá€á€¯á€™á€¾ á€†á€€á€ºá€á€­á€¯á€€á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º:

```
RL online data:
[(s_1, a_1, r_2, s_2), (s_2, a_2, r_3, s_3), ...] â† correlated, NOT IID âŒ
```

**IID violation á á‚ á€€á€»á€•á€ºá€†á€¯á€¶á€¸ á€€á€­á€…á€¹á€…:**

1. **Not Independent:** $s_{t+1}$ á outcome á€á€Šá€º $s_t$ á€•á€±á€«á€º depend á€•á€«á€á€šá€º â†’ samples correlated
2. **Not Identically Distributed:** Policy improving á€–á€¼á€…á€ºá€”á€±á€á€²á€·á€¡á€á€½á€€á€º data distribution á€•á€¼á€±á€¬á€„á€ºá€¸á€”á€±á€•á€«á€á€šá€º â†’ distribution shift

```mermaid
graph LR
    subgraph SL["Supervised Learning âœ…"]
        D1["Preshuffled dataset"]
        D2["IID samples"]
        D3["Fixed distribution"]
    end
    
    subgraph RL["RL âŒ"]
        R1["Online collection"]
        R2["Correlated: s_t â†’ s_{t+1}"]
        R3["Distribution changes<br/>as Ï€ improves"]
    end
    
    style SL fill:#4CAF50,color:#fff
    style RL fill:#ef5350,color:#fff
```

### á‰.á‚.á‚ â€” Problem 2: Non-stationary Targets

Supervised learning á€™á€¾á€¬ labels á€™á€»á€¬á€¸á€á€Šá€º **constants** á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” training á€¡á€á€±á€¬á€•á€á€ºá€œá€¯á€¶á€¸ fixed á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

$$\text{SL Target: } y_i \text{ (label)} = \text{constant â€” FIXED throughout training}$$

RL TD target á€™á€¾á€¬:

$$y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$$

$\theta$ update á€–á€¼á€…á€ºá€•á€«á€á€šá€º â†’ $Q(S_{t+1}, a'; \theta)$ á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€º â†’ target $y_t$ á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€º â†’ **moving target!**

```mermaid
graph LR
    A["Optimize<br/>Q(s_t, a_t; Î¸)"] -->|"Î¸ changes"| B["Q(s_{t+1}; Î¸) changes"]
    B -->|"But this IS the target!"| C["Target y_t moves!"]
    C -->|"Previous update now outdated"| D["UNSTABLE/DIVERGE!"]
    
    style A fill:#2196F3,color:#fff
    style D fill:#ef5350,color:#fff
```

NFQ á€á€Šá€º batching á€–á€¼á€„á€·á€º á€’á€® problems á€€á€­á€¯ somewhat address á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ â€” á€’á€«á€•á€±á€™á€šá€·á€º DQN á€€á€á€±á€¬á€· **fundamentally better** á€–á€¼á€„á€·á€º approach á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

> **DQN á á€’á€¿á€”á€á€˜á€±á€¬:** "Reinforcement learning á€€á€­á€¯ supervised learning á€†á€„á€ºá€á€°á€œá€±á€¬á€€á€ºá€¡á€±á€¬á€„á€º á€˜á€šá€ºá€œá€­á€¯ á€œá€¯á€•á€ºá€™á€œá€²?"

---

## á‰.áƒ â€” DQN: RL á€€á€­á€¯ Supervised Learning á€†á€„á€ºá€á€°á€œá€¬á€…á€±á€á€¼á€„á€ºá€¸

### á‰.áƒ.á â€” Target Networks á€–á€¼á€„á€·á€º Targets á€€á€­á€¯ á€á€Šá€ºá€„á€¼á€­á€™á€ºá€¡á€±á€¬á€„á€ºá€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

**Idea:** Target values á€€á€­á€¯ compute á€•á€¼á€¯á€œá€¯á€•á€ºá€–á€­á€¯á€· **á€á€½á€²á€•á€¼á€¬á€¸á€á€±á€¬ network** á€á€…á€ºá€á€¯á€€á€­á€¯ fix á€•á€¼á€¯á€œá€¯á€•á€ºá€€á€¬ multiple steps á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€™á€Šá€º!

> **Target Network** = Online network á previous version á€–á€¼á€…á€ºá€•á€¼á€®á€¸ weights á€€á€­á€¯ periodically freeze á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ targets calculate á€›á€¬á€á€½á€„á€º á€á€¯á€¶á€¸á€•á€«á€á€šá€º

#### Target Network á€™á€•á€«á€˜á€² (Non-stationary issue):
```
Step 1: Î¸ optimize â†’ Q changes â†’ target Q(s') changes â†’ update invalid!
Step 2: Î¸ optimize â†’ Q changes â†’ target Q(s') changes â†’ update invalid!
... â†’ spiraling instability!
```

#### Target Network á€•á€«á€á€±á€¬ (Stable):
```
Step 1-15: Î¸ (online) optimize â†’ targets Q(s'; Î¸_target) fixed â†’ STABLE! âœ…
Step 16: Î¸_target â† Î¸ (copy weights), restart freeze
Step 17-31: Î¸ optimize again â†’ targets stable again âœ…
...
```

```mermaid
graph LR
    subgraph WITHOUT["Target Network á€™á€•á€«á€˜á€² âŒ"]
        Q1["Q(s,a;Î¸)"] -->|update| Q1b["Q(s,a;Î¸) changes"]
        Q1b -->|"target also changes!"| Q1c["Next target invalid"]
        Q1c -->|"chase tail..."| Q1d["DIVERGE"]
    end
    
    subgraph WITH["Target Network á€•á€«á€œá€»á€¾á€„á€º âœ…"]
        ON["Online Q(s,a;Î¸)"] -->|"update every step"| ON2["Online Q improves"]
        TG["Target Q(s,a;Î¸-)"] -->|"frozen for C steps"| TG2["Targets FIXED"]
        TG2 -->|"stable optimization"| ON2
        ON2 -->|"every C steps"| TG3["Copy Î¸ â†’ Î¸-"]
    end
    
    style WITHOUT fill:#ef5350,color:#fff
    style WITH fill:#4CAF50,color:#fff
```

**Target Network á Gradient Update:**

$$\theta \leftarrow \theta + \alpha \nabla_\theta \sum_{(s,a,r,s') \sim U(D)} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]$$

- $\theta$ = online network weights (update á€–á€¼á€…á€ºá€á€²á€· weights)
- $\theta^-$ = target network weights (freeze á€–á€¼á€…á€ºá€•á€¼á€®á€¸ periodically update)
- **á€€á€½á€¬á€á€¼á€¬á€¸á€™á€¾á€¯:** Gradient calculation á€á€½á€„á€º target computation á€™á€¾á€¬ older, frozen weights $\theta^-$ á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€º

> âš ï¸ **Terms:** Online network = Q(s,a;Î¸), Target network = Q(s,a;Î¸-). Single architecture, 2 different weight instances á€á€¬ á€–á€¼á€…á€ºá€•á€«á€á€šá€º

**Target network frequency hyperparameter:**
| Environment | Update frequency |
|---|---|
| Cart-pole (simple) | 10â€“20 steps |
| Atari (CNN) | 10,000 steps |

Target Networks á **trade-off:**
- **Stability â†‘** â€” frozen targets â†’ optimization converge á€•á€­á€¯á€›á€œá€½á€šá€º
- **Speed â†“** â€” outdated values á€•á€±á€«á€ºá€™á€¾á€¬ train á€”á€±á€›á€á€²á€·á€¡á€á€½á€€á€º learning slower

---

### á‰.áƒ.á‚ â€” Experience Replay á€–á€¼á€„á€·á€º Data á€€á€­á€¯ IID á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€ºá€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

**Idea:** Past experiences á€á€½á€±á€€á€­á€¯ **replay buffer** (memory) á€‘á€²á€á€­á€¯á€· store á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ random mini-batches sample á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º!

**0001 A Bit of History â€” Experience Replay á á€™á€½á€±á€¸á€–á€½á€¬á€¸á€•á€¯á€¶:**
Experience replay á€€á€­á€¯ **Long-Ji Lin** á€€ 1992 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching" paper á€á€½á€„á€º introduce á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º! á€Ÿá€¯á€á€ºá€•á€«á€á€šá€º â€” **1992** â€” neural networks á€€á€­á€¯ "connectionism" á€Ÿá€¯á€á€¬ á€á€±á€«á€ºá€”á€±á€€á€¼á€á€±á€¬á€€á€¬á€œ! Dr. Lin á€€ CMU á€™á€¾ PhD á€šá€°á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º companies á€™á€»á€¬á€¸á€á€½á€„á€º technical roles á€¡á€™á€»á€¬á€¸á€€á€¼á€­á€¯á€¸á€•á€«á€¸á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€•á€¼á€®á€¸ á€™á€€á€¼á€¬á€™á€® Signifyd á Chief Scientist á€–á€¼á€…á€ºá€•á€«á€á€šá€ºâ€”online fraud á€€á€­á€¯ predict/prevent á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ team á€€á€­á€¯ á€¦á€¸á€†á€±á€¬á€„á€ºá€•á€«á€á€šá€º!

```mermaid
graph LR
    ENV["Environment"] -->|"s, a, r, s'"| STORE["store(experience)"]
    STORE --> BUFFER["Replay Buffer D<br/>size: 10k - 1M"]
    BUFFER -->|"uniform random<br/>sample mini-batch"| TRAIN["Train Q(s,a;Î¸)"]
    TRAIN -->|"select action"| ENV
    
    style BUFFER fill:#ff922b,color:#fff
    style TRAIN fill:#2196F3,color:#fff
```

**Experience Replay á Benefits:**

1. **Data IID á€–á€¼á€…á€ºá€œá€¬á€•á€¼á€¯á€™á€°** â€” Multiple trajectories á€”á€¾á€„á€·á€º policies á€™á€¾ sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€€á€¼á€±á€¬á€„á€·á€º independent á€–á€¼á€…á€ºá€•á€¯á€¶á€•á€±á€«á€º
2. **More diverse mini-batches** â€” ÎFQ á€™á€¾ single trajectory 1,024 samples á€™á€Ÿá€¯á€á€ºá€á€±á€¬á€·á€˜á€² diverse past experiences
3. **One-step + larger effective dataset** â€” Every step áŒ train á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€¼á€®á€¸ large history á€€á€­á€¯ leverage á€•á€¼á€¯á€œá€¯á€•á€º
4. **Slower-moving targets** â€” Large buffer á€™á€¾ uniformly sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ â†’ targets slowly change

**Replay Buffer Design:**

| Parameter | Cart-pole | Atari |
|---|---|---|
| **Buffer size** | 50,000 | 1,000,000 |
| **Min samples (warmup)** | 320 | 50,000 |
| **Batch size** | 64 | 32 |
| **Eviction policy** | FIFO (oldest out) | FIFO |

**Formal notation:**

$$D = \{e_1, e_2, \ldots, e_M\}, \quad e_t = (S_t, A_t, R_{t+1}, S_{t+1})$$

Training: Sample uniformly $e_i \sim U(D)$ â†’ IID data á€†á€„á€ºá€á€°!

---

### Python Code â€” Replay Buffer

```python
import numpy as np

class ReplayBuffer():
    def __init__(self, 
                 m_size=50000,     # Maximum buffer size (50k for cart-pole)
                 batch_size=64):   # Training mini-batch size
        
        # Initialize storage arrays for each component
        self.ss_mem = np.empty(shape=(m_size), dtype=np.ndarray)  # states
        self.as_mem = np.empty(shape=(m_size), dtype=np.ndarray)  # actions
        self.rs_mem = np.empty(shape=(m_size), dtype=np.ndarray)  # rewards
        self.ps_mem = np.empty(shape=(m_size), dtype=np.ndarray)  # next states
        self.ds_mem = np.empty(shape=(m_size), dtype=np.ndarray)  # done flags
        
        self.m_size = m_size
        self.batch_size = batch_size
        self._idx = 0    # Next insertion index (circular)
        self.size = 0    # Current number of experiences stored
    
    def store(self, sample):
        """Experience tuple á€á€…á€ºá€á€¯á€€á€­á€¯ buffer á€‘á€²á€á€­á€¯á€· store á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        s, a, r, p, d = sample
        self.ss_mem[self._idx] = s
        self.as_mem[self._idx] = a
        self.rs_mem[self._idx] = r
        self.ps_mem[self._idx] = p
        self.ds_mem[self._idx] = d
        
        # Circular index â€” buffer full á€–á€¼á€…á€ºá€›á€„á€º oldest experience á€€á€­á€¯ overwrite
        self._idx += 1
        self._idx = self._idx % self.m_size   # wrap around (FIFO eviction)
        
        # Size grows until max, then stays at max
        self.size += 1
        self.size = min(self.size, self.m_size)
    
    def sample(self, batch_size=None):
        """Buffer á€™á€¾ random mini-batch sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        if batch_size is None:
            batch_size = self.batch_size
        
        # Uniform random sampling â€” IID appearance!
        idxs = np.random.choice(self.size, batch_size, replace=False)
        
        # Extract and return experiences
        experiences = (
            np.vstack(self.ss_mem[idxs]),   # states batch
            np.vstack(self.as_mem[idxs]),   # actions batch
            np.vstack(self.rs_mem[idxs]),   # rewards batch
            np.vstack(self.ps_mem[idxs]),   # next_states batch
            np.vstack(self.ds_mem[idxs])    # done_flags batch
        )
        return experiences
    
    def __len__(self):
        return self.size
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `m_size=50000` â€” cart-pole á€¡á€á€½á€€á€º 50k experiences max store
2. `batch_size=64` â€” default sampling batch size 64
3. Arrays á… á€á€¯ (s, a, r, s', d) á€€á€­á€¯ numpy arrays á€–á€¼á€„á€·á€º initialize
4. `store()` â€” experience á€€á€­á€¯ circular buffer á€‘á€² insert
5. `_idx % m_size` â€” buffer full á€–á€¼á€…á€ºá€•á€«á€€ oldest experience á€€á€­á€¯ overwrite (FIFO)
6. `size = min(size, m_size)` â€” size á€€á€­á€¯ max á€‘á€­á€á€¬ grow á€á€½á€„á€·á€ºá€•á€¼á€¯
7. `sample()` â€” uniform random sampling â†’ IID á€–á€¼á€…á€ºá€•á€¯á€¶á€•á€±á€«á€º
8. `np.vstack` â€” batch format á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º vertically stack

> ğŸ’¡ **High-dimensional environments (Atari) áŒ warning:** Images á€€á€­á€¯ naively store á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€„á€º 1M Ã— 4 frames Ã— 84Ã—84 = **28GB RAM** á€œá€­á€¯á€•á€«á€á€šá€º! Efficient storage techniques (lazy frames, uint8) á€€á€­á€¯ á€á€¯á€¶á€¸á€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€º!

---

### á‰.áƒ.áƒ â€” DQN á Full Algorithm

**DQN = NFQ + Experience Replay + Target Network**

**DQN á Gradient Update (Nature DQN):**

$$\theta \leftarrow \theta + \alpha \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right) \nabla_\theta Q(s, a; \theta)$$

where experiences $(s, a, r, s')$ are sampled uniformly from replay buffer $D$, and $\theta^-$ are the **frozen target network weights**.

**DQN Algorithm Steps:**

```mermaid
graph LR
    STEP1["Step 1:<br/>Collect experience<br/>(s, a, r, s', d)<br/>into replay buffer D"] --> STEP2["Step 2:<br/>Random sample<br/>mini-batch from D<br/>calculate TD targets<br/>r + Î³ max Q(s',a';Î¸-)"]
    STEP2 --> STEP3["Step 3:<br/>Fit Q(s,a;Î¸)<br/>using MSE + RMSprop<br/>(online network update)"]
    STEP3 -->|"every C steps"| UPDATE["Update Î¸- â† Î¸<br/>(target network sync)"]
    UPDATE --> STEP1
    STEP3 --> STEP1
    
    style STEP1 fill:#ff922b,color:#fff
    style STEP2 fill:#2196F3,color:#fff
    style STEP3 fill:#4CAF50,color:#fff
    style UPDATE fill:#9C27B0,color:#fff
```

**DQN vs NFQ Comparison:**

| Component | NFQ | DQN |
|---|---|---|
| Network | Single network | Online + Target networks |
| Targets | Same network (non-stationary) | Frozen target network (stationary â†‘) |
| Data | Mini-batch 1024 (correlated) | Replay buffer 50k (IID â†‘) |
| Training | Batch then K=40 fits | Every step, 1 gradient update |
| Exploration | Îµ=0.5 constant | Exponentially decaying (1.0â†’0.3) |
| LR | 0.0005 | 0.0005 |

---

### Python Code â€” DQN (Online + Target Networks)

```python
class DQN:
    def __init__(self, env, value_model_fn, value_optimizer_fn,
                 value_optimizer_lr, training_strategy_fn,
                 evaluation_strategy_fn, n_warmup_batches,
                 update_target_every_steps, max_gradient_norm):
        
        self.env = env
        self.gamma = 0.99
        
        nS = env.observation_space.shape[0]
        nA = env.action_space.n
        
        # TWO networks: online (trains every step) + target (periodically updated)
        self.online_model = value_model_fn(nS, nA)
        self.target_model = value_model_fn(nS, nA)
        
        self.optimizer = value_optimizer_fn(
            self.online_model.parameters(), lr=value_optimizer_lr)
        
        self.training_strategy = training_strategy_fn()
        self.evaluation_strategy = evaluation_strategy_fn()
        
        self.n_warmup_batches = n_warmup_batches
        self.update_target_every_steps = update_target_every_steps
        self.max_gradient_norm = max_gradient_norm
    
    def optimize_model(self, experiences):
        states, actions, rewards, next_states, is_terminals = experiences
        
        # === Target calculation using FROZEN target network! ===
        q_sp = self.target_model(next_states).detach()  # target network â† KEY!
        max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)
        max_a_q_sp *= (1 - is_terminals)
        target_q_sa = rewards + self.gamma * max_a_q_sp
        
        # === Current Q-values from ONLINE network (gradient flows here) ===
        q_sa = self.online_model(states).gather(1, actions)
        
        # === Loss: TD error squared (MSE) ===
        td_error = q_sa - target_q_sa
        value_loss = td_error.pow(2).mul(0.5).mean()
        
        # === Backpropagation through ONLINE network only ===
        self.optimizer.zero_grad()
        value_loss.backward()
        self.optimizer.step()
    
    def interaction_step(self, state, env):
        """Act using ONLINE network for exploration"""
        action = self.training_strategy.select_action(
            self.online_model, state)  # online network â†’ action selection
        new_state, reward, is_terminal, _ = env.step(action)
        return new_state, is_terminal
    
    def update_network(self):
        """Target network á€€á€­á€¯ online network á weights á€–á€¼á€„á€·á€º update á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        for target, online in zip(
                self.target_model.parameters(),
                self.online_model.parameters()):
            target.data.copy_(online.data)  # Î¸- â† Î¸ (full copy)
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. **Online + Target**: `online_model` á€”á€¾á€„á€·á€º `target_model` â€” same architecture, different weights
2. `target_model(next_states).detach()` â€” target network á€€á€”á€± Q(s') á€›á€šá€° â†’ **frozen, no gradient**
3. `online_model(states)` â€” current Q(s,a) â† **gradient flows here** (online network update)
4. `td_error.pow(2).mul(0.5)` â€” MSE loss = $\frac{1}{2}(Q(s,a;\theta) - y)^2$
5. `interaction_step` â€” action selection á€€á€­á€¯ **online network** á€–á€¼á€„á€·á€º á€•á€¼á€¯á€œá€¯á€•á€º (latest policy)
6. `update_network()` â€” every C steps áŒ weights á€€á€­á€¯ hard copy á€•á€¼á€¯á€œá€¯á€•á€º â†’ `Î¸- â† Î¸`

---

### DQN á History

**0001 A Bit of History:**
- **2013:** Volodymyr "Vlad" Mnih á€€ "Playing Atari with Deep Reinforcement Learning" paper á€á€½á€„á€º DQN **with experience replay** á€€á€­á€¯ introduce á€•á€¼á€¯á€œá€¯á€•á€º
- **2015:** "Human-level control through deep reinforcement learning" paper á€á€½á€„á€º DQN **with target networks** á€€á€­á€¯ add â†’ **Nature DQN** (Full DQN version)
- Vlad á€á€Šá€º Geoffrey Hinton (deep learning á founding fathers á€‘á€²á€™á€¾ á€á€¦á€¸) á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸
- Google DeepMind research scientist á€–á€¼á€…á€ºá€•á€¼á€®á€¸ 2017 MIT Technology Review **35 Innovators Under 35** list á€á€½á€„á€º á€•á€«á€á€„á€º

**DQN á Performance (Cart-pole):**
- **NFQ:** ~2,500 episodes á€”á€¾á€„á€·á€º ~250,000 steps á€œá€­á€¯á€•á€«á€á€šá€º
- **DQN:** ~250 episodes á€”á€¾á€„á€·á€º ~50,000 steps á€á€¬ á€œá€­á€¯á€•á€«á€á€šá€º
- â†’ **10x sample efficiency improvement!**

---

## á‰.á„ â€” Exploration Strategies

DQN á€á€½á€„á€º exploration strategies á€™á€¾á€¬ Chapter 4 á€™á€¾ á€™á€­á€á€ºá€†á€€á€ºá€‘á€¬á€¸á€á€±á€¬ strategies á€€á€­á€¯ neural networks á€–á€¼á€„á€·á€º adapt á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ use á€•á€«á€á€šá€º:

### á‰.á„.á â€” Linearly Decaying Îµ-greedy

```python
class EGreedyLinearStrategy:
    """Îµ á€€á€­á€¯ linear curve á€–á€¼á€„á€·á€º decay á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
    
    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, max_steps=20000):
        self.init_epsilon = init_epsilon
        self.min_epsilon = min_epsilon
        self.max_steps = max_steps
        self.epsilon = init_epsilon
        self.t = 0
    
    def _epsilon_update(self):
        # Linear decay formula
        self.epsilon = 1 - self.t / self.max_steps
        self.epsilon = (self.init_epsilon - self.min_epsilon) * \
                       self.epsilon + self.min_epsilon
        # Clip to valid range
        self.epsilon = np.clip(self.epsilon, 
                               self.min_epsilon, self.init_epsilon)
        self.t += 1
        return self.epsilon
    
    def select_action(self, model, state):
        self.exploratory_action = False
        
        with torch.no_grad():
            q_values = model(state).cpu().detach()
            q_values = q_values.data.numpy().squeeze()
        
        if np.random.rand() > self.epsilon:
            action = np.argmax(q_values)    # exploit
        else:
            action = np.random.randint(len(q_values))  # explore
        
        self._epsilon_update()
        self.exploratory_action = action != np.argmax(q_values)
        return action
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Îµ á€€á€­á€¯ `init_epsilon=1.0` á€™á€¾ `min_epsilon=0.1` á€‘á€­ linear á€–á€¼á€„á€·á€º decay
2. `t / max_steps` ratio á€–á€¼á€„á€·á€º step by step Ï„ decrease
3. `clip` á€–á€¼á€„á€·á€º valid range á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º ensure
4. `exploratory_action` variable â€” logging á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º (% of exploratory actions per episode)

### â‚‰.á„.á‚ â€” Exponentially Decaying Îµ-greedy (DQN á€™á€¾á€¬ á€á€¯á€¶á€¸á€á€±á€¬)

```python
class EGreedyExpStrategy:
    """Îµ á€€á€­á€¯ exponential curve á€–á€¼á€„á€·á€º decay á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
    
    def __init__(self, init_epsilon=1.0, min_epsilon=0.3, 
                 decay_rate=0.9995):
        self.init_epsilon = init_epsilon
        self.min_epsilon = min_epsilon
        self.decay_rate = decay_rate
        self.epsilon = init_epsilon
    
    def _epsilon_update(self):
        # Exponential decay: Îµ = max(min_Îµ, decay_rate Ã— Îµ)
        self.epsilon = max(self.min_epsilon, 
                          self.decay_rate * self.epsilon)
        return self.epsilon
    
    def select_action(self, model, state):
        self.exploratory_action = False
        
        with torch.no_grad():
            q_values = model(state).cpu().detach()
            q_values = q_values.data.numpy().squeeze()
        
        if np.random.rand() > self.epsilon:
            action = np.argmax(q_values)
        else:
            action = np.random.randint(len(q_values))
        
        self._epsilon_update()
        self.exploratory_action = action != np.argmax(q_values)
        return action
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `decay_rate Ã— Îµ` á€–á€¼á€„á€·á€º exponential decrease (curve faster than linear)
2. `max(min_epsilon, ...)` â€” minimum floor á€€á€­á€¯ ensure
3. DQN á€™á€¾á€¬: Îµ = 1.0 â†’ 0.3 in ~20,000 steps

### á‰.á„.áƒ â€” Softmax Exploration Strategy

```python
class SoftMaxStrategy:
    """Temperature parameter á€–á€¼á€„á€·á€º softmax á€€á€­á€¯ action selection áŒ á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸"""
    
    def __init__(self, init_temp=1.0, min_temp=0.01, 
                 explore_ratio=0.8, max_steps=20000):
        self.init_temp = init_temp
        self.min_temp = min_temp
        self.explore_ratio = explore_ratio
        self.max_steps = max_steps
        self.t = 0
    
    def _update_temp(self):
        temp = 1 - self.t / (self.max_steps * self.explore_ratio)
        temp = (self.init_temp - self.min_temp) * temp + self.min_temp
        temp = np.clip(temp, self.min_temp, self.init_temp)
        self.t += 1
        return temp
    
    def select_action(self, model, state):
        temp = self._update_temp()  # Get current temperature
        
        with torch.no_grad():
            q_values = model(state).cpu().detach()
            q_values = q_values.data.numpy().squeeze()
            
            # Softmax with temperature scaling
            scaled_qs = q_values / temp      # scale by temperature
            norm_qs = scaled_qs - scaled_qs.max()  # subtract max (numerical stability)
            e = np.exp(norm_qs)
            probs = e / np.sum(e)            # normalize to probabilities
            assert np.isclose(probs.sum(), 1.0)
        
        # Sample action proportional to Q-values
        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]
        return action
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Temperature parameter `Ï„`:
   - `Ï„ â†’ 0`: Q-values á€€á€¼á€¬á€¸á€á€¼á€¬á€¸á€”á€¬á€¸á€á€»á€€á€º magnify â†’ more greedy
   - `Ï„ = 1`: Original Q-values distribution â†’ balanced
   - `Ï„ â†’ âˆ`: All actions equal probability â†’ full random
2. `q_values / temp` â€” temperature scaling
3. `norm_qs - max` â€” overflow prevent (numerical stability)
4. `np.exp(norm_qs) / sum(exp)` â€” softmax formula

$$P(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}$$

**NFQ vs DQN Exploration:**
| Method | Exploration Strategy | Îµ value |
|---|---|---|
| **NFQ** | Constant Îµ-greedy | Îµ = 0.5 (constant) |
| **DQN** | Exponentially decaying | Îµ: 1.0 â†’ 0.3 (~20k steps) |

> ğŸ’¡ **Effective greedy probability (Cart-pole, 2 actions):**
> - Îµ = 0.5 â†’ Greedy probability = 75%, Exploratory = 25%
> - Îµ = 0.3 â†’ Greedy probability = 85%, Exploratory = 15%
> - Large action space á€™á€¾á€¬ non-greedy action probability increases significantly!

---

## á‰.á… â€” DDQN: Overestimation Bias á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸

DQN á€€á€±á€¬á€„á€ºá€¸á€•á€±á€™á€šá€·á€º **overestimation bias** problem á€á€…á€ºá€™á€»á€­á€¯á€¸ á€›á€¾á€­á€á€±á€¸á€•á€«á€á€šá€º! Chapter 6 á€™á€¾ Q-learning á same problem á€•á€²á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

### â‚‰.â‚….1 â€” Overestimation á á€•á€¼á€¿á€”á€¬

**á€•á€¼á€¿á€”á€¬:** TD target á€‘á€² $\max_{a'} Q(s', a'; \theta)$ á€€á€­á€¯á€á€¯á€¶á€¸á€á€²á€·á€¡á€á€½á€€á€ºá€€á€¼á€±á€¬á€„á€·á€º estimated values á€™á€¾á€¬ **positive bias** á€›á€¾á€­á€•á€«á€á€šá€º!

Estimated values á€á€¿ true values á€€á€”á€± off-center á€–á€¼á€…á€ºá€•á€¼á€®á€¸ above/below á€–á€¼á€…á€ºá€€á€¼á€•á€«á€á€šá€ºá‹ **max**á€€á€­á€¯á€á€¬ á€›á€½á€±á€¸á€á€»á€šá€ºá€á€²á€·á€¡á€á€½á€€á€º overestimated values á€€á€­á€¯ prefer á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€ºá€–á€¼á€…á€ºá€•á€¼á€®á€¸ performance á€€á€»á€†á€„á€ºá€¸á€•á€«á€á€šá€º!

**Miguel á Casino Analogy:**

> DQN agent á€Ÿá€¬ extremely optimistic á€–á€¼á€…á€ºá€á€²á€· DQN á€Ÿá€¯á€á€±á€«á€ºá€á€±á€¬ á€œá€°á€á€…á€ºá€šá€±á€¬á€€á€ºá€Ÿá€¯ imagine á€•á€«: 
>
> DQN casino á€€á€­á€¯ á€•á€‘á€™á€†á€¯á€¶á€¸ á€á€½á€¬á€¸á€•á€¼á€®á€¸ **slot machine jackpot** á€›á€•á€«á€á€šá€º! Optimistic á€–á€¼á€…á€ºá€á€²á€· DQN á€€ á€á€»á€€á€ºá€á€»á€„á€ºá€¸ "casino á€á€½á€¬á€¸á€á€¬ á€€á€±á€¬á€„á€ºá€¸á€á€šá€º (Q(s,a) high!)" á€Ÿá€¯ values á€€á€­á€¯ update á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€ºâ€”á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬ next state á€™á€¾á€¬ slot machines á€€á€­á€¯ play á€•á€¼á€®á€¸ jackpot á€›á€”á€­á€¯á€„á€ºá€á€²á€· `max_a' Q(s',a')` á€€á€­á€¯ á€€á€¼á€Šá€·á€ºá€á€²á€·á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º!
>
> **Problems:**
> - DQN á€á€Šá€º casino á€€á€­á€¯ á€á€½á€¬á€¸á€á€­á€¯á€„á€ºá€¸ slot machine á€™á€á€½á€¬á€¸á€á€±á€¸á€á€„á€º â€” roulette/poker/blackjack á€€á€­á€¯ explore á€á€á€ºá€•á€«á€á€šá€º
> - Slot machine area á€á€Šá€º sometimes maintenance á€–á€¼á€„á€·á€º á€•á€­á€á€ºá€‘á€¬á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º (environment is stochastic)
> - Slot machines á€€á€­á€¯ play á€á€¿á€á€¿á€™á€¾á€¬ jackpot á€›á€á€²á€· probability á€á€Šá€º very small á€•á€«!
>
> â†’ **Overoptimistic estimation = bad strategy planning!**

### â‚‰.â‚….â‚‚ â€” Argmax Unwrapping

Max operator á€€á€­á€¯ unwrap á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ bias á source á€€á€­á€¯ identify á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€•á€«á€™á€Šá€º:

**Standard DQN target:**

$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

**Unwrapped form (equivalent):**

$$y = r + \gamma Q\left(s', \underbrace{\arg\max_{a'} Q(s', a'; \theta^-)}_{\text{action selection}};\ \underbrace{\theta^-}_{\text{action evaluation}}\right)$$

**á€•á€¼á€¿á€”á€¬:** Action selection á€”á€¾á€„á€·á€º action evaluation á€€á€­á€¯ **same network** á€€á€”á€± á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º â†’ same direction áŒ bias á€›á€¾á€­ â†’ two questions á€€á€­á€¯ biased source á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€€á€”á€± á€–á€¼á€±á€™á€Šá€·á€ºá€•á€¯á€¶!

```python
# DQN target (original)
q_sp = self.target_model(next_states).detach()
max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)
max_a_q_sp *= (1 - is_terminals)
target_q_sa = rewards + self.gamma * max_a_q_sp

# DQN target (unwrapped - mathematically equivalent)
argmax_a_q_sp = self.target_model(next_states).max(1)[1]  # SELECT action
q_sp = self.target_model(next_states).detach()
max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp]   # EVALUATE action
max_a_q_sp = max_a_q_sp.unsqueeze(1)
max_a_q_sp *= (1 - is_terminals)
target_q_sa = rewards + self.gamma * max_a_q_sp
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Top block: Standard DQN â€” `.max(1)[0]` á€–á€¼á€„á€·á€º max value á€€á€­á€¯ directly á€›á€šá€°
2. Bottom block: Unwrapped â€” `.max(1)[1]` á€–á€¼á€„á€·á€º argmax index á€€á€­á€¯ á€›á€šá€°á€•á€¼á€®á€¸ value á€€á€­á€¯ separately á€›á€šá€°
3. Both blocks: **Mathematically identical** (same Q-function á€€á€”á€± select + evaluate)
4. Unwrapping á€á€Šá€º DDQN improvement á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€›á€œá€½á€šá€ºá€á€²á€· stepping stone á€–á€¼á€…á€ºá€•á€«á€á€šá€º

### â‚‰.â‚….â‚ƒ â€” DDQN Solution: Network á‚ á€á€¯á€–á€¼á€„á€·á€º Cross-validate

**Key insight:** Action **selection** á€”á€¾á€„á€·á€º action **evaluation** á€€á€­á€¯ **á€€á€½á€²á€•á€¼á€¬á€¸á€á€±á€¬ networks á‚ á€á€¯** á€–á€¼á€„á€·á€º split á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€™á€Šá€º!

**Practical DDQN solution:**
- **Online network Î¸** â†’ Action SELECTION (argmax) â€” "Which action is best?"
- **Target network Î¸-** â†’ Action EVALUATION (Q-value) â€” "How good is that action?"

$$y = r + \gamma Q\left(s', \underbrace{\arg\max_{a'} Q(s', a'; \theta)}_{\text{online: SELECT}};\ \underbrace{\theta^-}_{\text{target: EVALUATE}}\right)$$

```mermaid
graph TD
    subgraph DQN_OLD["DQN âŒ â€” Same network for both"]
        DQNS["Target Q(s',a';Î¸-)"] -->|"select action"| DQNA["argmax action"]
        DQNS -->|"evaluate action"| DQNV["Q-value"]
        DQNA -->|"same bias direction"| BIAS["OVERESTIMATE ğŸ“ˆ"]
    end
    
    subgraph DDQN_NEW["DDQN âœ… â€” Different networks"]
        DDQNO["Online Q(s',a';Î¸)"] -->|"SELECT: which action?"| DDQNA["argmax index"]
        DDQNT["Target Q(s',a';Î¸-)"] -->|"EVALUATE: what value?"| DDQNV["Q-value for that action"]
        DDQNA --> DDQNV
        DDQNV -->|"cross-validate"| NOBI["Less bias â†“"]
    end
    
    style DQN_OLD fill:#ef5350,color:#fff
    style DDQN_NEW fill:#4CAF50,color:#fff
```

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º online â†’ select, target â†’ evaluate á€†á€­á€¯á€á€²á€· ordering á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€œá€²?**
- Online network á€–á€¼á€„á€·á€º select: Latest knowledge á€–á€¼á€„á€·á€º best action á€€á€­á€¯ identify
- Target network á€–á€¼á€„á€·á€º evaluate: Frozen, stable values á€–á€¼á€„á€·á€º estimate â†’ stability âœ…
- Reversed (target select, online evaluate) á€†á€­á€¯á€›á€„á€º values are from online (changing every step) â†’ unstable!

---

### Python Code â€” DDQN

```python
class DDQN:
    def optimize_model(self, experiences):
        states, actions, rewards, next_states, is_terminals = experiences
        batch_size = len(is_terminals)
        
        # === DDQN: Action SELECTION from ONLINE network ===
        # Online network â†’ "Which action is best in s'?"
        argmax_a_q_sp = self.online_model(next_states).max(1)[1]
        # Note: .max(1)[1] returns indices (argmax), not values. No need to detach.
        
        # === DDQN: Action EVALUATION from TARGET network ===
        # Target network â†’ "What is the value of that action?"
        q_sp = self.target_model(next_states).detach()  # frozen target
        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp]  # index by online's choice
        max_a_q_sp = max_a_q_sp.unsqueeze(1)
        max_a_q_sp *= (1 - is_terminals)   # terminal states â†’ 0
        
        # === TD target using cross-validated values ===
        target_q_sa = rewards + (self.gamma * max_a_q_sp)
        
        # === Current Q-values from ONLINE network (gradient here) ===
        q_sa = self.online_model(states).gather(1, actions)
        td_error = q_sa - target_q_sa
        value_loss = td_error.pow(2).mul(0.5).mean()
        
        # === Optimize ONLINE network ===
        self.optimizer.zero_grad()
        value_loss.backward()        
        self.optimizer.step()
    
    def interaction_step(self, state, env):
        """Online network á€–á€¼á€„á€·á€º action select (same as DQN)"""
        action = self.training_strategy.select_action(
            self.online_model, state)  # online network
        new_state, reward, is_terminal, _ = env.step(action)
        return new_state, is_terminal
    
    def update_network(self):
        """Target network á€€á€­á€¯ periodically sync (same as DQN)"""
        for target, online in zip(
                self.target_model.parameters(),
                self.online_model.parameters()):
            target.data.copy_(online.data)  # Î¸- â† Î¸
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `online_model(next_states).max(1)[1]` â€” online network á€€á€”á€± argmax action INDEX á€€á€­á€¯ á€›á€šá€° (no detach needed â€” indices not differentiable)
2. `target_model(next_states).detach()` â€” target network á€€á€”á€± Q-values á€€á€­á€¯ frozen á€–á€¼á€„á€·á€º á€›á€šá€°
3. `q_sp[np.arange(batch_size), argmax_a_q_sp]` â€” online's recommended action index á€–á€¼á€„á€·á€º target's Q-value á€€á€­á€¯ index
4. Remaining steps: DQN á€”á€¾á€„á€·á€º identical (TD target â†’ loss â†’ optimize â†’ update)
5. **Key difference from DQN:** Line 1 = `online_model` not `target_model` for argmax!

**DDQN á History:**

**0001 A Bit of History:**
Double DQN á€€á€­á€¯ 2015 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Hado van Hasselt** á€€ introduce á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º (Nature DQN á shortly after)!

- **2010:** Hado á€á€Šá€º tabular RL áŒ **double Q-learning** algorithm á€€á€­á€¯ author á€•á€¼á€¯á€œá€¯á€•á€º (Chapter 6 á€™á€¾ algorithm!)
- **2015:** DQN á€€á€­á€¯ function approximation á€–á€¼á€„á€·á€º double learning extend = **DDQN**
- 2015 á€á€½á€„á€º Atari domain áŒ **state-of-the-art** results á€›á€›á€¾á€­
- Hado = University of Utrecht (Netherlands) PhD in AI/RL â†’ Google DeepMind research scientist

---

## â‚‰.â‚† â€” Loss Functions: MSE á€™á€¾ Huber á€†á€®

### â‚‰.â‚†.â‚ â€” Loss Function Types

RL á€á€½á€„á€º loss function á€›á€½á€±á€¸á€á€»á€šá€ºá€™á€¾á€¯á€á€Šá€º important á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” targets uncertain á€–á€¼á€…á€ºá€•á€¼á€®á€¸ agents incorrect early á€–á€¼á€…á€ºá€™á€Šá€ºá€Ÿá€¯ expect á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

```mermaid
graph LR
    MSE["MSE / L2<br/>âˆ‘(Å· - y)Â²<br/>Quadratic"] -->|"pros"| MSE_P["Gradients â†’ 0 near optima<br/>Smooth optimization"]
    MSE -->|"cons"| MSE_C["Penalizes large errors heavily<br/>Sensitive to outliers"]
    
    MAE["MAE / L1<br/>âˆ‘|Å· - y|<br/>Linear"] -->|"pros"| MAE_P["Robust to outliers<br/>Treats all errors equally"]
    MAE -->|"cons"| MAE_C["Constant gradient<br/>(hard to converge near optima)"]
    
    HUBER["Huber Loss<br/>Best of both!"] -->|"< Î´"| H1["Quadratic (MSE)"]
    HUBER -->|"> Î´"| H2["Linear (MAE)"]
    
    style HUBER fill:#4CAF50,color:#fff
```

**Loss function formal definitions:**

$$\mathcal{L}_{MSE}(\theta) = \frac{1}{N}\sum_i (Q(s_i,a_i;\theta) - y_i)^2$$

$$\mathcal{L}_{MAE}(\theta) = \frac{1}{N}\sum_i |Q(s_i,a_i;\theta) - y_i|$$

$$\mathcal{L}_{Huber}(\theta) = \frac{1}{N}\sum_i \begin{cases} \frac{1}{2}(Q - y)^2 & \text{if } |Q - y| \leq \delta \\ \delta \cdot (|Q - y| - \frac{1}{2}\delta) & \text{if } |Q - y| > \delta \end{cases}$$

**RL á€á€½á€„á€º Huber Loss á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€Šá€·á€º á€¡á€€á€¼á€±á€¬á€„á€ºá€¸:**

| Aspect | RL áŒ Impact |
|---|---|
| **Large errors early in training** | Expected! Agent á€™á€á€­á€á€²á€· states á€™á€»á€¬á€¸ â† Robust to outliers (MAE behavior) á€œá€­á€¯á€•á€«á€á€šá€º |
| **Near-optimal late in training** | Fine-grained updates á€œá€­á€¯á€•á€«á€á€šá€º â† Quadratic gradients (MSE behavior) á€€á€±á€¬á€„á€ºá€¸ |
| **Moving targets** | Aggressive gradient á€€á€­á€¯ avoid á€–á€­á€¯á€· outlier robustness á€œá€­á€¯á€•á€«á€á€šá€º |

**Î´ hyperparameter:**
- Î´ â†’ 0: MAE á€–á€¼á€…á€ºá€á€½á€¬á€¸á€•á€«á€á€šá€º
- Î´ â†’ âˆ: MSE á€–á€¼á€…á€ºá€á€½á€¬á€¸á€•á€«á€á€šá€º
- Typical: Î´ = 1

### â‚‰.â‚†.â‚‚ â€” Gradient Clipping Implementation

**Practical approach:** MSE loss á€€á€­á€¯ calculate á€•á€¼á€®á€¸ gradients á€€á€­á€¯ clip á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

```python
def optimize_model(self, experiences):
    states, actions, rewards, next_states, is_terminals = experiences
    batch_size = len(is_terminals)
    
    # === DDQN targets (same as before) ===
    argmax_a_q_sp = self.online_model(next_states).max(1)[1]
    q_sp = self.target_model(next_states).detach()
    max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp]
    max_a_q_sp = max_a_q_sp.unsqueeze(1)
    max_a_q_sp *= (1 - is_terminals)
    target_q_sa = rewards + (self.gamma * max_a_q_sp)
    q_sa = self.online_model(states).gather(1, actions)
    
    # === Calculate MSE loss ===
    td_error = q_sa - target_q_sa
    value_loss = td_error.pow(2).mul(0.5).mean()
    
    # === Backpropagation ===
    self.optimizer.zero_grad()
    value_loss.backward()
    
    # === Gradient Clipping (Huber loss equivalent) ===
    # max_gradient_norm = float('inf') â†’ effectively MSE (no clipping)
    # max_gradient_norm = 1.0 â†’ clip gradients above magnitude 1
    torch.nn.utils.clip_grad_norm_(
        self.online_model.parameters(),
        self.max_gradient_norm)   # â† hyperparameter: how aggressively to clip
    
    self.optimizer.step()
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. MSE loss á€€á€­á€¯ calculate (`td_error.pow(2).mul(0.5).mean()`)
2. `.backward()` á€–á€¼á€„á€·á€º gradients compute
3. `clip_grad_norm_` â€” gradient magnitude á€€á€­á€¯ `max_gradient_norm` á€€á€»á€±á€¬á€ºá€•á€«á€€ clip
4. DDQN experiments á€™á€¾á€¬: `max_gradient_norm = float('inf')` â†’ effectively MSE (no clipping)
5. Challenging environments á€™á€¾á€¬: `max_gradient_norm = 1.0` â†’ gradient clipping active

> âš ï¸ **Reward Clipping vs Gradient Clipping vs Q-value Clipping:**
> - **Reward clipping:** Rewards á€€á€­á€¯ clip (e.g., [-1, 1] range) â€” different!
> - **Gradient clipping:** Gradient magnitudes á€€á€­á€¯ clip â† á€’á€«á€€á€­á€¯ DDQN á€á€¯á€¶á€¸
> - **Q-value clipping:** Q-values á€€á€­á€¯ clip â€” **mistake!** á€™á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€”á€¾á€„á€·á€º

---

## â‚‰.â‚‡ â€” Full DDQN Algorithm Summary

### DDQN á Components

| Component | DQN | DDQN |
|---|---|---|
| **Value function** | $Q(s, a; \theta)$ | $Q(s, a; \theta)$ |
| **Architecture** | 4 â†’ 512 â†’ 128 â†’ 2 | 4 â†’ 512 â†’ 128 â†’ 2 |
| **Target** | Off-policy TD (same network) | Off-policy TD (cross-validated) |
| **Action selection** | Target network | **Online network** |
| **Action evaluation** | Target network | Target network |
| **Loss** | MSE | Adjustable Huber (effectively MSE) |
| **Optimizer** | RMSprop, lr=0.0005 | RMSprop, **lr=0.0007** |
| **Exploration** | Exp decay Îµ (1.0â†’0.3) | Exp decay Îµ (1.0â†’0.3) |
| **Replay buffer** | 50k, batch=64 | 50k, batch=64 |
| **Target update** | Every 15 steps | Every 15 steps |

> âš ï¸ **LR difference:** DDQN á€™á€¾á€¬ higher LR (0.0007) work á€•á€«á€á€šá€º â€” double learning á€€á€¼á€±á€¬á€„á€·á€º more stable á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º! DQN á€™á€¾á€¬ 0.0007 á€–á€¼á€„á€·á€º test á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€„á€º some seeds failá‹

**DDQN Gradient Update (Math):**

$$\theta \leftarrow \theta + \alpha \nabla_\theta \mathbb{E}_{(s,a,r,s') \sim U(D)} \left[ \left( r + \gamma Q\!\left(s', \arg\max_{a'} Q(s',a';\theta);\ \theta^- \right) - Q(s,a;\theta) \right)^2 \right]$$

- Experienes sampled uniformly from buffer $D$
- Online weights $\theta$ â†’ action selection
- Target weights $\theta^-$ â†’ action evaluation

---

## â‚‰.â‚ˆ â€” á€†á€€á€ºá€œá€€á€ºá€á€­á€¯á€¸á€á€€á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬á€”á€±á€›á€¬á€™á€»á€¬á€¸ (What's Next)

DDQN á€á€Šá€º strong algorithm á€–á€¼á€…á€ºá€•á€¼á€®á€¸ Atari games á€™á€»á€¬á€¸á€á€½á€„á€º superhuman performance á€›á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º! á€’á€«á€•á€±á€™á€šá€·á€º still improvements á€›á€”á€­á€¯á€„á€ºá€•á€«á€á€±á€¸á€á€šá€º:

### â‚‰.â‚ˆ.â‚ â€” Network Architecture Improvement

Current: State-in, values-out (naive Q-function representation)

**Observation:** Q-values á€¹ state $s$ áŒ actions á€¡á€¬á€¸á€œá€¯á€¶á€¸á€á€Šá€º **same state** á€€á€”á€± indexed á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

$$Q(s, 0) \text{ á€”á€¾á€„á€·á€º } Q(s, 1) \text{ á€á€Šá€º same } V(s) \text{ á€•á€±á€«á€ºá€™á€¾á€¬ depend á€•á€«á€á€šá€º!}$$

**Question:** Q(s, 0) á€€á€”á€± Q(s, 1) á€¡á€€á€¼á€±á€¬á€„á€ºá€¸ learn á€”á€­á€¯á€„á€ºá€™á€Šá€ºá€œá€¬á€¸?

**Answer:** Chapter 10 áŒ **Dueling Network Architecture** â€” $V(s)$ á€”á€¾á€„á€·á€º $A(s,a)$ á€€á€­á€¯ separately learn á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ $Q(s,a) = V(s) + A(s,a)$ á€Ÿá€¯ decompose á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º!

### â‚‰.â‚ˆ.â‚‚ â€” Experience Replay Sampling Improvement

Current: Uniform random sampling from replay buffer

**Observation:** Humans replay **important experiences** more â€” unexpected successes/failures!

**Question:** All samples equal importance á€–á€¼á€„á€·á€º treat á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¬ optimal á€–á€¼á€…á€ºá€™á€Šá€ºá€œá€¬á€¸?

**Answer:** Chapter 10 áŒ **Prioritized Experience Replay (PER)** â€” larger TD errors á€›á€¾á€­á€á€±á€¬ experiences á€€á€­á€¯ more frequently sample á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º!

```mermaid
graph TD
    DQN["DQN<br/>(Experience Replay + Target Network)"] --> DDQN["DDQN<br/>(+ Double Learning)"]
    DDQN -->|"Chapter 10"| DUELING["Dueling DDQN<br/>(+ V(s) + A(s,a) architecture)"]
    DDQN -->|"Chapter 10"| PER["Prioritized Experience Replay<br/>(+ Importance Sampling)"]
    
    style DQN fill:#2196F3,color:#fff
    style DDQN fill:#4CAF50,color:#fff
    style DUELING fill:#9C27B0,color:#fff
    style PER fill:#ff922b,color:#fff
```

---

## â‚‰.â‚‰ â€” Key Equations Summary

| Equation | Formula |
|---|---|
| **DQN TD target** | $y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$ |
| **DDQN TD target** | $y = r + \gamma Q(s', \arg\max_{a'} Q(s',a';\theta); \theta^-)$ |
| **MSE Loss** | $\mathcal{L} = \frac{1}{N}\sum_i (Q(s_i,a_i;\theta) - y_i)^2$ |
| **Huber Loss** | $= \frac{1}{2}(Q-y)^2$ if $\|Q-y\|\leq\delta$ else $\delta(\|Q-y\|-\frac{\delta}{2})$ |
| **Target update** | $\theta^- \leftarrow \theta$ (every C steps, hard copy) |
| **Softmax action** | $P(a\|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}$ |
| **Replay buffer** | $D = \{e_1, \ldots, e_M\}$, sample $e_i \sim U(D)$ |

---

## â‚‰.áá€ â€” Algorithm Performance Comparison

```mermaid
graph LR
    subgraph PERF["Cart-pole Performance Comparison"]
        NFQ_P["NFQ<br/>~2,500 episodes<br/>~250,000 steps<br/>~80 seconds"] 
        DQN_P["DQN<br/>~250 episodes<br/>~50,000 steps<br/>~5 minutes"]
        DDQN_P["DDQN<br/>~250 episodes<br/>~50,000 steps<br/>Tighter bounds!"]
    end
    
    NFQ_P -->|"10x sample efficient"| DQN_P
    DQN_P -->|"Similar speed<br/>More STABLE"| DDQN_P
    
    style NFQ_P fill:#9E9E9E,color:#fff
    style DQN_P fill:#2196F3,color:#fff
    style DDQN_P fill:#4CAF50,color:#fff
```

**Performance Summary:**
- **Sample Efficiency:** DDQN â‰ˆ DQN > NFQ (10x improvement)
- **Stability:** DDQN > DQN > NFQ (tighter performance bounds)
- **Speed:** DQN â‰ˆ DDQN â‰ˆ 5 minutes (similar wall-clock time)
- **Consistency across seeds:** DDQN best (narrowest min-max bounds)

---

## â‚‰.11 â€” á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

á€’á€® chapter á€™á€¾á€¬ value-based deep RL methods á€€á€­á€¯ significantly á€•á€­á€¯á€á€Šá€ºá€„á€¼á€­á€™á€ºá€¡á€±á€¬á€„á€º á€†á€±á€¬á€„á€ºá€›á€½á€€á€ºá€›á€¬ techniques á€™á€»á€­á€¯á€¸á€…á€¯á€¶á€€á€­á€¯ learn á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º:

**á€á€„á€ºá€šá€°á€á€²á€·á€á€Šá€ºá€™á€»á€¬á€¸:**

| Technique | Problem Solved | Method |
|---|---|---|
| **Experience Replay** | Data NOT IID | Store & uniformly sample from buffer |
| **Target Network** | Non-stationary targets | Freeze weights for C steps |
| **Double Learning** | Overestimation bias | Separate action select/evaluation |
| **Huber/Grad Clipping** | Large early errors | Linear loss for outliers, quadratic near-zero |
| **Decaying Îµ-greedy** | Exploration balance | High Îµ early (explore), low Îµ later (exploit) |

```mermaid
graph TD
    PROB1["IID Violation<br/>(correlated data)"] -->|"solved by"| ER["Experience Replay<br/>Buffer size: 50k<br/>Uniform sampling"]
    PROB2["Non-stationary targets<br/>(moving target)"] -->|"solved by"| TN["Target Network<br/>Freeze for C=15 steps<br/>Î¸- â† Î¸ periodically"]
    PROB3["Overestimation bias<br/>(positive bias)"] -->|"solved by"| DL["Double Learning<br/>Online â†’ select<br/>Target â†’ evaluate"]
    
    ER --> DQN["DQN"]
    TN --> DQN
    DQN --> DDQN["DDQN"]
    DL --> DDQN
    
    style PROB1 fill:#ef5350,color:#fff
    style PROB2 fill:#ef5350,color:#fff
    style PROB3 fill:#ef5350,color:#fff
    style ER fill:#4CAF50,color:#fff
    style TN fill:#4CAF50,color:#fff
    style DL fill:#4CAF50,color:#fff
    style DQN fill:#2196F3,color:#fff
    style DDQN fill:#9C27B0,color:#fff
```

**Chapter 10 Preview:**
- **Dueling DDQN** â€” $Q(s,a) = V(s) + A(s,a)$ á€€á€­á€¯ explicitly model á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€· smarter architecture
- **Prioritized Experience Replay (PER)** â€” Important experiences á€€á€­á€¯ more frequently sample á€•á€¼á€¯á€œá€¯á€•á€º â†’ sample efficiency â†‘
- â†’ **Sample-efficient value-based methods!**

> ğŸ’¡ **Deadly Triad á€€á€­á€¯ á€á€á€­á€•á€¼á€¯á€•á€«:**
> Neural networks + Bootstrapping (TD) + Off-policy learning = **potentially unstable combination**!
> DQN/DDQN á techniques á€á€½á€±á€€ á€’á€® instability á€€á€­á€¯ practical á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º mitigate á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€· engineering solutions á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
>
