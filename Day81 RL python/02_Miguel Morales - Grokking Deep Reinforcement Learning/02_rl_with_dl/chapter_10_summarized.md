# Chapter 10: Sample-Efficient Value-Based Methods - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

Chapter 9 á€™á€¾á€¬ DQN/DDQN á€–á€¼á€„á€·á€º stability á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€® Chapter á€™á€¾á€¬ **sample efficiency** á€€á€­á€¯ focus á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ episodes á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸á€–á€¼á€„á€·á€º á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€±á€¬ policy á€€á€­á€¯ learn á€–á€­á€¯á€· techniques á‚ á€á€¯á€€á€­á€¯ introduce á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

1. **Dueling DDQN** â€” Q-function á€€á€­á€¯ V(s) + A(s,a) á€Ÿá€¯ split á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ RL-aware architecture
2. **Prioritized Experience Replay (PER)** â€” Surprising experiences á€€á€­á€¯ more frequently sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

```mermaid
graph LR
    DDQN["DDQN<br/>(Ch 9)"] -->|"+ Dueling Architecture<br/>+ Polyak Averaging"| DUAL["Dueling DDQN"]
    DUAL -->|"+ Priority Replay<br/>+ IS Weights"| PER["Dueling DDQN + PER"]
    
    style DDQN fill:#2196F3,color:#fff
    style DUAL fill:#4CAF50,color:#fff
    style PER fill:#9C27B0,color:#fff
```

| Algorithm | Main Focus |
|---|---|
| **NFQ (Ch 8)** | Deep RL first attempt |
| **DQN (Ch 9)** | Stability (target net + replay) |
| **DDQN (Ch 9)** | Reduce overestimation bias |
| **Dueling DDQN (Ch 10)** | Sample efficiency via architecture |
| **PER (Ch 10)** | Sample efficiency via smarter replay |

---

## 2. Dueling DDQN: RL-Aware Architecture

### Value Functions á Relationship

Q-function á€€á€­á€¯ V(s) á€”á€¾á€„á€·á€º A(s,a) á€–á€¼á€„á€·á€º decompose á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º:

$$Q(s, a) = V(s) + A(s, a)$$

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

$$\mathbb{E}_{a \sim \pi}\left[A^\pi(s, a)\right] = 0$$

| Function | Meaning | Output Size |
|---|---|---|
| **$V(s)$** | State á overall goodness | scalar (1 value) |
| **$A(s,a)$** | Action $a$ á€€ default á€‘á€€á€º á€™á€Šá€ºá€™á€»á€¾ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸ | per-action (n_actions values) |
| **$Q(s,a)$** | State $s$ á€™á€¾ action $a$ á expected return | per-action (n_actions values) |

### Insight: V(s) á€á€Šá€º All Actions á€–á€¼á€„á€·á€º Shared

```mermaid
graph TD
    subgraph NAIVE["Naive Q-network âŒ"]
        E1["Experience (s, a1, r)"] --> U1["Update Q(s,a1) only"]
        U1 -.->|"indirect only"| U2["Q(s,a2) barely learns"]
    end
    
    subgraph DUAL["Dueling Network âœ…"]
        E2["Experience (s, a1, r)"] --> VS["Update V(s)"]
        E2 --> AS["Update A(s,a1)"]
        VS -->|"shared by ALL actions"| Q1["Q(s,a1) improved!"]
        VS -->|"shared by ALL actions"| Q2["Q(s,a2) ALSO improved!"]
        AS --> Q1
    end
    
    style NAIVE fill:#ef5350,color:#fff
    style DUAL fill:#4CAF50,color:#fff
```

**Cart-pole á€¥á€•á€™á€¬:**

```
State: pole almost vertical â†’ V(s) HIGH, A(left) â‰ˆ A(right) â‰ˆ 0
â†’ Q(left) â‰ˆ Q(right) HIGH (both fine!)

State: pole falling right  â†’ V(s) LOW, A(right) >> A(left)
â†’ Q(right) high (save it!), Q(left) << 0 (bad!)
```

> ğŸ’¡ V(s) update á€á€…á€ºá€á€¯á€–á€¼á€„á€·á€º actions **á€¡á€¬á€¸á€œá€¯á€¶á€¸** á€†á€® information á€•á€¼á€”á€·á€ºá€”á€¾á€¶á€· â†’ per-sample á€›á€›á€¾á€­á€á€±á€¬ information á€á€­á€¯á€¸á€™á€¼á€¾á€„á€·á€º â†’ **sample efficiency â†‘**

### Dueling Architecture

```mermaid
graph LR
    INPUT["Input [4]"] --> S1["Hidden 512<br/>(shared)"]
    S1 --> S2["Hidden 128<br/>(shared)"]
    S2 --> VSTR["Value Stream<br/>1 node â†’ V(s)"]
    S2 --> ASTR["Advantage Stream<br/>2 nodes â†’ A(s,a)"]
    VSTR -->|"V(s)"| AGG["Q = V(s) + A(s,a) âˆ’ mean(A)"]
    ASTR -->|"A(s,a)"| AGG
    AGG --> OUT["Q-values [2]"]
    
    style INPUT fill:#ff922b,color:#fff
    style S1 fill:#64B5F6,color:#fff
    style S2 fill:#64B5F6,color:#fff
    style VSTR fill:#4CAF50,color:#fff
    style ASTR fill:#9C27B0,color:#fff
    style AGG fill:#ef5350,color:#fff
    style OUT fill:#2196F3,color:#fff
```

### Aggregation Equation

**NaÃ¯ve (don't use):**
$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + A(s, a; \theta, \alpha)$$

Problem: Vì™€ A á€€á€­á€¯ Q á€™á€¾ uniquely recover á€™á€› â†’ identifiability issue!

**Practical approach (mean subtraction):**

$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left(A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s, a'; \theta, \alpha)\right)$$

Mean subtraction á€–á€¼á€„á€·á€º V/A á€€á€­á€¯ constantly shifted á€–á€¼á€…á€ºá€…á€±á€•á€¼á€®á€¸ optimization á€€á€­á€¯ stabilize á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º!

### FCDuelingQ Implementation

```python
class FCDuelingQ(nn.Module):
    def __init__(self, input_dim, output_dim, 
                 hidden_dims=(32, 32), activation_fc=F.relu):
        super(FCDuelingQ, self).__init__()
        self.activation_fc = activation_fc
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            self.hidden_layers.append(
                nn.Linear(hidden_dims[i], hidden_dims[i+1]))
        # Two output heads (dueling)
        self.value_output = nn.Linear(hidden_dims[-1], 1)            # V(s): 1 node
        self.advantage_output = nn.Linear(hidden_dims[-1], output_dim)  # A(s,a): N nodes

    def forward(self, state):
        x = self._format(state)
        x = self.activation_fc(self.input_layer(x))
        for hidden_layer in self.hidden_layers:
            x = self.activation_fc(hidden_layer(x))
        a = self.advantage_output(x)   # [batch, n_actions]
        v = self.value_output(x)       # [batch, 1]
        v = v.expand_as(a)             # broadcast to match A shape
        q = v + a - a.mean(1, keepdim=True).expand_as(a)  # Q = V + A - mean(A)
        return q
```

> ğŸ’¡ Cart-pole architecture: `4 â†’ 512 â†’ 128 â†’ {V: 1, A: 2}` â†’ final Q output: 2 values

---

## 3. Polyak Averaging

### Hard Update vs Polyak Averaging

| Method | Formula | Behavior |
|---|---|---|
| **Hard Update (DQN)** | $\theta^- \leftarrow \theta$ (every C steps) | Sudden full copy, then frozen |
| **Polyak Averaging** | $\theta^- \leftarrow (1-\tau)\theta^- + \tau\theta$ (every step) | Smooth continuous lag |

$$\theta^- \leftarrow (1 - \tau)\theta^- + \tau\theta \quad (\tau = 0.1)$$

- $\tau = 0.1$ â†’ 10% online + 90% target mix every step
- Stable, gradual update â†’ no sudden loss landscape shift
- $\tau = 1.0$ â†’ hard copy (equivalent to DQN)

```python
def update_network(self, tau=None):
    tau = self.tau if tau is None else tau
    for target, online in zip(self.target_model.parameters(),
                               self.online_model.parameters()):
        target.data.copy_((1.0 - tau) * target.data + tau * online.data)
```

---

## 4. Dueling DDQN Summary

| Component | DDQN | Dueling DDQN |
|---|---|---|
| **Network** | FCQ (Q-only) | **FCDuelingQ (V + A streams)** |
| **Target update** | Hard copy every 15 steps | **Polyak averaging Ï„=0.1 every step** |
| **Double learning** | âœ… | âœ… |
| **Optimizer** | RMSprop lr=0.0007 | RMSprop lr=0.0007 |
| **Replay buffer** | 50k, uniform | 50k, uniform |

**Performance vs DDQN:**
- Episodes: similar
- Training bounds: **narrower** (more stable)
- Cart position variance: **lower** (better "tie-breaking" in near-equal states)

> ğŸ’¡ History: Ziyu Wang (2015, Oxford PhD â†’ DeepMind) "Dueling Network Architectures for Deep Reinforcement Learning" â€” RL á€€á€­á€¯ RL-specific architecture á€–á€¼á€„á€·á€º custom design á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ á€•á€‘á€™á€†á€¯á€¶á€¸ paper!

---

## 5. Prioritized Experience Replay (PER)

### Uniform Sampling á á€•á€¼á€¿á€”á€¬

```mermaid
graph LR
    UBUF["Uniform Buffer"] -->|"equal probability"| UNI["Unimportant experience"]
    UBUF -->|"equal probability"| IMP["Important experience"]
    UNI -->|"wastes compute"| GRAD["Gradient update"]
    IMP -->|"useful learning"| GRAD
    
    style UNI fill:#ef5350,color:#fff
    style IMP fill:#4CAF50,color:#fff
```

Uniform sampling â†’ unimportant experiences á€€á€­á€¯ equally sample â†’ resources waste!

### TD Error á€€á€­á€¯ Surprise Measure á€¡á€–á€¼á€…á€ºá€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸

$$\delta_i = \left| Q(s_i, a_i; \theta) - y_i \right| \quad \text{(absolute TD error)}$$

- **High $|\delta|$** â†’ expectation á€”á€¾á€„á€·á€º reality á€€á€¼á€®á€¸á€…á€½á€¬ á€€á€½á€¬ â†’ high surprise â†’ more to learn!
- **Low $|\delta|$** â†’ agent already knew this â†’ less informative

### Stochastic Prioritization

**Method 1: Proportional Prioritization**

$$p_i = |\delta_i| + \varepsilon \quad (\varepsilon \approx 0.01)$$

$\varepsilon$ â‰  0 á€†á€­á€¯á€á€±á€¬á€€á€¼á€±á€¬á€„á€·á€º zero-TD-error experiences á€€á€­á€¯á€œá€Šá€ºá€¸ replay chance á€•á€±á€¸á€•á€«á€á€šá€º!

**Method 2: Rank-based Prioritization**

$$p_i = \frac{1}{\text{rank}(i)} \quad \text{(sort by } |\delta| \text{ descending, rank 1 = highest)}$$

**Priority â†’ Probability (both methods):**

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

| Î± value | Behavior |
|---|---|
| **$\alpha = 0$** | Uniform sampling |
| **$\alpha = 1$** | Full priority sampling |
| **$\alpha = 0.6$** | Blended (used in practice) âœ… |

| Method | Outlier sensitivity | Speed |
|---|---|---|
| **Proportional** | Sensitive | Faster |
| **Rank-based** | Robust | Slower (needs sort) |

---

## 6. Importance Sampling Weights

### Bias Problem

Priority distribution $P(i) \neq U(D)$ á€–á€¼á€„á€·á€º sample â†’ **biased gradient estimates!**

RL update expectation á€á€Šá€º true data distribution á€™á€¾ á€–á€¼á€…á€ºá€›á€•á€«á€™á€Šá€º â€” PER á€–á€¼á€„á€·á€º á€’á€«á€€á€­á€¯ violate á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º!

### IS Weight Formula

$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$

$$\tilde{w}_i = \frac{w_i}{\max_j w_j}$$

| Î² value | Behavior |
|---|---|
| **$\beta = 0$** | No correction (pure PER) |
| **$\beta = 1$** | Full correction (unbiased) |
| **Annealing: 0.1 â†’ 1.0** | Gradually correct as training progresses âœ… |

**Î² Annealing:** Training á á€•á€‘á€™á€•á€­á€¯á€„á€ºá€¸ â†’ exploration dominant â†’ bias correction á€™á€œá€­á€¯á€á€±á€¸. Later â†’ exploitation â†’ full correction á€œá€­á€¯á€¡á€•á€º.

### PER Loss Function

$$\mathcal{L}(\theta) = \frac{1}{N}\sum_i \tilde{w}_i \cdot \delta_i^2 \cdot \frac{1}{2}$$

```python
# Without PER
value_loss = td_error.pow(2).mul(0.5).mean()

# With PER (IS-weighted)
value_loss = (weights * td_error).pow(2).mul(0.5).mean()
```

---

## 7. PrioritizedReplayBuffer Implementation

```python
class PrioritizedReplayBuffer:
    def store(self, sample):
        # New experience â†’ maximum priority (guarantee at least one replay)
        priority = self.memory[:self.n_entries, 0].max() if self.n_entries > 0 else 1.0
        self.memory[self.next_index, 0] = priority       # store td_error
        self.memory[self.next_index, 1] = np.array(sample)  # store sample
        self.n_entries = min(self.n_entries + 1, self.max_samples)
        self.next_index = (self.next_index + 1) % self.max_samples

    def update(self, idxs, td_errors):
        # Update TD errors after each training step
        self.memory[idxs, 0] = np.abs(td_errors)
        if self.rank_based:
            sorted_arg = self.memory[:self.n_entries, 0].argsort()[::-1]
            self.memory[:self.n_entries] = self.memory[sorted_arg]

    def sample(self, batch_size=None):
        self._update_beta()  # anneal Î² toward 1.0
        entries = self.memory[:self.n_entries]
        # Calculate probabilities
        if self.rank_based:
            priorities = 1 / (np.arange(self.n_entries) + 1)
        else:
            priorities = entries[:, 0] + EPS
        scaled = priorities ** self.alpha
        probs = scaled / scaled.sum()
        # IS weights
        weights = (self.n_entries * probs) ** (-self.beta)
        norm_weights = weights / weights.max()
        # Sample
        idxs = np.random.choice(self.n_entries, batch_size, replace=False, p=probs)
        return np.vstack(idxs), np.vstack(norm_weights[idxs]), samples_stacks
```

**Key design decisions:**
1. `store`: new experience â†’ **max priority** â†’ at least once replay guarantee
2. `update`: training í›„ TD errors update â†’ future sampling ì— ë°˜ì˜
3. `sample`: Î² anneal + IS weight normalize â†’ max weight = 1

---

## 8. PER Full Algorithm

### 4 Steps (Dueling DDQN á 3 steps + 1 new step)

```mermaid
graph LR
    S1["1. Collect experience<br/>Insert with MAX priority"] --> S2["2. Sample mini-batch<br/>Priority-weighted<br/>Get: idxs, IS weights, samples"]
    S2 --> S3["3. Fit Q(s,a;Î¸)<br/>IS-weighted MSE<br/>+ Double learning targets"]
    S3 --> S4["4. Update priorities<br/>New  Î´  â†’ replay buffer<br/>(new step!)"]
    S4 -->|"Polyak Î¸-"| S1
    
    style S1 fill:#ff922b,color:#fff
    style S2 fill:#2196F3,color:#fff
    style S3 fill:#4CAF50,color:#fff
    style S4 fill:#9C27B0,color:#fff
```

### Hyperparameters

| Parameter | Dueling DDQN | PER (Dueling DDQN + PER) |
|---|---|---|
| Architecture | 4 â†’ 512 â†’ 128 â†’ {V:1, A:2} | Same |
| Target update | Polyak Ï„=0.1 every step | Same |
| Buffer size | 50,000 | **10,000** (smaller!) |
| Priority Î± | â€” | 0.6 |
| IS Î² initial | â€” | 0.1 |
| Î² anneal rate | â€” | 0.99992 (~30k steps) |
| Min samples | 320 | 320 |
| Batch size | 64 | 64 |

---

## 9. Performance Comparison (All 5 Algorithms)

| Algorithm | Episodes to solve | Sample efficiency | Stability |
|---|---|---|---|
| **NFQ** | ~2,500 | Low | Noisy |
| **DQN** | ~250 | 10Ã— NFQ | Medium |
| **DDQN** | ~250 | ~DQN | Better |
| **Dueling DDQN** | slightly < DDQN | â†‘ | Best among 4 |
| **PER** | **fewest** | **Highest** | Very tight bounds |

> âš ï¸ PER á implementation slowness: Array sorting O(N log N) á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º! SumTree data structure á€–á€¼á€„á€·á€º O(log N) á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º optimize á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º (algorithmic issue á€™á€Ÿá€¯á€á€ºâ€”implementation issue)

```mermaid
graph TD
    subgraph PERF["Performance Ranking (Cart-pole)"]
        P5["5. NFQ<br/>~2,500 episodes"] --> P4["4. DQN<br/>~250 episodes"]
        P4 --> P3["3. DDQN<br/>~250 episodes"]
        P3 --> P2["2. Dueling DDQN<br/>< DDQN episodes"]
        P2 --> P1["1. PER<br/>fewest episodes âœ…"]
    end
    
    style P1 fill:#4CAF50,color:#fff
    style P5 fill:#9E9E9E,color:#fff
```

---

## 10. Value-Based Methods á Limitations

### Hyperparameter Sensitivity
Value-based methods á€á€½á€±á€á€Šá€º hyperparameters á€€á€­á€¯ sensitive á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Bad hyperparameter space á€€á€­á€¯ good space á€‘á€€á€º á€•á€­á€¯á€™á€»á€¬á€¸ meet á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

### The Deadly Triad

```mermaid
graph TD
    BOOT["Bootstrapping<br/>(TD Learning â† must)"] --- OFFPOL["Off-policy Learning<br/>(Q-learning â† must)"]
    OFFPOL --- FA["Function Approximation<br/>(Neutral Networks â† must)"]
    FA --- BOOT
    BOOT -->|"three combined"| DT["DEADLY TRIAD<br/>âš ï¸ Known divergence risk!"]
    
    style DT fill:#ef5350,color:#fff
```

Target networks + Replay buffers + Double learning á€–á€¼á€„á€·á€º mitigate á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€±á€™á€šá€·á€º **fundamental divergence risk á€™á€–á€šá€ºá€›á€¾á€¬á€¸á€”á€­á€¯á€„á€º** â€” open research problem!

---

## 11. Key Equations Summary

| Equation | Formula |
|---|---|
| **Q decomposition** | $Q(s,a) = V(s) + A(s,a)$ |
| **Advantage** | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ |
| **Dueling aggregation** | $Q = V(s) + \left(A(s,a) - \frac{1}{\|\mathcal{A}\|}\sum_{a'} A(s,a')\right)$ |
| **Polyak averaging** | $\theta^- \leftarrow (1-\tau)\theta^- + \tau\theta$ |
| **TD priority** | $p_i = \|\delta_i\| + \varepsilon$ (proportional) |
| **Rank priority** | $p_i = 1/\text{rank}(i)$ (rank-based) |
| **Priority â†’ Probability** | $P(i) = p_i^\alpha / \sum_k p_k^\alpha$ |
| **IS weight** | $w_i = (N \cdot P(i))^{-\beta}$ / $\max_j w_j$ |
| **PER loss** | $\mathcal{L} = \frac{1}{N}\sum_i \tilde{w}_i \cdot \delta_i^2 / 2$ |

---

## 12. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **Value function decomposition**: $Q = V + A$ â†’ architecture á€€á€­á€¯ RL-aware á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º design
2. **Dueling architecture**: V stream + A stream â†’ single experience á€–á€¼á€„á€·á€º all Q-values improve
3. **Polyak averaging**: Target network á€€á€­á€¯ smooth, continuous á€–á€¼á€„á€·á€º update â†’ stability â†‘
4. **PER**: TD error = surprise â†’ important experiences á€€á€­á€¯ more frequently replay
5. **IS weights**: Priority sampling á bias á€€á€­á€¯ Î² annealing á€–á€¼á€„á€·á€º correct
6. **Deadly triad**: Bootstrapping + off-policy + function approximation â†’ theoretical divergence risk

```mermaid
graph TD
    subgraph CH10["Chapter 10 Summary"]
        DUAL2["Dueling DDQN<br/>âœ… V+A split architecture<br/>âœ… Polyak averaging"]
        PER2["PER<br/>âœ… Priority = TD error surprise<br/>âœ… IS weight bias correction"]
    end
    
    DUAL2 --> SE1["Sample Efficiency â†‘"]
    PER2 --> SE2["Sample Efficiency â†‘â†‘"]
    SE1 --> CH11["Chapter 11: Policy-Gradient Methods<br/>REINFORCE, VPG, A2C, GAE..."]
    SE2 --> CH11
    
    style DUAL2 fill:#4CAF50,color:#fff
    style PER2 fill:#9C27B0,color:#fff
    style CH11 fill:#2196F3,color:#fff
```

> ğŸ’¡ History notes: Ziyu Wang (2015, Oxford PhD) â†’ Dueling architecture | Tom Schaul (2015, DeepMind) â†’ PER | Both released same year, often combined as "Rainbow DQN" components!
