# Chapter 9: More Stable Value-Based Methods - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

Chapter 8 á€™á€¾á€¬ NFQ á€–á€¼á€„á€·á€º deep RL á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€¼á€®á€¸ IID violation á€”á€¾á€„á€·á€º non-stationary targets á€•á€¼á€¿á€”á€¬á€á€½á€± á€›á€¾á€­á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€® Chapter á€™á€¾á€¬ **DQN** (**Deep Q-Network**) á€”á€²á€· **Double DQN (DDQN)** algorithms á€–á€¼á€„á€·á€º á€•á€­á€¯á€™á€­á€¯ stable á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€ºá€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    NFQ["NFQ<br/>(Ch 8)"] -->|"+ Target Network<br/>+ Experience Replay<br/>+ Larger Networks"| DQN["DQN"]
    DQN -->|"+ Double Learning<br/>+ Huber Loss"| DDQN["DDQN"]
    
    style NFQ fill:#ff922b,color:#fff
    style DQN fill:#2196F3,color:#fff
    style DDQN fill:#4CAF50,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Target networks** â€” targets á€€á€­á€¯ stabilize á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
2. **Experience replay** â€” data á€€á€­á€¯ IID á€•á€¯á€¶á€…á€¶ á€–á€”á€ºá€á€®á€¸á€á€¼á€„á€ºá€¸
3. **Larger networks** â€” state aliasing á€œá€»á€¾á€±á€¬á€·á€á€»á€á€¼á€„á€ºá€¸
4. **Double DQN** â€” overestimation bias á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸
5. **Huber loss** â€” MSE á€‘á€€á€º robust loss function
6. **Exploration strategies** â€” linear decay, exponential decay, softmax

---

## 2. DQN: RL á€€á€­á€¯ Supervised Learning á€•á€¯á€¶á€…á€¶ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸

### Problems á‚ á€á€¯ (á€¡á€“á€­á€€á€•á€¼á€¿á€”á€¬)

```mermaid
graph TD
    subgraph P1["Problem 1: Data Not IID"]
        P1A["Samples are sequential<br/>s_t â†’ s_{t+1} (correlated)"]
        P1B["Policy changes<br/>â†’ distribution changes"]
    end
    
    subgraph P2["Problem 2: Non-stationary Targets"]
        P2A["Target = r + Î³ max Q(s',a';Î¸)"]
        P2B["Î¸ update â†’ target changes"]
        P2C["Chasing moving target!"]
    end
    
    P1 --> SOL1["Solution: Experience Replay"]
    P2 --> SOL2["Solution: Target Network"]
    
    SOL1 --> DQN["DQN Algorithm"]
    SOL2 --> DQN
    
    style P1 fill:#ef5350,color:#fff
    style P2 fill:#ef5350,color:#fff
    style SOL1 fill:#4CAF50,color:#fff
    style SOL2 fill:#4CAF50,color:#fff
    style DQN fill:#2196F3,color:#fff
```

| Issue | Supervised Learning | Reinforcement Learning |
|---|---|---|
| **Data** | Shuffled dataset, IID | Sequential, correlated |
| **Distribution** | Fixed | Changes as Ï€ improves |
| **Targets** | Fixed labels | Move with every Î¸ update |
| **Training** | Stable convergence | Can diverge |

---

## 3. Target Network

### Concept

Target network á€†á€­á€¯á€á€¬ online network á€›á€²á€· **freeze** á€œá€¯á€•á€ºá€‘á€¬á€¸á€á€²á€· copy á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€•á€¼á€®á€¸ target values á€€á€­á€¯ calculate á€–á€­á€¯á€· á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph WITHOUT["âŒ Without Target Network"]
        W1["Q-function improves"] --> W2["Targets move"]
        W2 --> W3["Optimizer chases<br/>moving target"]
        W3 --> W4["Possible divergence!"]
    end
    
    subgraph WITH["âœ… With Target Network"]
        T1["Freeze target for N steps"] --> T2["Optimizer makes<br/>stable progress"]
        T2 --> T3["Update target, repeat"]
        T3 --> T4["Stable convergence"]
    end
    
    style WITHOUT fill:#ef5350,color:#fff
    style WITH fill:#4CAF50,color:#fff
```

### Implementation

```python
# Target network á€€á€­á€¯ online network á€›á€²á€· weights á€–á€¼á€„á€·á€º update
def update_network(self):
    for target, online in zip(
        self.target_model.parameters(),
        self.online_model.parameters()):
        target.data.copy_(online.data)  # copy online â†’ target
```

### Update Frequency

| Environment | Update Frequency |
|---|---|
| **Cart-Pole** (simple) | Every 10-20 steps |
| **Atari** (image-based) | Every 10,000 steps |

> ğŸ’¡ Target network á€€á€­á€¯ freeze á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º supervised learning á€›á€²á€· fixed targets á€•á€¯á€¶á€…á€¶ simulate á€–á€”á€ºá€á€®á€¸á€•á€«á€á€šá€ºá‹ Stability á€€á€±á€¬á€„á€ºá€¸á€•á€±á€™á€šá€·á€º learning speed á€”á€¾á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

## 4. Experience Replay

### Replay Buffer Concept

Agent á€›á€²á€· experiences á€á€½á€±á€€á€­á€¯ buffer á€‘á€²á€á€­á€™á€ºá€¸á€•á€¼á€®á€¸ uniformly at random sample á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

$$D = \{e_1, e_2, \ldots, e_M\}, \quad e_t = (S_t, A_t, R_{t+1}, S_{t+1}, D_{t+1})$$

```mermaid
graph LR
    ENV["Environment"] -->|"experience tuple<br/>(s,a,r,s',d)"| BUF["Replay Buffer<br/>D (size M)"]
    BUF -->|"uniformly sample<br/>mini-batch"| TRAIN["Train Q-network"]
    TRAIN -->|"select action"| ENV
    
    style BUF fill:#ffd43b,color:#000
    style TRAIN fill:#2196F3,color:#fff
    style ENV fill:#ff922b,color:#fff
```

### Benefits
1. **Data looks IID** â€” multiple trajectories/policies á€€á€”á€± random sample á€›á€á€¬á€€á€¼á€±á€¬á€„á€·á€º independent á€•á€¯á€¶ á€•á€±á€«á€º
2. **Reduced variance** â€” diverse mini-batches á€–á€¼á€„á€·á€º update
3. **Sample reuse** â€” experience sample á€á€…á€ºá€á€¯á€€á€­á€¯ multiple times train á€–á€­á€¯á€· á€á€¯á€¶á€¸á€”á€­á€¯á€„á€º
4. **More stable targets** â€” large buffer á€–á€¼á€„á€·á€º targets slowly change

### Implementation

```python
class ReplayBuffer():
    def __init__(self, m_size=50000, batch_size=64):
        self.ss_mem = np.empty(shape=(m_size), dtype=np.ndarray)
        self.as_mem = np.empty(shape=(m_size), dtype=np.ndarray)
        self.rs_mem = np.empty(shape=(m_size), dtype=np.ndarray)
        self.ps_mem = np.empty(shape=(m_size), dtype=np.ndarray)
        self.ds_mem = np.empty(shape=(m_size), dtype=np.ndarray)
        self.m_size, self.batch_size = m_size, batch_size
        self._idx, self.size = 0, 0
    
    def store(self, sample):
        s, a, r, p, d = sample
        self.ss_mem[self._idx] = s
        self.as_mem[self._idx] = a
        self.rs_mem[self._idx] = r
        self.ps_mem[self._idx] = p
        self.ds_mem[self._idx] = d
        self._idx = (self._idx + 1) % self.m_size  # circular buffer
        self.size = min(self.size + 1, self.m_size)
    
    def sample(self, batch_size=None):
        if batch_size is None:
            batch_size = self.batch_size
        idxs = np.random.choice(self.size, batch_size, replace=False)
        return (np.vstack(self.ss_mem[idxs]),
                np.vstack(self.as_mem[idxs]),
                np.vstack(self.rs_mem[idxs]),
                np.vstack(self.ps_mem[idxs]),
                np.vstack(self.ds_mem[idxs]))
```

| Buffer Parameter | Value |
|---|---|
| **Min samples** | 320 |
| **Max capacity** | 50,000 |
| **Batch size** | 64 |

> ğŸ’¡ Experience replay á€€á€­á€¯ 1992 á€á€¯á€”á€¾á€…á€ºá€€á€á€Šá€ºá€¸á€€ Long-Ji Lin á€€ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º! DQN (2013/2015) á€€ á€’á€® technique á€€á€­á€¯ deep RL á€™á€¾á€¬ effectively á€¡á€á€¯á€¶á€¸á€á€»á€•á€¼á€á€²á€·á€•á€«á€á€šá€ºá‹

---

## 5. Exploration Strategies

### Strategy Comparison

```mermaid
graph TD
    EXP["Exploration Strategies"] --> CONST["Constant Îµ-greedy<br/>Îµ = 0.5 (NFQ)"]
    EXP --> LINEAR["Linearly Decaying Îµ-greedy<br/>Îµ: 1.0 â†’ 0.3"]
    EXP --> EXPO["Exponentially Decaying Îµ-greedy<br/>Îµ: 1.0 â†’ 0.3 âœ…"]
    EXP --> SOFT["Softmax<br/>temp: high â†’ low"]
    
    style EXPO fill:#4CAF50,color:#fff
    style CONST fill:#ff922b,color:#fff
```

### Linearly Decaying Îµ-greedy

$$\epsilon_t = (\epsilon_{\text{init}} - \epsilon_{\text{min}}) \cdot \left(1 - \frac{t}{t_{\max}}\right) + \epsilon_{\text{min}}$$

### Exponentially Decaying Îµ-greedy (DQN Default)

$$\epsilon_t = \max(\epsilon_{\text{min}}, \; \lambda \cdot \epsilon_{t-1})$$

- $\lambda$ = decay rate
- $\epsilon_{\text{init}}$ = 1.0, $\epsilon_{\text{min}}$ = 0.3
- Roughly 20,000 steps á€–á€¼á€„á€·á€º decay

### Softmax Strategy

$$P(a_i \mid s) = \frac{e^{Q(s, a_i) / \tau}}{\sum_j e^{Q(s, a_j) / \tau}}$$

- $\tau$ â†’ 0: greedy á€•á€­á€¯á€–á€¼á€…á€º
- $\tau$ = 1: value differences á€¡á€á€­á€¯á€„á€ºá€¸
- $\tau$ â†’ âˆ: uniform random

```python
# Softmax select_action
scaled_qs = q_values / temp
norm_qs = scaled_qs - scaled_qs.max()  # overflow prevention
e = np.exp(norm_qs)
probs = e / np.sum(e)
action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]
```

> ğŸ’¡ DQN á€”á€¾á€„á€·á€º DDQN experiments á€™á€¾á€¬ **exponentially decaying Îµ-greedy** á€€á€­á€¯ default á€á€¯á€¶á€¸á€•á€«á€á€šá€ºá‹ Exploration strategy á€›á€²á€· hyperparameters á€€ performance á€€á€­á€¯ significant á€á€€á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯ á€›á€¾á€­á€•á€«á€á€šá€ºá‹

---

## 6. Full DQN Algorithm

### DQN Architecture Summary

```mermaid
graph TD
    subgraph AGENT["DQN Agent"]
        ONLINE["Online Network<br/>Q(s,a; Î¸)"]
        TARGET["Target Network<br/>Q(s,a; Î¸â»)"]
        BUF["Replay Buffer<br/>D (50,000)"]
        STRAT["Exp-Decay Îµ-greedy"]
    end
    
    ENV["Environment"] -->|"(s,a,r,s',d)"| BUF
    BUF -->|"sample batch=64"| TRAIN["Calculate TD targets<br/>using Target Network"]
    TRAIN --> LOSS["MSE Loss + RMSprop"]
    LOSS --> ONLINE
    ONLINE -->|"copy every 15 steps"| TARGET
    STRAT --> ONLINE
    ONLINE -->|"action"| ENV
    
    style ONLINE fill:#2196F3,color:#fff
    style TARGET fill:#9C27B0,color:#fff
    style BUF fill:#ffd43b,color:#000
```

### DQN Hyperparameters

| Parameter | Value |
|---|---|
| Architecture | State-in Values-out (4, 512, 128, 2) |
| Objective | Approximate $q^*(s,a)$ |
| Target | Off-policy TD: $r + \gamma \max_{a'} Q(s', a'; \theta^-)$ |
| Exploration | Exp-decay Îµ-greedy (1.0 â†’ 0.3, ~20k steps) |
| Loss | MSE |
| Optimizer | RMSprop (lr = 0.0005) |
| Buffer | min=320, max=50,000, batch=64 |
| Target update | Every 15 time steps |

### DQN Steps

1. **Collect**: experience $(S_t, A_t, R_{t+1}, S_{t+1}, D_{t+1})$ â†’ insert into replay buffer
2. **Sample**: mini-batch from buffer â†’ calculate TD targets using **target network**
3. **Fit**: optimize online Q-network with MSE + RMSprop

### DQN vs NFQ Results

| Metric | NFQ | DQN |
|---|---|---|
| **Episodes to solve** | ~2,500 | ~250 |
| **Experience tuples** | ~250,000 | ~50,000 |
| **Sample efficiency** | Low | **10Ã— better** |
| **Stability** | Noisy | More stable |
| **Wall-clock time** | ~5 min | ~5 min |

> ğŸ’¡ DQN á€€ NFQ á€‘á€€á€º **10 á€†** sample efficient á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Target network + experience replay á€›á€²á€· combined effect á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 7. Double DQN (DDQN)

### Overestimation Problem

Q-learning á€›á€²á€· max operator á€€ estimated values á€‘á€²á€€ highest á€€á€­á€¯ á€šá€°á€á€¬á€€á€¼á€±á€¬á€„á€·á€º **positive bias** á€›á€¾á€­á€•á€«á€á€šá€º:

$$\max_{a'} Q(s', a'; \theta) \geq Q(s', a^*; \theta)$$

á€¡á€“á€­á€€ á€•á€¼á€¿á€”á€¬:
- Estimated values á€¡á€¬á€¸á€œá€¯á€¶á€¸ imperfect (á€¡á€á€»á€­á€¯á€· higher, á€¡á€á€»á€­á€¯á€· lower)
- `max` á€€á€­á€¯ á€šá€°á€á€¬á€€ **always higher values á€€á€­á€¯ prefer** â†’ positive bias
- á€’á€«á€€ overoptimistic value estimates á€–á€¼á€…á€ºá€…á€± â†’ performance á€€á€»

### Unwrapping the Max Operator

$$\max_{a'} Q(s', a') = Q\big(s', \arg\max_{a'} Q(s', a')\big)$$

á€’á€«á€†á€­á€¯á€›á€„á€º questions á‚ á€á€¯ á€–á€¼á€…á€ºá€á€šá€º:
1. **Action selection**: $\arg\max_{a'} Q(s', a')$ â€” á€˜á€šá€º action á€€ best?
2. **Action evaluation**: $Q(s', a^*)$ â€” á€’á€® action á€›á€²á€· value á€˜á€šá€ºá€œá€±á€¬á€€á€º?

### DQN vs DDQN Target

```mermaid
graph TD
    subgraph DQN_T["DQN Target"]
        DQN1["Target Network<br/>selects action (argmax)"]
        DQN2["Target Network<br/>evaluates action"]
        DQN1 --> DQN3["Same network â†’ Same bias"]
    end
    
    subgraph DDQN_T["DDQN Target âœ…"]
        DD1["Online Network<br/>selects action (argmax)"]
        DD2["Target Network<br/>evaluates action"]
        DD1 --> DD3["Different networks<br/>â†’ Cross-validation!"]
        DD2 --> DD3
    end
    
    style DQN_T fill:#ff922b,color:#fff
    style DDQN_T fill:#4CAF50,color:#fff
```

### DDQN Target Formula

**DQN target:**
$$y_t^{\text{DQN}} = R_{t+1} + \gamma Q\big(S_{t+1}, \arg\max_{a'} Q(S_{t+1}, a'; \theta^-); \theta^-\big)$$

**DDQN target:**
$$y_t^{\text{DDQN}} = R_{t+1} + \gamma Q\Big(S_{t+1}, \underbrace{\arg\max_{a'} Q(S_{t+1}, a'; \theta)}_{\text{online selects}}\;;\; \underbrace{\theta^-}_{\text{target evaluates}}\Big)$$

### DDQN Implementation

```python
def optimize_model(self, experiences):
    states, actions, rewards, next_states, is_terminals = experiences
    batch_size = len(is_terminals)
    
    # DDQN: online network selects action
    argmax_a_q_sp = self.online_model(next_states).max(1)[1]
    
    # Target network evaluates that action
    q_sp = self.target_model(next_states).detach()
    max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp]
    max_a_q_sp = max_a_q_sp.unsqueeze(1)
    max_a_q_sp *= (1 - is_terminals)
    
    # TD target
    target_q_sa = rewards + (self.gamma * max_a_q_sp)
    
    # Current estimates (gradients flow through here)
    q_sa = self.online_model(states).gather(1, actions)
    
    # Loss and optimization
    td_error = q_sa - target_q_sa
    value_loss = td_error.pow(2).mul(0.5).mean()
    self.value_optimizer.zero_grad()
    value_loss.backward()
    self.value_optimizer.step()
```

> âš ï¸ DQN target network á€‘á€²á€™á€¾á€¬ action selection á€”á€²á€· evaluation **same network** â†’ same bias direction. DDQN á€™á€¾á€¬ **online** selects, **target** evaluates â†’ cross-validation effect!

---

## 8. Huber Loss â€” More Forgiving Loss Function

### MSE vs MAE vs Huber

```mermaid
graph TD
    subgraph MSE["MSE (L2 Loss)"]
        M1["Large errors â†’ Heavy penalty"]
        M2["Gradients â†’ 0 near minimum âœ…"]
        M3["Sensitive to outliers âŒ"]
    end
    
    subgraph MAE["MAE (L1 Loss)"]
        A1["Linear penalty everywhere"]
        A2["Robust to outliers âœ…"]
        A3["Not differentiable at 0 âŒ"]
    end
    
    subgraph HUBER["Huber Loss âœ…"]
        H1["Quadratic near 0<br/>(like MSE)"]
        H2["Linear for large errors<br/>(like MAE)"]
        H3["Best of both worlds"]
    end
    
    style HUBER fill:#4CAF50,color:#fff
```

### Huber Loss Formula

$$L_\delta(\theta) = \begin{cases} \frac{1}{2}(Q(s,a;\theta) - y)^2 & \text{if } |Q(s,a;\theta) - y| \leq \delta \\ \delta \cdot |Q(s,a;\theta) - y| - \frac{1}{2}\delta^2 & \text{otherwise} \end{cases}$$

- $\delta = 0$: MAE á€–á€¼á€…á€º
- $\delta \to \infty$: MSE á€–á€¼á€…á€º
- Typical $\delta = 1$

### Gradient Clipping Implementation

```python
def optimize_model(self, experiences):
    # ... calculate targets using double learning ...
    td_error = q_sa - target_q_sa
    value_loss = td_error.pow(2).mul(0.5).mean()  # MSE
    
    self.value_optimizer.zero_grad()
    value_loss.backward()
    
    # Gradient clipping (Huber loss equivalent)
    torch.nn.utils.clip_grad_norm_(
        self.online_model.parameters(),
        self.max_gradient_norm)  # float('inf') â†’ effectively MSE
    
    self.value_optimizer.step()
```

| Loss Function | Small Errors | Large Errors | RL Suitability |
|---|---|---|---|
| **MSE (L2)** | Quadratic | Quadratic (heavy) | âš ï¸ Penalizes early mistakes harshly |
| **MAE (L1)** | Linear | Linear | âš ï¸ Not differentiable at 0 |
| **Huber** | Quadratic | Linear | âœ… **Best for RL** |

> ğŸ’¡ RL á€™á€¾á€¬ targets á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º inaccurate á€–á€¼á€…á€ºá€á€¬á€€á€¼á€±á€¬á€„á€·á€º large errors á€€á€­á€¯ MSE á€œá€­á€¯ heavily penalize á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€€ á€¡á€“á€­á€•á€¹á€•á€¬á€šá€ºá€™á€›á€¾á€­á€•á€«á‹ Huber loss á€€ **outlier-robust** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 9. DDQN Full Algorithm

### DDQN Hyperparameters

| Parameter | DQN | DDQN |
|---|---|---|
| Architecture | 4, 512, 128, 2 | 4, 512, 128, 2 |
| Learning rate | 0.0005 | **0.0007** |
| Exploration | Exp-decay Îµ-greedy | Exp-decay Îµ-greedy |
| Buffer | 320 min, 50k max, batch 64 | 320 min, 50k max, batch 64 |
| Target update | Every 15 steps | Every 15 steps |
| Loss | MSE | **Huber (grad clip)** |
| Double learning | âŒ | âœ… |

### DDQN vs DQN Results

| Metric | DQN | DDQN |
|---|---|---|
| **Performance** | Similar episodes | Similar episodes |
| **Stability** | Wider bounds | **Narrower bounds** |
| **Consistency across seeds** | Variable | **More consistent** |
| **Learning rate tolerance** | lr=0.0005 only | lr=0.0007 works |

> ğŸ’¡ Cart-Pole environment á€á€½á€„á€º mean performance á€€á€á€°á€”á€®á€¸á€•á€«á€¸á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º DDQN á€€ **all seeds** á€™á€¾á€¬ consistent á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Complex environments (Atari) á€™á€¾á€¬ DDQN significantly á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

---

## 10. Things We Can Still Improve (Preview of Ch 10)

```mermaid
graph TD
    DDQN["DDQN<br/>(Current Best)"] --> IMP1["Dueling Architecture<br/>V(s) + A(s,a) = Q(s,a)"]
    DDQN --> IMP2["Prioritized Experience Replay<br/>Important experiences first"]
    
    IMP1 --> CH10["Chapter 10:<br/>Dueling DDQN + PER"]
    IMP2 --> CH10
    
    style DDQN fill:#2196F3,color:#fff
    style CH10 fill:#4CAF50,color:#fff
```

1. **Dueling Architecture**: Q-function á€€á€­á€¯ V(s) á€”á€²á€· A(s,a) á€á€½á€²á€•á€¼á€®á€¸ learn â†’ data efficiency á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸
2. **Prioritized Experience Replay (PER)**: Uniform random sampling á€¡á€…á€¬á€¸ TD error á€€á€¼á€®á€¸á€á€²á€· experiences á€€á€­á€¯ priority á€•á€±á€¸á€•á€¼á€®á€¸ sample

---

## 11. Key Equations Summary

| Equation | Formula |
|---|---|
| **DQN target** | $y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta^-)$ |
| **DDQN target** | $y_t = R_{t+1} + \gamma Q(S_{t+1}, \arg\max_{a'} Q(S_{t+1}, a'; \theta); \theta^-)$ |
| **MSE Loss** | $\mathcal{L} = \frac{1}{N}\sum_i (Q(s_i,a_i;\theta) - y_i)^2$ |
| **Huber Loss** | Quadratic if $|\text{error}| \leq \delta$, Linear otherwise |
| **Exp Îµ-decay** | $\epsilon_t = \max(\epsilon_{\min}, \lambda \cdot \epsilon_{t-1})$ |
| **Softmax** | $P(a_i|s) = \frac{e^{Q(s,a_i)/\tau}}{\sum_j e^{Q(s,a_j)/\tau}}$ |
| **Target update** | $\theta^- \leftarrow \theta$ (every $N$ steps) |

---

## 12. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **Target networks** â€” targets á€€á€­á€¯ temporarily freeze á€–á€¼á€„á€·á€º stability á€›á€›á€¾á€­
2. **Experience replay** â€” online data á€€á€­á€¯ IID á€•á€¯á€¶á€…á€¶ á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ diverse mini-batch á€–á€¼á€„á€·á€º train
3. **Larger networks** â€” state aliasing reduce, subtle differences detect
4. **DQN** â€” target network + replay buffer á€–á€¼á€„á€·á€º NFQ á€‘á€€á€º 10Ã— sample efficient
5. **Double DQN** â€” online network (select) + target network (evaluate) á€–á€¼á€„á€·á€º overestimation bias á€–á€¼á€±á€›á€¾á€„á€ºá€¸
6. **Huber loss** â€” MSE+MAE hybrid, RL á€›á€²á€· early-stage large errors á€€á€­á€¯ robust
7. **Exploration strategies** â€” constant, linear decay, exponential decay, softmax options

```mermaid
graph TD
    subgraph EVOLUTION["Value-Based DRL Evolution"]
        NFQ2["NFQ<br/>Batch + Fitting"] --> DQN2["DQN<br/>+ Target Net<br/>+ Replay Buffer"]
        DQN2 --> DDQN2["DDQN<br/>+ Double Learning<br/>+ Huber Loss"]
        DDQN2 --> NEXT["Ch 10: Dueling DDQN<br/>+ PER"]
    end
    
    style NFQ2 fill:#ff922b,color:#fff
    style DQN2 fill:#2196F3,color:#fff
    style DDQN2 fill:#4CAF50,color:#fff
    style NEXT fill:#9C27B0,color:#fff
```

> ğŸ’¡ DQN/DDQN á€Ÿá€¬ Atari benchmarks á€™á€¾á€¬ **superhuman performance** á€›á€›á€¾á€­á€á€²á€·á€•á€«á€á€šá€ºá‹ á€’á€® chapter á€›á€²á€· techniques á€á€½á€±á€á€Šá€º modern value-based DRL á€›á€²á€· foundation á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
