# Chapter 8: Introduction to Value-Based Deep Reinforcement Learning - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Chapter á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€º

á€’á€® Chapter á€™á€¾á€¬ **tabular RL** á€€á€”á€± **deep RL** á€†á€® á€€á€°á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€«á€á€šá€ºá‹ Continuous/high-dimensional state spaces á€›á€¾á€­á€á€²á€· problems á€€á€­á€¯ neural networks (function approximation) á€–á€¼á€„á€·á€º á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º **Neural Fitted Q-iteration (NFQ)** algorithm á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph TAB["ğŸ“— Tabular RL (Chapters 4-7)"]
        T1["Feedback: Sequential + Evaluative"]
        T2["State spaces: Discrete, Small"]
        T3["Value functions: Tables/Matrices"]
    end
    
    subgraph DEEP["ğŸ“˜ Deep RL (Chapters 8+)"]
        D1["Feedback: Sequential + Evaluative + Sampled"]
        D2["State spaces: Continuous, High-dimensional"]
        D3["Value functions: Neural Networks"]
    end
    
    TAB -->|"+ Sampled Feedback<br/>+ Function Approximation"| DEEP
    
    style TAB fill:#2196F3,color:#fff
    style DEEP fill:#4CAF50,color:#fff
```

á€¡á€“á€­á€€ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€™á€»á€¬á€¸:
1. **Sampled feedback** â€” exhaustive sampling á€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬á€·á€á€¼á€„á€ºá€¸
2. **Function approximation** â€” tables á€¡á€…á€¬á€¸ neural networks
3. **NFQ algorithm** â€” first value-based DRL method
4. **Decision points** â€” architecture, targets, loss, optimizer choices
5. **IID assumption** á€”á€¾á€„á€·á€º **non-stationary targets** problems

---

## 2. Three Types of Feedback in DRL

### Sequential + Evaluative + Sampled

```mermaid
graph TD
    subgraph SEQ["Sequential Feedback"]
        S1["Actions á€›á€²á€· consequences á€€<br/>future states á€€á€­á€¯ affect"]
        S2["Immediate vs long-term<br/>trade-off"]
    end
    
    subgraph EVAL["Evaluative Feedback"]
        E1["Reward signals á€€á€á€¬ feedback"]
        E2["Best possible reward<br/>á€€á€­á€¯ á€™á€á€­"]
    end
    
    subgraph SAMP["Sampled Feedback âœ¨ NEW"]
        SA1["All states á€€á€­á€¯ visit<br/>á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º"]
        SA2["Generalization<br/>á€œá€­á€¯á€¡á€•á€º"]
    end
    
    SEQ --> RL["Deep RL"]
    EVAL --> RL
    SAMP --> RL
    
    style SAMP fill:#ef5350,color:#fff
    style RL fill:#4CAF50,color:#fff
```

| Feedback Type | Tabular RL | Deep RL |
|---|---|---|
| **Sequential** | âœ… | âœ… |
| **Evaluative** | âœ… | âœ… |
| **Exhaustive** | âœ… (all states visitable) | âŒ |
| **Sampled** | âŒ | âœ… (cannot visit all states) |

### Why Exhaustive Sampling is Impossible

- Atari game states: $(255^3)^{210 \times 160}$ = 242,580-digit number
- Go board: $10^{170}$ possible states
- Robotic arms: Continuous joint angles (infinitesimal precision)

> ğŸ’¡ Observable universe á€›á€²á€· atoms á€¡á€›á€±á€¡á€á€½á€€á€ºá€€ $10^{78}$ to $10^{82}$ (83-digit number at most) á€–á€¼á€…á€ºá€•á€¼á€®á€¸ Atari state space á€€ á€’á€®á€‘á€€á€º **á€¡á€†á€•á€±á€«á€„á€ºá€¸á€™á€»á€¬á€¸á€…á€½á€¬** á€€á€¼á€®á€¸á€•á€«á€á€šá€ºá‹

---

## 3. Function Approximation á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€œá€­á€¯á€á€¬á€œá€²

### High-dimensional vs Continuous State Spaces

```mermaid
graph TD
    subgraph HD["High-dimensional"]
        HD1["State variables á€¡á€›á€±á€¡á€á€½á€€á€º á€™á€»á€¬á€¸"]
        HD2["Example: Atari image<br/>210Ã—160Ã—3 = 100,800 pixels"]
    end
    
    subgraph CS["Continuous"]
        CS1["Variable á€á€…á€ºá€á€¯á€€<br/>infinite values á€šá€°á€”á€­á€¯á€„á€º"]
        CS2["Example: Robot joint angle<br/>1.56, 1.5683, 1.56832..."]
    end
    
    HD --> NEED["Function Approximation<br/>Required!"]
    CS --> NEED
    
    style NEED fill:#ef5350,color:#fff
```

### Benefits of Function Approximation

1. **Solve otherwise unsolvable problems** â€” continuous state spaces á€›á€¾á€­á€á€²á€· problems
2. **Generalization** â€” visited á€–á€°á€¸á€á€²á€· states á€€á€”á€± unvisited states á€€á€­á€¯ generalize
3. **Efficiency** â€” underlying relationships á€€á€­á€¯ discover á€•á€¼á€®á€¸ fewer samples á€–á€¼á€„á€·á€º learn

$$\text{Single update to } V(s) \xrightarrow{\text{with FA}} \text{Updates to similar states } V(s'), V(s''), ...$$

> ğŸ’¡ Function approximation á€™á€›á€¾á€­á€˜á€² state 2.35 á€›á€²á€· value á€€á€­á€¯ á€á€­á€–á€­á€¯á€· exactly state 2.35 á€€á€­á€¯ visit á€›á€™á€šá€ºá‹ Function approximation á€›á€¾á€­á€›á€„á€º state 2.3 á€”á€²á€· 2.4 á€€á€”á€± **generalize** á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

## 4. Cart-Pole Environment

### Environment Description

Cart-Pole á€†á€­á€¯á€á€¬ classic RL environment á€–á€¼á€…á€ºá€•á€¼á€®á€¸ low-dimensional but continuous state space á€›á€¾á€­á€•á€«á€á€šá€ºá‹

| Property | Value |
|---|---|
| **State variables** | 4 (cart position, cart velocity, pole angle, pole tip velocity) |
| **Actions** | 2 (push left, push right) |
| **Reward** | +1 per time step |
| **Terminal conditions** | Pole angle > 12Â°, Cart > 2.4 units from center, 500 steps |

```mermaid
graph LR
    STATE["State s<br/>[position, velocity,<br/>angle, tip_velocity]"] -->|"Neural Network<br/>Q(s; Î¸)"| VALUES["Q-values<br/>[Q(s, left), Q(s, right)]"]
    VALUES -->|"argmax"| ACTION["Action<br/>push left or right"]
    
    style STATE fill:#ff922b,color:#fff
    style VALUES fill:#2196F3,color:#fff
    style ACTION fill:#4CAF50,color:#fff
```

---

## 5. NFQ Algorithm â€” Decision Points

### NFQ á€›á€²á€· Decision Points 7 á€á€¯

```mermaid
graph TD
    D1["1. Value Function<br/>â†’ Q(s,a; Î¸)"] --> D2["2. Architecture<br/>â†’ State-in, Values-out"]
    D2 --> D3["3. Objective<br/>â†’ Approximate q*(s,a)"]
    D3 --> D4["4. Targets<br/>â†’ Off-policy TD target"]
    D4 --> D5["5. Exploration<br/>â†’ Îµ-greedy (Îµ=0.5)"]
    D5 --> D6["6. Loss Function<br/>â†’ MSE (L2 loss)"]
    D6 --> D7["7. Optimizer<br/>â†’ RMSprop"]
    
    style D1 fill:#ff922b,color:#fff
    style D2 fill:#2196F3,color:#fff
    style D3 fill:#4CAF50,color:#fff
    style D4 fill:#9C27B0,color:#fff
    style D5 fill:#ef5350,color:#fff
    style D6 fill:#ffd43b,color:#000
    style D7 fill:#64B5F6,color:#fff
```

### Decision 1: Value Function to Approximate

$$Q(s, a; \theta) \approx q^*(s, a)$$

- $\theta$ â€” neural network weights
- Q-function á€€á€­á€¯ approximate (V-function á€™á€Ÿá€¯á€á€º) â€” MDP á€™á€œá€­á€¯á€˜á€² policy improvement á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º

### Decision 2: Neural Network Architecture

**State-in, Values-out** architecture (á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€²á€· choice):

```mermaid
graph LR
    INPUT["Input: State s<br/>[4 variables]"] --> HL1["Hidden Layer 1<br/>512 units"]
    HL1 --> HL2["Hidden Layer 2<br/>128 units"]
    HL2 --> OUTPUT["Output: Q-values<br/>[Q(s,left), Q(s,right)]"]
    
    style INPUT fill:#ff922b,color:#fff
    style HL1 fill:#64B5F6,color:#fff
    style HL2 fill:#64B5F6,color:#fff
    style OUTPUT fill:#4CAF50,color:#fff
```

### Decision 3: Objective â€” Ideal vs Practical

**Ideal objective (impossible):**

$$\mathcal{L}(\theta) = \mathbb{E}\Big[ \big( q^*(s, a) - Q(s, a; \theta) \big)^2 \Big]$$

**Practical (GPI approach):** Start with random Q, evaluate with TD targets, improve with Îµ-greedy, iterate.

### Decision 4: TD Target (Off-policy)

$$y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$$

```python
# Q-learning target implementation
q_sp = self.online_model(next_states).detach()  # MUST detach!
max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)
max_a_q_sp *= (1 - is_terminals)  # terminal states â†’ 0
target_q_s = rewards + self.gamma * max_a_q_sp
```

> âš ï¸ **Critical**: `detach()` á€€á€­á€¯ target calculation á€™á€¾á€¬ **á€™á€–á€¼á€…á€ºá€™á€”á€±** á€á€¯á€¶á€¸á€›á€•á€«á€™á€šá€ºá‹ Gradient á€€á€­á€¯ target á€€á€”á€± backpropagate á€œá€¯á€•á€ºá€á€½á€„á€·á€ºá€™á€•á€¼á€¯á€›á€•á€«á‹

### Decision 5: Exploration Strategy

Îµ-greedy with Îµ = 0.5 (50% random, 50% greedy)

### Decision 6: Loss Function â€” MSE (L2)

$$\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \Big( Q(s_i, a_i; \theta) - y_i \Big)^2$$

### Decision 7: Optimizer â€” RMSprop

- Learning rate: 0.0005
- RMSprop: gradient magnitude á€›á€²á€· moving average á€–á€¼á€„á€·á€º scale
- Adam á€‘á€€á€º stable, hyperparameters sensitivity á€”á€Šá€ºá€¸

---

## 6. Optimization Methods Overview

```mermaid
graph TD
    BGD["Batch Gradient Descent<br/>(entire dataset at once)"] -->|"too slow"| MBGD["Mini-batch GD<br/>(subset of data)"]
    MBGD -->|"extreme case"| SGD["Stochastic GD<br/>(single sample)"]
    
    MBGD -->|"+ moving avg<br/>of gradients"| MOM["Momentum"]
    MBGD -->|"+ avg magnitude<br/>of gradients"| RMS["RMSprop"]
    MOM -->|"combine"| ADAM["Adam<br/>(Momentum + RMSprop)"]
    RMS -->|"combine"| ADAM
    
    style RMS fill:#4CAF50,color:#fff
    style ADAM fill:#2196F3,color:#fff
```

| Optimizer | Description | DRL Suitability |
|---|---|---|
| **Batch GD** | Full dataset at once | âŒ Impractical (no dataset in advance) |
| **SGD** | Single sample per step | âš ï¸ High variance |
| **Mini-batch GD** | Small batch per step | âœ… Common |
| **Momentum** | Moving avg of gradients | âœ… Fast but aggressive |
| **RMSprop** | Scale by gradient magnitude | âœ… **Preferred** for value-based |
| **Adam** | Momentum + RMSprop | âœ… Good but more aggressive |

---

## 7. NFQ Full Algorithm

### NFQ Three Steps (Nested Loop)

```mermaid
graph LR
    S1["1. Collect E=1024<br/>experience samples"] --> S2["2. Calculate TD targets<br/>r + Î³ max Q(s',a';Î¸)"]
    S2 --> S3["3. Fit Q(s,a;Î¸)<br/>MSE + RMSprop"]
    S3 -->|"Repeat K=40<br/>times"| S2
    S3 -->|"Then collect<br/>new samples"| S1
    
    style S1 fill:#ff922b,color:#fff
    style S2 fill:#2196F3,color:#fff
    style S3 fill:#4CAF50,color:#fff
```

### FCQ Network Implementation

```python
class FCQ(nn.Module):
    def __init__(self, input_dim, output_dim,
                 hidden_dims=(32,32), activation_fc=F.relu):
        super().__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims)-1):
            self.hidden_layers.append(
                nn.Linear(hidden_dims[i], hidden_dims[i+1]))
        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
    
    def forward(self, state):
        x = self.activation_fc(self.input_layer(x))
        for hidden_layer in self.hidden_layers:
            x = self.activation_fc(hidden_layer(x))
        return self.output_layer(x)  # no activation on output!
```

> ğŸ’¡ Output layer á€™á€¾á€¬ activation function **á€™á€á€¯á€¶á€¸á€•á€«** â€” Q-values á€á€½á€±á€€ positive/negative á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€¬á€™á€­á€¯á€· unbounded output á€œá€­á€¯á€•á€«á€á€šá€ºá‹

---

## 8. Terminal State Handling

### Time Limit Trap

CartPole-v1 á€™á€¾á€¬ 500 steps limit á€›á€¾á€­á€•á€«á€á€šá€ºá‹ Pole straight up á€–á€¼á€…á€ºá€”á€±á€á€²á€· state 500 á€™á€¾á€¬ timeout á€–á€¼á€…á€ºá€›á€„á€º terminal flag á€›á€™á€šá€º â€” á€’á€«á€•á€±á€™á€šá€·á€º á€’á€® state á€›á€²á€· value á€€á€á€€á€šá€ºá€á€™á€ºá€¸ **infinite** á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Zero á€•á€±á€«á€ºá€™á€¾á€¬ bootstrap á€›á€„á€º á€™á€¾á€¬á€¸á€•á€«á€™á€šá€ºá‹

```python
# Proper terminal state handling
new_state, reward, is_terminal, info = env.step(action)
is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']
is_failure = is_terminal and not is_truncated
experience = (state, action, reward, new_state, float(is_failure))
```

> âš ï¸ `is_failure` á€€á€­á€¯á€á€¬ terminal flag á€¡á€–á€¼á€…á€ºá€á€¯á€¶á€¸á€•á€«á‹ Time limit á€€á€¼á€±á€¬á€„á€·á€º terminate á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸á€€á€­á€¯ real failure á€Ÿá€¯ á€™á€™á€¾á€á€ºá€šá€°á€•á€«á€”á€¾á€„á€·á€ºá‹

---

## 9. Things That Go Wrong â€” IID á€”á€¾á€„á€·á€º Non-stationary Targets

### Problem 1: Data is NOT IID

```mermaid
graph TD
    subgraph SL["Supervised Learning"]
        SL1["Dataset: shuffled, fixed"]
        SL2["Samples: independent"]
        SL3["Distribution: fixed"]
    end
    
    subgraph RL["Reinforcement Learning"]
        RL1["Data: collected online, sequential"]
        RL2["Samples: correlated (s_t â†’ s_{t+1})"]
        RL3["Distribution: changes as Ï€ improves"]
    end
    
    SL -->|"IID âœ…"| OK["Optimization works well"]
    RL -->|"NOT IID âŒ"| BAD["Optimization can diverge"]
    
    style SL fill:#4CAF50,color:#fff
    style RL fill:#ef5350,color:#fff
```

### Problem 2: Non-stationary Targets

$$\text{target} = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$$

Target á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º network $\theta$ á€•á€±á€«á€ºá€™á€¾á€¬ depend á€•á€«á€á€šá€ºá‹ $\theta$ update á€›á€„á€º target á€•á€«á€•á€¼á€±á€¬á€„á€ºá€¸á€á€½á€¬á€¸á€•á€«á€á€šá€º:

$$\theta \text{ changes} \Rightarrow Q(S_{t+1}, a'; \theta) \text{ changes} \Rightarrow \text{target changes} \Rightarrow \text{chasing moving target!}$$

### Circular Dependency

```mermaid
graph LR
    PI["Policy Ï€"] -->|"produces"| DATA["Data (experiences)"]
    DATA -->|"used to calculate"| TARGET["Targets"]
    TARGET -->|"used to train"| Q["Q-function Q(s,a;Î¸)"]
    Q -->|"produces"| PI
    
    style PI fill:#ff922b,color:#fff
    style Q fill:#2196F3,color:#fff
```

> ğŸ’¡ á€’á€® two problems á€€á€­á€¯ Chapter 9 á€™á€¾á€¬ target networks á€”á€²á€· experience replay á€–á€¼á€„á€·á€º á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€™á€šá€ºá‹

---

## 10. Key Equations Summary

| Equation | Formula |
|---|---|
| **Q-function approximation** | $Q(s, a; \theta) \approx q^*(s, a)$ |
| **TD target (off-policy)** | $y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$ |
| **MSE Loss** | $\mathcal{L}(\theta) = \frac{1}{N}\sum_i (Q(s_i,a_i;\theta) - y_i)^2$ |
| **Gradient update** | $\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)$ |
| **Atari state space** | $(255^3)^{210 \times 160}$ (242,580-digit number) |

---

## 11. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Conclusion)

á€’á€® Chapter á€™á€¾á€¬ á€á€„á€ºá€šá€°á€á€²á€·á€á€²á€· á€¡á€“á€­á€€ takeaways:

1. **Sampled feedback** â€” deep RL á€›á€²á€· third dimension, exhaustive sampling á€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º
2. **High-dimensional vs Continuous** â€” state space complexity á€›á€²á€· two axis
3. **Function approximation** â€” generalization á€–á€¼á€„á€·á€º unseen states á€€á€­á€¯á€œá€Šá€ºá€¸ handle
4. **NFQ algorithm** â€” first value-based DRL method, batch + fitting approach
5. **7 Decision points** â€” value function, architecture, objective, targets, exploration, loss, optimizer
6. **IID violation** â€” online data is correlated and non-identically distributed
7. **Non-stationary targets** â€” targets change as network updates
8. **Terminal state handling** â€” time limits vs real failures á€á€½á€²á€á€¼á€¬á€¸á€›á€”á€º á€¡á€›á€±á€¸á€€á€¼á€®á€¸

```mermaid
graph TD
    NFQ["NFQ Algorithm"] --> COMP["Components"]
    COMP --> VF["Q(s,a;Î¸)<br/>Value Function"]
    COMP --> ARCH["State-in, Values-out<br/>Architecture"]
    COMP --> TGT["Off-policy TD target"]
    COMP --> EXP["Îµ-greedy<br/>Exploration"]
    COMP --> LOSS["MSE Loss"]
    COMP --> OPT["RMSprop<br/>Optimizer"]
    
    NFQ --> ISSUES["âš ï¸ Known Issues"]
    ISSUES --> IID["Data not IID"]
    ISSUES --> NST["Non-stationary targets"]
    
    ISSUES -->|"Solved in Ch 9"| CH9["DQN:<br/>Target Networks +<br/>Experience Replay"]
    
    style NFQ fill:#ffd43b,color:#000
    style ISSUES fill:#ef5350,color:#fff
    style CH9 fill:#4CAF50,color:#fff
```

> ğŸ’¡ NFQ á€á€Šá€º deep RL á€›á€²á€· foundation á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º sample efficiency á€”á€Šá€ºá€¸á€•á€¼á€®á€¸ stability issues á€›á€¾á€­á€•á€«á€á€šá€ºá‹ Chapter 9 á€™á€¾á€¬ DQN á€–á€¼á€„á€·á€º á€’á€® issues á€á€½á€±á€€á€­á€¯ address á€œá€¯á€•á€ºá€•á€«á€™á€šá€ºá‹
