# á€¡á€á€”á€ºá€¸ áˆ â€” Value-based Deep Reinforcement Learning á€”á€­á€’á€«á€”á€ºá€¸ (Introduction to Value-Based Deep Reinforcement Learning)

> *"á€œá€°á€á€¬á€¸á á€¡á€•á€¼á€¯á€¡á€™á€°á€á€Šá€º á€¡á€“á€­á€€ source áƒ á€á€¯á€™á€¾ á€…á€®á€¸á€†á€„á€ºá€¸á€á€Šá€º â€” desire (á€†á€”á€¹á€’)áŠ emotion (á€…á€­á€á€ºá€á€¶á€…á€¬á€¸á€™á€¾á€¯)áŠ á€”á€¾á€„á€·á€º knowledge (á€—á€Ÿá€¯á€á€¯á€)á‹"*
> â€” Plato (Classical Greece á philosopheráŠ Academy á á€á€Šá€ºá€‘á€±á€¬á€„á€ºá€á€°)

## á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ á€á€„á€ºá€šá€°á€›á€™á€Šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

- Non-linear function approximators á€–á€¼á€„á€·á€º RL agents train á€›á€¬á€á€½á€„á€º á€›á€¾á€­á€”á€±á€á€±á€¬ challenges á€™á€»á€¬á€¸á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€›á€™á€Šá€º
- Minimal hyperparameter adjustments á€–á€¼á€„á€·á€º different kinds of problems á€€á€­á€¯ solve á€”á€­á€¯á€„á€ºá€á€²á€· deep RL agent á€á€…á€ºá€á€¯á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€›á€™á€Šá€º
- Value-based methods á€€á€­á€¯ RL problems á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€¬á€á€½á€„á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸á advantages á€”á€¾á€„á€·á€º disadvantages á€€á€­á€¯ identify á€œá€¯á€•á€ºá€›á€™á€Šá€º

---

## áˆ.á â€” TP á€™á€¾ DRL á€†á€®: á€€á€¼á€®á€¸á€™á€¬á€¸á€á€±á€¬ á€á€¼á€±á€œá€¾á€™á€ºá€¸ (From Tabular RL to Deep RL)

á€’á€®á€¡á€á€»á€­á€”á€ºá€‘á€­ á€™á€¼á€„á€ºá€á€¬á€‘á€„á€ºá€á€¬ á€á€­á€¯á€¸á€á€€á€ºá€™á€¾á€¯á€™á€»á€¬á€¸ á€–á€¼á€…á€ºá€á€²á€·á€•á€«á€á€šá€º:

- **Chapter 2** â€” MDP á€–á€¼á€„á€·á€º problems represent á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- **Chapter 3** â€” VI/PI á€–á€¼á€„á€·á€º MDPs solve á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- **Chapter 4** â€” MDP á€™á€á€­á€˜á€² one-step MDPs (bandits) á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸
- **Chapter 5** â€” Sequential + uncertain feedback á€€á€­á€¯ combine á€œá€¯á€•á€ºá€•á€¼á€®á€¸ policies evaluate á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
- **Chapter 6** â€” Sequential decision-making under uncertainty á€€á€­á€¯ optimal policies á€–á€¼á€„á€·á€º á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€¼á€„á€ºá€¸
- **Chapter 7** â€” á€•á€­á€¯á€‘á€­á€›á€±á€¬á€€á€ºá€•á€¼á€®á€¸ á€•á€­á€¯á€¡á€€á€»á€­á€¯á€¸á€›á€¾á€­ agents develop á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

Chapter 2 á€á€Šá€º á€’á€®á€…á€¬á€¡á€¯á€•á€ºá€›á€¾á€­ chapters á€¡á€¬á€¸á€œá€¯á€¶á€¸á foundation á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Chapter 3 = sequential feedback, Chapter 4 = evaluative feedback, Chapters 5-7 = sequential + evaluative feedback (tabular RL)ã€‚

**á€’á€®á€¡á€á€”á€ºá€¸á€€á€”á€±á€…á€•á€¼á€®á€¸ deep RL á details á€€á€­á€¯ explore á€œá€¯á€•á€ºá€•á€«á€™á€šá€º!** Neural networks (highly non-linear function approximators) á€€á€­á€¯ RL problems á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€¬á€á€½á€„á€º leverage á€œá€¯á€•á€ºá€•á€¯á€¶á€€á€­á€¯ depth á€–á€¼á€„á€·á€º á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€™á€šá€ºá‹ Deep RL methods á€™á€»á€­á€¯á€¸á€…á€¯á€¶ á€›á€¾á€­á€á€±á€¬á€ºá€œá€Šá€ºá€¸ á€’á€® chapter á€á€Šá€º **value-based deep RL methods** á€€á€­á€¯ á€¡á€¬á€›á€¯á€¶á€…á€­á€¯á€€á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph TD
    subgraph BOOK["á€…á€¬á€¡á€¯á€•á€ºá€›á€²á€· Algorithmic Approaches"]
        VB["Value-based<br/>ğŸ“ Ch 8-10"]
        PB["Policy-based"]
        AC["Actor-critic"]
        MB["Model-based"]
        GF["Derivative-free"]
    end
    
    style VB fill:#ef5350,color:#fff
    style PB fill:#64B5F6,color:#fff
    style AC fill:#4CAF50,color:#fff
    style MB fill:#9C27B0,color:#fff
    style GF fill:#ff922b,color:#fff
```

```mermaid
graph TD
    subgraph TAB["ğŸ“— Tabular RL (Chapters 4-7)"]
        T1["Feedback: Sequential + Evaluative"]
        T2["State spaces: Discrete, Small"]
        T3["Value functions: Tables / Matrices"]
    end
    
    subgraph DEEP["ğŸ“˜ Deep RL (Chapters 8+)"]
        D1["Feedback: Sequential + Evaluative + Sampled"]
        D2["State spaces: Continuous, High-dimensional"]
        D3["Value functions: Neural Networks"]
    end
    
    TAB -->|"+ Sampled Feedback<br/>+ Function Approximation"| DEEP
    
    style TAB fill:#2196F3,color:#fff
    style DEEP fill:#4CAF50,color:#fff
```

---

## áˆ.á‚ â€” Deep RL Agents á Feedback á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸

Deep RL á€™á€¾á€¬ agents á€á€½á€±á€€ **sequential, evaluative, á€”á€¾á€„á€·á€º sampled** feedback áƒ á€á€¯á€€á€­á€¯ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º deal á€›á€•á€«á€á€šá€ºá‹ á€’á€® point á€€á€­á€¯ chapter á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸ emphasis á€•á€±á€¸á€”á€±á€á€¬ â€” á€’á€«á á€¡á€“á€­á€•á€¹á€•á€«á€šá€ºá€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€ºá‹

**Feedback á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸á€™á€»á€¬á€¸ á€”á€¾á€„á€·á€º method comparison:**

| Method | Sequential | Evaluative | Sampled |
|---|---|---|---|
| **Supervised Learning** | âŒ | âŒ | âœ… |
| **Planning (Ch 3)** | âœ… | âŒ | âŒ |
| **Bandits (Ch 4)** | âŒ | âœ… | âŒ |
| **Tabular RL (Ch 5-7)** | âœ… | âœ… | âŒ |
| **Deep RL (Ch 8-12)** | âœ… | âœ… | âœ… |

---

### áˆ.á‚.á â€” Sequential Feedback

Deep RL agents á€á€½á€± sequential feedback á€€á€­á€¯ deal á€›á€•á€«á€á€šá€ºá‹ Sequential feedback á á€¡á€“á€­á€€ challenge á€€á€á€±á€¬á€· agents á€á€½á€± delayed information á€€á€­á€¯ receive á€á€á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Chess game á€á€½á€„á€º á€…á€€á€¼á€±á€¬á€„á€º early moves á€™á€¾á€¬á€¸á€•á€±á€™á€šá€·á€º consequences á€€ game ending á€™á€¾á€¬á€™á€¾ á€•á€±á€«á€ºá€•á€¼á€„á€ºá€á€á€ºá€•á€«á€á€šá€ºá‹

Delayed feedback á€€á€¼á€±á€¬á€„á€·á€º feedback á source á€€á€­á€¯ interpret á€› á€á€€á€ºá€•á€«á€á€šá€ºá‹ Sequential feedback á€á€Šá€º **temporal credit assignment problem** á€€á€­á€¯ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€…á€±á€•á€«á€á€šá€º â€” temporal component á€›á€¾á€­á€•á€¼á€®á€¸ actions á consequences delayed á€–á€¼á€…á€ºá€›á€„á€º rewards á€¡á€á€½á€€á€º credit assign á€›á€”á€º á€á€€á€ºá€•á€«á€á€šá€ºá‹

**á€¥á€•á€™á€¬ â€” Sequential feedback á challenge:**

```
State: [â€“10 path] â† á€’á€® path á€€á€±á€¬á€„á€ºá€¸á€•á€¯á€¶á€›á€™á€Šá€º
           â†“
[more â€“10 states] â† á€’á€«á€•á€±á€™á€šá€·á€º á€†á€€á€ºá€á€½á€¬á€¸á€›á€„á€ºâ€¦
           â†“
  [â€“100 penalty!] â† á€’á€® high penalty á€€á€­á€¯á€™á€¾ á€á€½á€±á€·á€›á€™á€Šá€º
```

Agent á€€ "short-term á€€á€±á€¬á€„á€ºá€¸á€•á€¯á€¶á€›" á€á€²á€· path á€€á€­á€¯ á€›á€¾á€±á€¬á€„á€ºá€–á€­á€¯á€· value functions á€€á€­á€¯á€á€¯á€¶á€¸á€•á€¼á€®á€¸ decide á€›á€•á€«á€á€šá€º â€” rewards á€€á€­á€¯ direct á€€á€¼á€Šá€·á€ºá€•á€¼á€®á€¸ decide á€œá€¯á€•á€ºá€›á€„á€º á€™á€œá€¯á€¶á€•á€«á‹

**Sequential feedback á á€†á€”á€·á€ºá€€á€»á€„á€ºá€–á€€á€º = one-shot feedback:**
Classification problem á€€á€²á€·á€á€­á€¯á€· supervised learning á€™á€¾á€¬ image á€€á€­á€¯ á€™á€¾á€”á€º/á€™á€™á€¾á€”á€º predict á€•á€±á€™á€šá€·á€º next image presentation á€€á€­á€¯ affect á€™á€¯á€•á€« â€” long-term consequences á€™á€›á€¾á€­á€•á€«á‹

---

### áˆ.á‚.á‚ â€” Evaluative Feedback

Evaluative feedback á á€¡á€“á€­á€•á€¹á€•á€«á€šá€ºá€€á€á€±á€¬á€· feedback á goodness á€á€Šá€º **relative** á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸á€•á€²á€–á€¼á€…á€ºá€•á€«á€á€šá€ºáŠ á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬ environment á€á€Šá€º uncertain á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€ºá€•á€«á‹ Transition function á€”á€¾á€„á€·á€º reward signal á€€á€­á€¯ access á€™á€›á€•á€«á‹

Explore á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€–á€¼á€„á€·á€º current knowledge á€€á€­á€¯ capitalize á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€˜á€² regret accumulate á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” **exploration-exploitation trade-off** á€–á€¼á€…á€ºá€•á€±á€«á€ºá€œá€¬á€•á€«á€á€šá€ºá‹

**á€¥á€•á€™á€¬ â€” Evaluative feedback challenge:**

> Agent á€€ state á€á€…á€ºá€á€¯á€™á€¾ â€“10 reward á€›á€•á€«á€á€šá€ºá‹ á€’á€« bad á€œá€¬á€¸á€œá€Šá€ºá€¸ á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€› á€á€€á€ºá€•á€«á€á€šá€º! Environment á map á€€á€­á€¯ agent á€™á€™á€¼á€„á€ºá€›á€á€²á€·á€¡á€á€½á€€á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ â€“100 penalty state á€€á€œá€Šá€ºá€¸ á€›á€¾á€­á€€á€±á€¬á€„á€ºá€¸á€›á€¾á€­á€™á€šá€º...

**Evaluative feedback á á€†á€”á€·á€ºá€€á€»á€„á€ºá€–á€€á€º = supervised feedback:**
Classification problem á€™á€¾á€¬ model á€€á€­á€¯ correct labels (á€–á€¼á€±á€™á€¾á€”á€º) á€á€»á€€á€ºá€á€»á€„á€ºá€¸á€•á€±á€¸á€á€šá€º â€” "cheating!" á€†á€­á€¯á€á€œá€­á€¯á€•á€«á‹ Supervised learning agent á€€ mistakes á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€„á€º correct answer á€€á€­á€¯ á€á€»á€€á€ºá€á€»á€„á€ºá€¸á€•á€±á€¸á€á€šá€ºá‹ Real life á€™á€¾á€¬ "right answer" á€™á€›á€•á€«!

---

### áˆ.á‚.áƒ â€” Sampled Feedback (Deep RL á á€á€á€­á€š Dimension âœ¨)

Deep RL á€€á€­á€¯ tabular RL á€€á€”á€± á€á€½á€²á€á€¼á€¬á€¸á€•á€±á€¸á€á€²á€· á€¡á€“á€­á€€ feature á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Deep RL á€™á€¾á€¬ agents á€á€½á€± possible feedback 
 á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ exhaustively sample á€™á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á‹ Gathered feedback á€€á€­á€¯á€á€¯á€¶á€¸á€•á€¼á€®á€¸ generalize á€•á€¼á€¯á€œá€¯á€•á€ºá€€á€¬ á€‰á€¬á€á€ºá€€á€±á€¬á€„á€ºá€¸á€á€±á€¬ decisions á€á€»á€™á€Šá€º á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

**Atari state space á á€€á€¼á€®á€¸á€™á€¬á€¸á€™á€¾á€¯:**

- Image: $210 \times 160$ pixels Ã— 3 channels
- Each pixel: 0â€“255 (8-bit image)
- Possible states: $(255^3)^{210 \times 160} = (16{,}581{,}375)^{33{,}600}$

$$\text{Atari states} = (255^3)^{210 \times 160} \approx 10^{242{,}580\text{-digit number!}}$$

> ğŸ’¡ Observable universe á atoms á€¡á€›á€±á€¡á€á€½á€€á€ºá€€ $10^{78}$ á€™á€¾ $10^{82}$ (83-digit number at most) á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Atari state space á€€á€¬á€¸ **á€’á€®á€‘á€€á€º á€¡á€†á€•á€±á€«á€„á€ºá€¸á€™á€»á€¬á€¸á€…á€½á€¬** á€€á€¼á€®á€¸á€•á€«á€á€šá€º!

**Sampled feedback á á€†á€”á€·á€ºá€€á€»á€„á€ºá€–á€€á€º = exhaustive feedback:**

Tabular RL á€™á€¾á€¬ agents á€á€½á€± long enough sample á€œá€¯á€•á€ºá€›á€„á€º necessary information á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ gather á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ RL convergence guarantees á€á€½á€± á€–á€¼á€…á€ºá€•á€±á€«á€ºá€á€²á€· reason á€€á€œá€Šá€ºá€¸ exhaustive feedback collect á€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸ á€€á€¼á€±á€¬á€„á€·á€ºá€•á€²á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ á€’á€«á€•á€±á€™á€šá€·á€º **complex problems** (Go: $10^{170}$ states, Atari: $10^{242{,}580}$ states, Robotics: continuous state space) á€™á€¾á€¬ exhaustive sampling impossible á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

```mermaid
graph TD
    subgraph WHY["Exhaustive Sampling á€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€²á€· á€€á€­á€…á€¹á€…á€™á€»á€¬á€¸"]
        A["Atari: (255Â³)^{210Ã—160}<br/>242,580-digit number"]
        B["Go: 10^170 states"]
        C["Robotic arm:<br/>continuous joint angles"]
    end
    
    WHY --> NEED["Function Approximation<br/>á€œá€­á€¯á€¡á€•á€ºá€•á€¼á€®!"]
    
    style NEED fill:#ef5350,color:#fff
```

---

## áˆ.áƒ â€” Function Approximation á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€œá€­á€¯á€á€¬á€œá€²

### High-dimensional vs. Continuous State Spaces

> **High-dimensional:** Single state á€€á€­á€¯á€–á€½á€²á€·á€…á€Šá€ºá€¸á€á€²á€· variables á€¡á€›á€±á€¡á€á€½á€€á€º á€™á€»á€¬á€¸
> (Atari: $210 \times 160 \times 3 = 100{,}800$ pixels)

> **Continuous:** Variable á€á€…á€ºá€á€¯á€€ infinite number of values á€šá€°á€”á€­á€¯á€„á€º
> (Robot position: 1.56, 1.5683, 1.5683256, ...)

State space á€á€½á€± high-dimensional á€›á€±á€¬ continuous á€›á€±á€¬ á€–á€¼á€…á€ºá€”á€­á€¯á€„á€º â€” á€’á€«á€á€Šá€º deep RL á existence á€›á€²á€· reason á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

### Function Approximation á Advantages

1. **Solve otherwise unsolvable problems** â€” continuous state spaces á€›á€¾á€­á€á€²á€· problems
2. **Generalization** â€” visited states á€€á€”á€± unvisited states á€†á€® generalize
3. **Efficiency** â€” underlying relationships á€€á€­á€¯ discover á€•á€¼á€®á€¸ fewer samples á€–á€¼á€„á€·á€º learn

**á€¥á€•á€™á€¬ â€” Function Approximation á generalization:**

```
Without FA:  Update V(state=2.35) â†’ Only state 2.35 changes
With FA:     Update V(state=2.35) â†’ Similar states V(2.3), V(2.4)... all update!
```

$$\underbrace{V(s)}_{\text{Table}} \xrightarrow{\text{update}} \text{Only } V(s) \text{ changes}$$

$$\underbrace{Q(s, a; \theta)}_{\text{Neural Network}} \xrightarrow{\text{update}} \text{All similar } Q(s', a'; \theta) \text{ also change}$$

> ğŸ’¡ Value Iteration á€”á€¾á€„á€·á€º Q-learning á€á€­á€¯á€· tables/matrices á€–á€¼á€„á€·á€º value functions represent á€œá€¯á€•á€ºá€•á€«á€á€šá€º:
> - Value Iteration: State-value function = **vector** indexed by states
> - Q-learning: Action-value function = **matrix** indexed by states Ã— actions
>
> Cart-pole á€™á€¾á€¬ FA á€™á€á€¯á€¶á€¸á€˜á€² state 2.35 á value á€€á€­á€¯ á€á€­á€–á€­á€¯á€· exactly 2.35 á€€á€­á€¯ visit á€›á€™á€šá€ºá‹ FA á€›á€¾á€­á€›á€„á€º 2.3 á€”á€¾á€„á€·á€º 2.4 á€€á€”á€± **generalize** á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º!

---

## áˆ.á„ â€” Cart-Pole Environment

### Environment á á€–á€½á€²á€·á€…á€Šá€ºá€¸á€•á€¯á€¶

Cart-Pole á€á€Šá€º reinforcement learning á **classic environment** á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ Low-dimensional á€•á€±á€™á€šá€·á€º continuous state space á€›á€¾á€­á€•á€¼á€®á€¸ algorithms develop á€›á€¬á€á€½á€„á€º excellent environment á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

```
    [pole]
      |
   [cart] â†â†’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      track
```

| Property | Value |
|---|---|
| **State variables** | 4 á€á€¯ (cart position, cart velocity, pole angle, pole tip velocity) |
| **State dimension** | Low-dimensional á€•á€±á€™á€šá€·á€º **continuous** |
| **Actions** | 2 á€á€¯ â€” Action 0 (push left, â€“1 force), Action 1 (push right, +1 force) |
| **Reward** | Step á€á€­á€¯á€„á€ºá€¸ **+1** |
| **Terminal conditions** | Pole angle > 12Â°, Cart > 2.4 units center á€€á€”á€±, 500 steps á€•á€¼á€Šá€·á€º |

**Cart-Pole á State Variables:**

| Variable | Range |
|---|---|
| Cart position (x-axis) | â€“2.4 to 2.4 |
| Cart velocity (x-axis) | â€“âˆ to +âˆ |
| Pole angle | ~â€“40Â° to ~40Â° |
| Pole tip velocity | â€“âˆ to +âˆ |

```mermaid
graph LR
    STATE["Input State s<br/>[cart_pos, cart_vel,<br/>pole_angle, tip_vel]"] -->|"Neural Network<br/>Q(s; Î¸)"| VALUES["Output Q-values<br/>[Q(s, left), Q(s, right)]"]
    VALUES -->|"argmax"| ACTION["Action a<br/>push left/right"]
    
    style STATE fill:#ff922b,color:#fff
    style VALUES fill:#2196F3,color:#fff
    style ACTION fill:#4CAF50,color:#fff
```

---

## áˆ.á… â€” NFQ: Value-Based Deep RL á á€•á€‘á€™á€†á€¯á€¶á€¸ á€€á€¼á€­á€¯á€¸á€…á€¬á€¸á€™á€¾á€¯

### NFQ (Neural Fitted Q-Iteration) á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€²

**Neural Fitted Q (NFQ) iteration** á€€á€á€±á€¬á€· neural networks á€€á€­á€¯ function approximation á€¡á€–á€¼á€…á€º RL problems á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€›á€¬á€á€½á€„á€º successfully á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€±á€¬ á€•á€‘á€™á€†á€¯á€¶á€¸ algorithm á€‘á€²á€€á€á€…á€ºá€á€¯ á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

NFQ á€€á€­á€¯ 2005 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Martin Riedmiller** á€€ "Neural Fitted Q Iteration â€” First Experiences with a Data Efficient Neural Reinforcement Learning Method" paper á€á€½á€„á€º introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹ European universities á€¡á€™á€»á€¬á€¸á€¡á€•á€¼á€¬á€¸á€á€½á€„á€º professor á€¡á€–á€¼á€…á€º 13 á€”á€¾á€…á€º á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€•á€¼á€®á€¸ Martin á€á€Šá€º Google DeepMind á research scientist á€˜á€á€€á€­á€¯ á€†á€€á€ºá€œá€€á€ºá€œá€»á€¾á€±á€¬á€€á€ºá€•á€«á€á€šá€ºá‹

á€’á€® chapter á á€€á€»á€”á€ºá€á€²á€· section á€á€½á€±á€™á€¾á€¬ value-based deep RL algorithms á€¡á€™á€»á€¬á€¸á€…á€¯ á€•á€«á€á€„á€ºá€á€²á€· components á€™á€»á€¬á€¸á€€á€­á€¯ á€†á€½á€±á€¸á€”á€½á€±á€¸á€•á€«á€™á€šá€ºá‹ **Decision points 7 á€á€¯** á€€á€­á€¯ á€‘á€±á€¬á€€á€ºá€€á€¬ components á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€•á€¯á€¶á€€á€­á€¯ á€œá€±á€·á€œá€¬á€•á€«á€™á€šá€º:

```mermaid
graph TD
    D1["1ï¸âƒ£ Value Function<br/>â†’ Q(s,a; Î¸) est. q*(s,a)"] --> D2["2ï¸âƒ£ Architecture<br/>â†’ State-in, Values-out"]
    D2 --> D3["3ï¸âƒ£ Objective<br/>â†’ Approximate q*"]
    D3 --> D4["4ï¸âƒ£ Targets<br/>â†’ Off-policy TD target"]
    D4 --> D5["5ï¸âƒ£ Exploration<br/>â†’ Îµ-greedy (Îµ=0.5)"]
    D5 --> D6["6ï¸âƒ£ Loss Function<br/>â†’ MSE (L2 loss)"]
    D6 --> D7["7ï¸âƒ£ Optimizer<br/>â†’ RMSprop"]
    
    style D1 fill:#ff922b,color:#fff
    style D2 fill:#2196F3,color:#fff
    style D3 fill:#4CAF50,color:#fff
    style D4 fill:#9C27B0,color:#fff
    style D5 fill:#ef5350,color:#fff
    style D6 fill:#ffd43b,color:#000
    style D7 fill:#64B5F6,color:#fff
```

---

### Decision Point 1 â€” Approximate á€œá€¯á€•á€ºá€™á€Šá€·á€º Value Function á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

Neural networks á€–á€¼á€„á€·á€º value functions approximate á€œá€¯á€•á€ºá€›á€¬á€á€½á€„á€º á€™á€á€°á€Šá€®á€á€Šá€·á€º ways á€™á€»á€¬á€¸ á€›á€¾á€­á€•á€«á€á€šá€º:

- **$v(s)$** â€” State-value function (control problem á€€á€­á€¯ á€á€…á€ºá€á€¯á€á€Šá€ºá€¸á€–á€¼á€„á€·á€º solve á€™á€› â€” MDP á€•á€« á€œá€­á€¯á€á€šá€º)
- **$q(s,a)$** â€” Action-value function âœ… â€” MDP á€™á€œá€­á€¯á€˜á€² control problem solve á€”á€­á€¯á€„á€º
- **$a(s,a)$** â€” Action-advantage function (later chapters)

NFQ á€™á€¾á€¬ **action-value function $q(s,a)$** á€€á€­á€¯ Q-learning style á€”á€¾á€„á€·á€ºá€†á€„á€ºá€á€°á€…á€½á€¬ approximate á€œá€¯á€•á€ºá€•á€«á€á€šá€º:

$$Q(s, a; \theta) \approx q^*(s, a)$$

- $\theta$ = neural network á weights
- $Q(s,a;\theta)$ = approximate action-value function *estimates*
- $q^*(s,a)$ = true optimal action-value function (á€›á€¾á€¬á€–á€½á€±á€”á€±á€á€Šá€º)

> **á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º $Q$-function á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€á€œá€²?** Cart-pole á€™á€¾á€¬ pole balance á€–á€¼á€…á€ºá€›á€”á€º actions á values á€€á€­á€¯ state á€¡á€¬á€¸á€œá€¯á€¶á€¸á€™á€¾á€¬ á€á€­á€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€ºá‹ State-action pairs á values á€€á€­á€¯ á€á€­á€›á€„á€º exploratory action (information gather) á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º greedy action (expected return maximize) á€€á€­á€¯ á€€á€½á€²á€•á€¼á€¬á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

### Decision Point 2 â€” Neural Network Architecture á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

Architecture á‚ á€™á€»á€­á€¯á€¸ possible á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

**Option A: State-action-in, value-out (less efficient):**

```
Input: [cart_pos, cart_vel, pole_angle, tip_vel, action] â†’ Output: Q(s,a)
```

**Option B: State-in, values-out (more efficient â€” NFQ á€›á€½á€±á€¸á€á€»á€šá€º):**

```
Input: [cart_pos, cart_vel, pole_angle, tip_vel] â†’ Output: [Q(s,left), Q(s,right)]
```

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º state-in, values-out á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€œá€²?**
- Îµ-greedy á€’á€«á€™á€¾á€™á€Ÿá€¯á€á€º softmax exploration á€€á€­á€¯ action values á€á€½á€± **single forward pass** á€–á€¼á€„á€·á€ºá€›
- Actions á€¡á€™á€»á€¬á€¸á€¡á€•á€¼á€¬á€¸á€›á€¾á€­á€á€²á€· environments á€™á€¾á€¬ **high-performance implementation** á€–á€¼á€…á€º
- State á€á€…á€ºá€á€¯á€€á€­á€¯ neural network á€™á€¾ pass á€á€…á€ºá€€á€¼á€­á€™á€ºá€–á€¼á€„á€·á€º all actions' Q-values á€›

**NFQ architecture (cart-pole): 4 â†’ 512 â†’ 128 â†’ 2**

```mermaid
graph LR
    I["Input Layer<br/>4 nodes"] -->|ReLU| H1["Hidden Layer 1<br/>512 nodes"]
    H1 -->|ReLU| H2["Hidden Layer 2<br/>128 nodes"]
    H2 -->|No activation| O["Output Layer<br/>2 nodes<br/>[Q(s,left), Q(s,right)]"]
    
    style I fill:#ff922b,color:#fff
    style H1 fill:#64B5F6,color:#fff
    style H2 fill:#64B5F6,color:#fff
    style O fill:#4CAF50,color:#fff
```

> âš ï¸ **Output layer á€™á€¾á€¬ activation function á€™á€á€¯á€¶á€¸á€•á€«** â€” Q-values á€á€½á€± positive/negative á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€²á€·á€¡á€á€½á€€á€º unbounded output á€œá€­á€¯á€•á€«á€á€šá€º!

---

### Python Code â€” Fully Connected Q-function (FCQ)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FCQ(nn.Module):
    def __init__(self, 
                 input_dim,          # State variables á€¡á€›á€±á€¡á€á€½á€€á€º (cart-pole: 4)
                 output_dim,         # Actions á€¡á€›á€±á€¡á€á€½á€€á€º (cart-pole: 2)
                 hidden_dims=(32,32),        # Hidden layers sizes (tuple)
                 activation_fc=F.relu):      # Activation function
        super(FCQ, self).__init__()
        self.activation_fc = activation_fc
        
        # Input layer: input_dim â†’ first hidden layer
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        
        # Hidden layers: flexible (any number, any size)
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])
            self.hidden_layers.append(hidden_layer)
        
        # Output layer: last hidden â†’ output_dim
        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
    
    def _format(self, state):
        """State á€€á€­á€¯ tensor format á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º convert"""
        x = state
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, device=self.device, dtype=torch.float32)
            x = x.unsqueeze(0)  # batch dimension á€‘á€Šá€·á€º
        return x
    
    def forward(self, state):
        x = self._format(state)
        
        # Input layer + ReLU
        x = self.activation_fc(self.input_layer(x))
        
        # Hidden layers + ReLU
        for hidden_layer in self.hidden_layers:
            x = self.activation_fc(hidden_layer(x))
        
        # Output layer â€” activation á€™á€á€¯á€¶á€¸! (unbounded Q-values)
        x = self.output_layer(x)
        return x
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `hidden_dims=(32,32)` â†’ hidden layers á‚ á€á€¯ (32, 32 units) á€–á€¼á€„á€·á€º initializeá‹ `(64, 32, 16)` á€†á€­á€¯á€›á€„á€º layers áƒ á€á€¯ á€–á€¼á€…á€ºá€™á€Šá€º
2. Input layer á€€á€­á€¯ `input_dim â†’ hidden_dims[0]` á€–á€¼á€„á€·á€º define
3. Loop á€–á€¼á€„á€·á€º flexible number of hidden layers á€–á€”á€ºá€á€®á€¸
4. Output layer á€€á€­á€¯ `hidden_dims[-1] â†’ output_dim` á€–á€¼á€„á€·á€º connect
5. `forward()` â€” state á€€á€­á€¯ raw input á€¡á€–á€¼á€…á€ºá€šá€°á€•á€¼á€®á€¸ tensor á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º convert
6. Input + hidden layers á€€á€”á€± activation function pass á€•á€¼á€¯á€œá€¯á€•á€º
7. Output layer á€™á€¾á€¬ activation á€™á€á€¯á€¶á€¸ â€” Q-values á€€ range á€™á€€á€”á€·á€ºá€á€á€ºá€•á€«

---

### Decision Point 3 â€” Optimize á€œá€¯á€•á€ºá€™á€Šá€·á€º Objective á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

**Ideal objective (impossible):**
RL á€€á€­á€¯ supervised learning problem á€Ÿá€¯ á€šá€°á€†á€•á€¼á€®á€¸ optimal Q-values á€€á€­á€¯ labels á€¡á€–á€¼á€…á€ºá€‘á€¬á€¸á€”á€­á€¯á€„á€ºá€›á€„á€º á€€á€±á€¬á€„á€ºá€¸á€•á€±á€™á€Šá€º:

$$\mathcal{L}(\theta) = \mathbb{E}\left[ \left( q^*(s, a) - Q(s, a; \theta) \right)^2 \right]$$

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º impossible á€œá€²?**
- Optimal action-value function $q^*(s,a)$ á€™á€á€­á€•á€«
- Optimal policy á€€á€­á€¯á€™á€á€­á€á€±á€¬á€€á€¼á€±á€¬á€„á€·á€º $q^*$ á€€á€­á€¯ sample á€œá€¯á€•á€ºá€œá€Šá€ºá€¸ á€™á€–á€¼á€…á€ºá€•á€«

**Practical approach â€” GPI pattern:**
Randomly initialized Q-function á€€á€”á€± start á€•á€¼á€®á€¸ã€policy evaluation + policy improvement á€€á€­á€¯ alternate á€•á€¼á€¯á€œá€¯á€•á€ºá€€á€¬ repeat á€•á€«á€á€šá€º:

1. Randomly initialized action-value function + implicit policy
2. Sample actions from it (policy evaluation)
3. Îµ-greedy strategy á€–á€¼á€„á€·á€º improve (policy improvement)
4. Desired performance á€›á€á€²á€·á€‘á€­ iterate

> âš ï¸ **Warning:** Non-linear function approximation á€á€¯á€¶á€¸á€á€²á€·á€¡á€á€½á€€á€ºá€€á€¼á€±á€¬á€„á€·á€º convergence guarantees á€›á€¾á€­á€á€±á€¬á€·á€™á€•á€«! á€’á€«á€Ÿá€¬ "deep RL á Wild West" á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

---

### Decision Point 4 â€” Policy Evaluation á Targets á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

Policy evaluate á€œá€¯á€•á€ºá€›á€¬á€á€½á€„á€º target á€™á€»á€­á€¯á€¸á€…á€¯á€¶ á€›á€¾á€­á€•á€«á€á€šá€º:

```
MC target     â†’ Full trajectory reward
TD target     â†’ r + Î³ V(s')       â† NFQ á€á€¯á€¶á€¸
N-step target â†’ r + r' + ... + Î³â¿V(sâ¿)
Lambda target â†’ weighted combination of N-step targets
```

NFQ á€™á€¾á€¬ simplicity á€›á€¾á€­á€…á€±á€–á€­á€¯á€· **off-policy TD target (Q-learning target)** á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€á€šá€º:

$$y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$$

**On-policy vs. Off-policy targets:**

| Target Type | á€˜á€¬á€€á€­á€¯ approximate á€œá€¯á€•á€ºá€á€œá€² |
|---|---|
| **On-policy (SARSA target)** | Behavioral policy (Îµ-greedy á€›á€²á€· value function) |
| **Off-policy (Q-learning target)** | Greedy policy (behavior policy á€˜á€¬á€•á€²á€–á€¼á€…á€ºá€–á€¼á€…á€º) |

> ğŸ’¡ Off-policy method á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º behavior policy á€€ virtually anything á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º (broad support á€›á€¾á€­á€á€›á€½á€±á€·)!

---

### Python Code â€” Q-learning Target

```python
# Q-learning target implementation
# next_states: batch of next states s'
q_sp = self.online_model(next_states).detach()  # â† CRITICAL: detach() á€™á€–á€¼á€…á€ºá€™á€”á€±!
max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)       # max action value for each s'
max_a_q_sp *= (1 - is_terminals)                # terminal states â†’ 0
target_q_s = rewards + self.gamma * max_a_q_sp  # Q-learning target

q_sa = self.online_model(states).gather(1, actions)  # current Q(s,a) prediction
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `online_model(next_states)` â€” Q-values á€€á€­á€¯ s' (next state batch) á€¡á€á€½á€€á€º á€›á€šá€°
2. **`.detach()` â€” CRITICAL!** Target calculation á€¡á€á€½á€€á€º gradient propagate á€œá€¯á€•á€ºá€á€½á€„á€·á€º á€™á€•á€±á€¸á€›
3. `.max(1)[0]` â€” action dimension á€–á€¼á€„á€·á€º max Q-value á€›á€šá€° (greedy action á value)
4. `.unsqueeze(1)` â€” dimension add á‹
5. `(1 - is_terminals)` â€” terminal states á value á€€á€­á€¯ 0 á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º ground á€•á€¼á€¯á€œá€¯á€•á€º â† **Important!**
6. Q-learning target á€€á€­á€¯ á€á€½á€€á€º
7. `gather(1, actions)` â€” current Q(s,a) prediction á€€á€­á€¯ action indices á€–á€¼á€„á€·á€º gather

> âš ï¸ **RL implementation á common error 1:** Predicted values á€€á€”á€±á€á€¬ backpropagate á€œá€¯á€•á€ºá€›á€•á€«á€™á€Šá€º! Target calculation á€€á€”á€± gradient á€€á€­á€¯ propagate á€œá€¯á€•á€ºá€á€½á€„á€·á€º á€™á€•á€±á€¸á€› â€” targets á€€á€­á€¯ **constants** á€¡á€–á€¼á€…á€º treat á€›á€•á€«á€™á€Šá€º!

---

### Terminal State Handling â€” Time Limit Trap

OpenAI Gym á cart-pole environment á€á€½á€„á€º **time limit wrapper** á€›á€¾á€­á€•á€«á€á€šá€º:
- CartPole-v0: 200 steps
- CartPole-v1: 500 steps

**Problem:** Pole straight up á€–á€¼á€…á€ºá€”á€±á€•á€¼á€®á€¸ step 500 á€™á€¾á€¬ timeout á€–á€¼á€…á€ºá€›á€„á€º terminal flag á€€á€­á€¯ pass á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º! á€’á€«á€•á€±á€™á€šá€·á€º á€’á€® state á true value á€€á€á€±á€¬á€· **infinite** á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Careful á€™á€–á€¼á€…á€ºá€›á€„á€º zero á€•á€±á€«á€ºá€™á€¾á€¬ bootstrap á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€­á€™á€Šá€ºá€Ÿá€¯ á€†á€­á€¯á€œá€­á€¯á€•á€«á€á€Šá€º!

**Solution:**

```python
# Proper terminal state handling â€” Time limit trap á€€á€­á€¯ á€›á€¾á€±á€¬á€„á€ºá€›á€¾á€¬á€¸á€›á€”á€º
new_state, reward, is_terminal, info = env.step(action)

# Time limit á€€á€¼á€±á€¬á€„á€·á€º terminated á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸á€œá€¬á€¸ á€…á€…á€ºá€†á€±á€¸
is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']

# Real failure = terminate + NOT truncated
is_failure = is_terminal and not is_truncated

# Only is_failure á€€á€­á€¯á€á€¬ terminal flag á€¡á€–á€¼á€…á€º use á€•á€«!
experience = (state, action, reward, new_state, float(is_failure))
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. Experience tuple á€€á€­á€¯ á€•á€¯á€¶á€™á€¾á€”á€ºá€¡á€á€­á€¯á€„á€ºá€¸ collect
2. `TimeLimit.truncated` key á€€á€­á€¯ info dict á€‘á€² á€…á€…á€ºá€†á€±á€¸
3. Real failure = terminal á€–á€¼á€…á€ºá€•á€¼á€®á€¸ time limit á€€á€¼á€±á€¬á€„á€·á€º á€™á€Ÿá€¯á€á€ºá€˜á€² á€–á€¼á€…á€ºá€á€¬
4. `is_failure` á€€á€­á€¯á€á€¬ terminal flag á€¡á€–á€¼á€…á€ºá€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€•á€« â€” time limit á€€á€¼á€±á€¬á€„á€·á€º á€–á€¼á€…á€ºá€•á€«á€€ value of new_state á€•á€±á€«á€ºá€™á€¾á€¬ bootstrap á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º

> ğŸ’¡ **Pole straight up á€–á€¼á€…á€ºá€”á€±á€á€²á€· state 500 á value á€€á€˜á€¬á€œá€²?** Pole straight up á€–á€¼á€…á€ºá€•á€¼á€®á€¸ step á€á€­á€¯á€„á€ºá€¸ +1 á€›á€”á€±á€•á€«á€€ á€’á€® state á true value á€€á€á€±á€¬á€· **infinite** á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Zero á€•á€±á€«á€ºá€™á€¾á€¬ bootstrap á€†á€­á€¯á€›á€„á€º algorithm á€€á€­á€¯ mislead á€•á€±á€¸á€•á€«á€™á€Šá€º!

---

### Decision Point 5 â€” Exploration Strategy á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

GPI á policy improvement step á€¡á€á€½á€€á€º exploration strategy á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€›á€•á€«á€™á€Šá€ºá‹ Chapter 4 á€™á€¾á€¬ exploration-exploitation trade-off balance á€œá€¯á€•á€ºá€á€²á€· techniques á€™á€»á€¬á€¸ survey á€œá€¯á€•á€ºá€á€²á€·á€•á€¼á€®á€¸ á€’á€®á€¡á€á€”á€ºá€¸á€á€½á€„á€º simplicity á€›á€¾á€­á€–á€­á€¯á€· **Îµ-greedy strategy** á€€á€­á€¯ á€á€¯á€¶á€¸á€•á€«á€™á€Šá€ºá‹

**Off-policy learning á implication:**
NFQ á€á€Šá€º off-policy learning algorithm á€–á€¼á€…á€ºá€•á€¼á€®á€¸ **policies á‚ á€á€¯** á€›á€¾á€­á€•á€«á€á€šá€º:
- **Behavior policy** â€” behavior generate á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€· policy (Îµ-greedy with Îµ=0.5)
- **Target policy** â€” learn á€”á€±á€á€±á€¬ policy (greedy/optimal)

Off-policy learning á interesting fact: Behavior policy á€€ virtually anything á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º (state-action pairs á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ sufficient exploration á€›á€¾á€­á€á€›á€½á€±á€·)!

NFQ training á€™á€¾á€¬: **Îµ = 0.5** (50% random, 50% greedy)
NFQ evaluation á€™á€¾á€¬: **greedy policy** (learned Q-function á€¡á€•á€±á€«á€º greedy)

---

### Python Code â€” Îµ-greedy Exploration Strategy

```python
class EGreedyStrategy:
    def __init__(self, epsilon=0.1):
        self.epsilon = epsilon
    
    def select_action(self, model, state):
        with torch.no_grad():
            # State á Q-values á€€á€­á€¯ model á€€á€”á€± á€›á€šá€°
            q_values = model(state).cpu().detach()
            q_values = q_values.data.numpy().squeeze()  # NumPy-friendly
        
        if np.random.rand() > self.epsilon:
            # Exploit: greedy action (highest Q-value)
            action = np.argmax(q_values)
        else:
            # Explore: random action
            action = np.random.randint(len(q_values))
        
        return action
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `select_action` â€” state s á€¡á€á€½á€€á€º Q-values á€€á€­á€¯ á€›á€šá€°á€•á€¼á€®á€¸ action á€›á€½á€±á€¸á€á€»á€šá€º
2. `torch.no_grad()` â€” inference á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º gradient computation á€™á€œá€­á€¯á€˜á€°á€¸
3. Q-values á€€á€­á€¯ NumPy-friendly á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º convert á€•á€¼á€¯á€œá€¯á€•á€º
4. `rand() > epsilon` â€” Îµ probability á€–á€¼á€„á€·á€º exploit (greedy)
5. Otherwise â†’ explore (random)

> ğŸ’¡ Performance goal á€†á€­á€¯á€›á€„á€º model á€€á€­á€¯ query á€™á€œá€¯á€•á€ºá€•á€«á€”á€²á€·! Stats calculation á€”á€¾á€„á€·á€º training á€€á€­á€¯á€á€¬ query á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«!

---

### Decision Point 6 â€” Loss Function á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

Loss function á€€á€á€±á€¬á€· neural network predictions á accuracy á€€á€­á€¯ measure á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ Supervised learning á€™á€¾á€¬ true values á€€á€­á€¯ á€€á€¼á€­á€¯á€á€„á€ºá€á€­á€•á€¼á€®á€¸ loss calculate á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€á€¬ á€•á€­á€¯á€›á€­á€¯á€¸á€›á€¾á€„á€ºá€¸á€•á€«á€á€šá€ºá‹

**NFQ á€™á€¾á€¬ MSE (Mean Squared Error / L2 loss) á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€šá€º:**

$$\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \Big( Q(s_i, a_i; \theta) - y_i \Big)^2$$

- $Q(s_i, a_i; \theta)$ = predicted values (neural network á€€á€”á€± directly â€” good!)
- $y_i$ = TD targets (network prediction á€•á€±á€«á€ºá€™á€¾á€¬ depend â€” problematic!)

**Circular Dependency Problem:**

```mermaid
graph LR
    PI["Policy Ï€"] -->|"produces"| DATA["Data (experiences)"]
    DATA -->|"used to calculate"| TARGET["Targets (y)"]
    TARGET -->|"used to train"| Q["Q-function Q(s,a;Î¸)"]
    Q -->|"produces"| PI
    
    style PI fill:#ff922b,color:#fff
    style Q fill:#2196F3,color:#fff
    style TARGET fill:#ef5350,color:#fff
```

> âš ï¸ "True values" á€á€½á€±á€Ÿá€¬ network á predictions á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º á€•á€±á€«á€ºá€™á€¾á€¬ depend á€•á€«á€á€šá€º! á€’á€«á€á€Šá€º supervised learning á assumptions á€á€»á€­á€¯á€¸á€–á€±á€¬á€€á€ºá€•á€¼á€®á€¸ **problems á€–á€¼á€…á€ºá€•á€±á€«á€º á€á€á€ºá€•á€«á€á€šá€º**!

---

### Decision Point 7 â€” Optimization Method á€›á€½á€±á€¸á€á€»á€šá€ºá€á€¼á€„á€ºá€¸

Gradient descent á€á€Šá€º data **IID (independent and identically distributed)** á€–á€¼á€…á€ºá€•á€¼á€®á€¸ targets **stationary** á€–á€¼á€…á€ºá€™á€¾ stable optimization method á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ RL á€™á€¾á€¬ á€’á€® assumptions á€”á€¾á€…á€ºá€á€¯á€œá€¯á€¶á€¸ violated á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

**Optimization Methods Comparison:**

```mermaid
graph TD
    BGD["Batch Gradient Descent<br/>(full dataset at once)"] -->|"too slow<br/>no dataset in advance"| MBGD["Mini-batch GD<br/>(subset, 32-1024 samples)"]
    MBGD -->|"single sample"| SGD["Stochastic GD<br/>(per sample, noisy)"]
    
    MBGD -->|"+ moving avg<br/>of gradients"| MOM["Momentum<br/>(faster but aggressive)"]
    MBGD -->|"+ avg magnitude<br/>of gradient"| RMS["RMSprop âœ…<br/>(stable, preferred)"]
    MOM -->|"combine"| ADAM["Adam âœ…<br/>(RMSprop + Momentum)"]
    RMS -->|"combine"| ADAM
    
    style RMS fill:#4CAF50,color:#fff
    style ADAM fill:#2196F3,color:#fff
```

| Optimizer | á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º | DRL Suitability |
|---|---|---|
| **Batch GD** | Dataset á€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ á€á€…á€ºá€€á€¼á€­á€™á€ºá€á€Šá€ºá€¸ | âŒ RL á€™á€¾á€¬ dataset á€€á€¼á€­á€¯á€á€„á€ºá€™á€›á€¾á€­ |
| **Mini-batch GD** | Small batch per step | âœ… Common (batch size 32-1024) |
| **SGD** | Single sample per step | âš ï¸ High variance |
| **Momentum** | Moving average of gradients á€–á€¼á€„á€·á€º step á€œá€¾á€™á€ºá€¸ | âœ… Fast but aggressive |
| **RMSprop** | Gradient magnitude á moving avg á€–á€¼á€„á€·á€º scale | âœ… **Preferred** for value-based |
| **Adam** | Momentum + RMSprop á€•á€±á€«á€„á€ºá€¸á€…á€•á€º | âœ… Good but more aggressive |

**NFQ á€á€½á€„á€º RMSprop á€€á€­á€¯ á€›á€½á€±á€¸á€á€»á€šá€ºá€á€Šá€·á€º á€¡á€€á€¼á€±á€¬á€„á€ºá€¸:**
- Stable, hyperparameters sensitivity á€”á€Šá€ºá€¸
- Value-based deep RL methods á€™á€¾á€¬ á€‘á€­á€‘á€­á€›á€±á€¬á€€á€ºá€›á€±á€¬á€€á€º work á€•á€«á€á€šá€º

---

### Miguel á Analogy â€” Optimization Methods á€€á€­á€¯ Visualize á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

> **RMSprop á€€á€­á€¯ landscape analogy á€–á€¼á€„á€·á€º á€”á€¬á€¸á€œá€Šá€ºá€€á€¼á€•á€«á€™á€Šá€º:**
>
> Loss function á€€á€­á€¯ hills, valleys, flat plains á€•á€«á€á€²á€· landscape á€Ÿá€¯ imagine á€•á€«á‹
>
> - **Downhill á€€á€­á€¯ á€á€½á€¬á€¸á€”á€±á€›á€„á€º** (high gradients) â†’ steep surface á€™á€¾ flat valley á€†á€® change á€–á€¼á€…á€ºá€œá€¬á€•á€«á€€ gradients' moving average magnitude á€á€Šá€º most recent gradient á€‘á€€á€º á€€á€¼á€®á€¸á€•á€«á€™á€Šá€º â†’ step size á€€á€­á€¯ **reduce** á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ oscillations/overshoot á€€á€­á€¯ prevent á€•á€«á€á€šá€º
>
> - **Near-flat surface á€™á€¾á€¬ á€›á€¾á€­á€›á€„á€º** (small gradients) â†’ significant gradient á€†á€® change á€–á€¼á€…á€ºá€œá€¬á€•á€«á€€ average magnitude á€€ small á€–á€¼á€…á€ºá€•á€¼á€®á€¸ new gradient á€€ large á€–á€¼á€…á€ºá€™á€Šá€º â†’ step size á€€á€­á€¯ **increase** á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ learning á€€á€­á€¯ speed up á€•á€«á€á€šá€º
>
> **Adam** á€€á€á€±á€¬á€· gradient velocity direction á€–á€¼á€„á€·á€º step á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ gradient magnitude moving avg á€–á€¼á€„á€·á€º scale á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€· combo á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” RMSprop á€‘á€€á€º aggressive á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º Momentum á€‘á€€á€º conservative á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## áˆ.á† â€” NFQ á Full Algorithm

### NFQ á Selections Summary

NFQ algorithm á final components:

| Component | Selection |
|---|---|
| **Value function** | $Q(s, a; \theta)$ â€” approximate optimal action-value function |
| **Architecture** | State-in, values-out: 4 â†’ 512 â†’ 128 â†’ 2 |
| **Objective** | Optimize to approximate $q^*(s,a)$ |
| **Targets** | Off-policy TD: $r + \gamma \max_{a'} Q(s', a'; \theta)$ |
| **Exploration** | Îµ-greedy with Îµ = 0.5 |
| **Loss** | MSE (Mean Squared Error) |
| **Optimizer** | RMSprop, learning rate = 0.0005 |

### NFQ á 3 Main Steps (Nested Loop)

```mermaid
graph LR
    S1["Step 1:<br/>E=1024 experiences<br/>collect"] --> S2["Step 2:<br/>Off-policy TD targets<br/>r + Î³ max Q(s',a';Î¸) calculate"]
    S2 --> S3["Step 3:<br/>Q(s,a;Î¸) fit<br/>MSE + RMSprop"]
    S3 -->|"K=40 á€€á€¼á€­á€™á€º<br/>steps 2-3 repeat"| S2
    S3 -->|"K steps á€•á€¼á€®á€¸á€›á€„á€º<br/>step 1 á€á€­á€¯á€· á€•á€¼á€”á€º"| S1
    
    style S1 fill:#ff922b,color:#fff
    style S2 fill:#2196F3,color:#fff
    style S3 fill:#4CAF50,color:#fff
```

**NFQ Algorithm á Key:**
- **"Fitted"** á€†á€­á€¯á€á€¬ nested loop á€•á€«á€á€²á€· structure á€€á€¼á€±á€¬á€„á€·á€º á€–á€¼á€…á€ºá€•á€«á€á€šá€º
- Step 1 á€™á€¾ experiences collect á€•á€¼á€®á€¸á€›á€„á€º â†’ Step 2-3 á€€á€­á€¯ K=40 á€€á€¼á€­á€™á€º repeat
- K fitting steps á€¡á€•á€¼á€Šá€·á€ºá€•á€¼á€Šá€·á€ºá€•á€¼á€®á€¸á€™á€¾ â†’ Step 1 á€á€­á€¯á€· á€•á€¼á€”á€ºá€€á€¬ new experiences collect

---

### Python Code â€” NFQ Agent Core

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque

class NFQ:
    def __init__(self, 
                 env,
                 value_model_fn,       # Neural network factory (FCQ)
                 value_optimizer_fn,   # Optimizer factory (RMSprop)
                 value_optimizer_lr,   # Learning rate
                 training_strategy_fn, # Exploration strategy (Îµ-greedy)
                 evaluation_strategy_fn,
                 n_warmup_batches,     # Initial random exploration
                 max_gradient_norm):   # Gradient clipping
        
        self.env = env
        self.gamma = 0.99
        
        nS, nA = env.observation_space.shape[0], env.action_space.n
        
        # Neural network: state â†’ Q-values for all actions
        self.online_model = value_model_fn(nS, nA)
        self.optimizer = value_optimizer_fn(
            self.online_model.parameters(), lr=value_optimizer_lr)
        
        self.training_strategy = training_strategy_fn()
        self.evaluation_strategy = evaluation_strategy_fn()
        
        self.max_gradient_norm = max_gradient_norm
        self.n_warmup_batches = n_warmup_batches
    
    def optimize_model(self, experiences):
        """Experience batch á€€á€”á€± neural network á€€á€­á€¯ update á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        states, actions, rewards, next_states, is_terminals = experiences
        
        # === TD Target calculation ===
        q_sp = self.online_model(next_states).detach()      # gradient á€€á€­á€¯ cut
        max_a_q_sp = q_sp.max(1)[0].unsqueeze(1)
        max_a_q_sp *= (1 - is_terminals)                    # terminal â†’ 0
        target_q_s = rewards + self.gamma * max_a_q_sp
        
        # === Predicted values ===
        q_sa = self.online_model(states).gather(1, actions)
        
        # === Loss calculation (MSE) ===
        loss = nn.MSELoss()(q_sa, target_q_s)
        
        # === Backpropagation ===
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping (optional â€” explosion á€€á€­á€¯ prevent)
        nn.utils.clip_grad_norm_(
            self.online_model.parameters(), self.max_gradient_norm)
        
        self.optimizer.step()
        return loss.item()
    
    def interaction_step(self, state, env):
        """Environment á€”á€¾á€„á€·á€º interact á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€¼á€®á€¸ experience á€€á€­á€¯ return"""
        action = self.training_strategy.select_action(
            self.online_model, state)
        
        new_state, reward, is_terminal, info = env.step(action)
        
        # Time limit trap handling
        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']
        is_failure = is_terminal and not is_truncated
        
        experience = (state, action, reward, new_state, float(is_failure))
        return new_state, is_terminal, experience
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `online_model` â€” FCQ neural network (state â†’ Q-values)
2. `optimize_model()` â€” experience batch á€€á€”á€± network update
3. `.detach()` â€” target computation á€™á€¾á€¬ gradient á€€á€­á€¯ cut (**critical!**)
4. `(1 - is_terminals)` â€” terminal states á€€á€­á€¯ zero á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º ground
5. MSE loss á€–á€¼á€„á€·á€º predicted Q(s,a) á€”á€¾á€„á€·á€º target á€€á€¼á€¬á€¸ loss compute
6. `clip_grad_norm_` â€” gradient explosion á€€á€­á€¯ prevent (**important for stability!**)
7. `interaction_step()` â€” single environment step á€†á€±á€¬á€„á€ºá€›á€½á€€á€º

---

### Python Code â€” NFQ Training Loop

```python
def train(self, make_envs_fn, make_env_fn, seed, gamma,
          max_minutes, max_episodes, goal_mean_100_reward):
    """Main NFQ training loop"""
    
    training_start = time.time()
    last_debug_time = float('-inf')
    
    self.seed = seed
    torch.manual_seed(self.seed)
    np.random.seed(self.seed)
    
    env = make_env_fn(CARTPOLE_ID, seed=self.seed)
    eval_env = make_env_fn(CARTPOLE_ID, seed=self.seed + 1)
    
    nS, nA = env.observation_space.shape[0], env.action_space.n
    
    # NFQ á 3 steps
    training_experiences = []  # Step 1: experience buffer
    
    for episode in range(1, max_episodes + 1):
        # === Step 1: Collect E=1024 experiences ===
        state, is_terminal = env.reset(), False
        episode_reward = 0
        
        while not is_terminal:
            new_state, is_terminal, experience = \
                self.interaction_step(state, env)
            training_experiences.append(experience)
            state = new_state
        
        # Enough experiences gathered? Start training
        if len(training_experiences) >= self.n_warmup_batches * BATCH_SIZE:
            
            # === Steps 2 & 3: K=40 fitting iterations ===
            for _ in range(K_FITTING_STEPS):
                # Random mini-batch from collected experiences
                idxs = np.random.choice(
                    len(training_experiences), BATCH_SIZE, replace=False)
                batch = [training_experiences[i] for i in idxs]
                
                # Experience batch format
                states = torch.FloatTensor([e[0] for e in batch])
                actions = torch.LongTensor([e[1] for e in batch]).unsqueeze(1)
                rewards = torch.FloatTensor([e[2] for e in batch]).unsqueeze(1)
                next_states = torch.FloatTensor([e[3] for e in batch])
                is_terminals = torch.FloatTensor([e[4] for e in batch]).unsqueeze(1)
                
                experiences = (states, actions, rewards, next_states, is_terminals)
                self.optimize_model(experiences)
            
            # Next episode áŒ training experiences á€€á€­á€¯ clear
            training_experiences = []
    
    return self.online_model
```

---

## áˆ.á‡ â€” NFQ á Result

### Cart-Pole Environment áŒ NFQ á Performance

NFQ á€á€Šá€º state-of-the-art method á€™á€Ÿá€¯á€á€ºá€•á€±á€™á€šá€·á€º cart-pole á€€á€²á€·á€á€­á€¯á€· simple environment á€á€½á€„á€º decent performance á€•á€¼á€á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º:

**Observations:**
1. **Training reward** á€á€Šá€º max 500 á€€á€­á€¯ á€˜á€šá€ºá€á€±á€¬á€·á€™á€¾ á€™á€›á€±á€¬á€€á€ºá€•á€« â€” Îµ=0.5 exploration rate á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º! High exploration á€€á€¼á€±á€¬á€„á€·á€º more accurate value functions á€›á€”á€­á€¯á€„á€ºá€•á€±á€™á€šá€·á€º training â†“â†“
2. **Evaluation reward** (greedy) â€” agent á best possible performance
3. **Main issue** â€” NFQ á€á€Šá€º decent performance á€›á€›á€¾á€­á€›á€”á€º samples **á€•á€­á€¯á€™á€»á€¬á€¸á€œá€½á€”á€ºá€¸** á€•á€«á€á€šá€º â€” sample efficiency á€Šá€¶á€·á€á€²á€· method
4. **Time** â€” Average ~80 seconds á€á€”á€·á€º environment pass á€–á€¼á€…á€ºá€•á€«á€á€šá€º

---

## áˆ.áˆ â€” á€–á€¼á€…á€ºá€•á€±á€«á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ (á€–á€¼á€…á€ºá€•á€±á€«á€ºá€á€Šá€·á€º) á€•á€¼á€¿á€”á€¬á€™á€»á€¬á€¸

### Problem 1 â€” Non-stationary Targets

**á€•á€¼á€¿á€”á€¬:** Non-linear function approximator á€–á€¼á€…á€ºá€á€Šá€·á€º neural network á€€á€­á€¯ á€á€¯á€¶á€¸á€á€²á€·á€¡á€á€½á€€á€ºá€€á€¼á€±á€¬á€„á€·á€º **similar states á€á€½á€±á€Ÿá€¬ value á€á€½á€±á€€á€­á€¯ á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º adjust á€–á€¼á€…á€ºá€•á€«á€á€šá€º**ã€‚

**Target values á€á€½á€±á€Ÿá€¬ next state á values á€•á€±á€«á€ºá€™á€¾á€¬ depend á€•á€«á€á€šá€º** â€” á€’á€® states á€á€½á€±á€€á€­á€¯ adjust á€”á€±á€á€±á€¬ states á€á€½á€±á€”á€¾á€„á€·á€º similar á€–á€¼á€…á€ºá€™á€Šá€ºá€Ÿá€¯ á€šá€°á€†á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º?

```mermaid
graph LR
    A["Optimization adjusts<br/>Q(s_t, a_t; Î¸)"] -->|"Since similar states also change"| B["Q(s_{t+1}, a'; Î¸) also changes!"]
    B -->|"But this IS the target!"| C["Target also moves!"]
    C -->|"Most recent update is now outdated"| D["Training becomes unstable!"]
    
    style A fill:#2196F3,color:#fff
    style D fill:#ef5350,color:#fff
```

Q-function á weights update á€–á€¼á€…á€ºá€á€¿ target á€á€½á€±á€œá€Šá€ºá€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€á€½á€¬á€¸á€•á€¼á€®á€¸ most recent update á€€á€­á€¯ outdated á€–á€¼á€…á€ºá€…á€±á€•á€«á€á€šá€º! **Moving target á€€á€­á€¯ á€›á€¾á€…á€ºá€”á€±á€á€œá€­á€¯á€•á€²**-á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

$$\theta \text{ changes} \Rightarrow Q(S_{t+1}, a'; \theta) \text{ changes} \Rightarrow y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta) \text{ changes!}$$

### Problem 2 â€” Data is NOT IID (Correlated Data)

**á€•á€¼á€¿á€”á€¬:** NFQ á€™á€¾á€¬ **online collected 1024 experience samples** á€–á€¼á€„á€·á€º network á€€á€­á€¯ batch update á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹ á€’á€® samples á€á€¿ same trajectory á€”á€¾á€„á€·á€º policy á€€á€”á€± á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” **highly correlated!**

Optimization methods á€á€¿ training samples á€á€¿ $\text{IID}$ (independent and identically distributed) á€–á€¼á€…á€ºá€á€á€Šá€ºá€Ÿá€¯ assume á€•á€«á€á€šá€º:
- **Independent:** State $s_{t+1}$ á outcome á€á€Šá€º current state $s_t$ á€•á€±á€«á€º depend á€•á€«á€á€šá€º â† violated!
- **Identically distributed:** Policy á€€á€•á€¼á€±á€¬á€„á€ºá€¸á€”á€±á€á€²á€·á€¡á€á€½á€€á€º data-generating process á€•á€¼á€±á€¬á€„á€ºá€¸á€á€½á€¬á€¸á€•á€«á€á€šá€º â† violated!

```mermaid
graph TD
    subgraph SL["Supervised Learning (IID âœ…)"]
        SL1["Dataset: shuffled, fixed samples"]
        SL2["Samples: independent of each other"]
        SL3["Distribution: fixed, static"]
    end
    
    subgraph RL["RL (NOT IID âŒ)"]
        RL1["Data: online collected, sequential"]
        RL2["Samples: correlated (s_t â†’ s_{t+1})"]
        RL3["Distribution: changes as Ï€ improves"]
    end
    
    SL -->|"IID âœ…"| OK["Optimization works well"]
    RL -->|"NOT IID âŒ"| BAD["Optimization instable/diverge!"]
    
    style SL fill:#4CAF50,color:#fff
    style RL fill:#ef5350,color:#fff
    style BAD fill:#9C27B0,color:#fff
```

**Data correlated with time:**
> Imagine trajectory á€á€…á€ºá€á€¯á€‘á€²á€€ cart y-position á€€á€­á€¯ x-axis = time, y-axis = state variable á€–á€¼á€„á€·á€º plot á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á‹ Adjacent time steps áŒ data points á€á€¿ similar á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€™á€Šá€ºá€€á€­á€¯ á€™á€¼á€„á€ºá€›á€•á€«á€™á€Šá€º â€” function approximator á€€á€­á€¯ á€’á€® local region á€†á€® overfit á€–á€¼á€…á€ºá€…á€±á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º!

> ğŸ’¡ **á€’á€® problems á‚ á€á€¯á€€á€­á€¯ Chapter 9 á€™á€¾á€¬ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€«á€™á€Šá€º:**
> - **Experience Replay** â†’ Data correlation á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸ (IID violation)
> - **Target Network** â†’ Non-stationary targets á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸
> á€’á€® á‚ á€á€¯á€€á€­á€¯ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€•á€¼á€®á€¸ **DQN (Deep Q-Network)** á€†á€­á€¯á€á€²á€· algorithm á€–á€¼á€…á€ºá€œá€¬á€•á€«á€™á€Šá€º!

---

## áˆ.á‰ â€” Key Equations Summary

| Equation | Formula |
|---|---|
| **Q-function approximation** | $Q(s, a; \theta) \approx q^*(s, a)$ |
| **Ideal objective** | $\mathcal{L}(\theta) = \mathbb{E}\left[ (q^*(s,a) - Q(s,a;\theta))^2 \right]$ |
| **TD target (off-policy)** | $y_t = R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; \theta)$ |
| **MSE Loss** | $\mathcal{L}(\theta) = \frac{1}{N}\sum_i (Q(s_i,a_i;\theta) - y_i)^2$ |
| **Gradient update** | $\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)$ |
| **Atari state space** | $(255^3)^{210 \times 160}$ (242,580-digit number) |
| **Go states** | $10^{170}$ |
| **Observable universe atoms** | $10^{78}$ to $10^{82}$ |

---

## áˆ.áá€ â€” á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º (Summary)

á€’á€® chapter á€™á€¾á€¬ sampled feedback á€á€Šá€º sequential á€”á€¾á€„á€·á€º evaluative feedback á€á€­á€¯á€·á€”á€¾á€„á€·á€º interact á€–á€¼á€…á€ºá€•á€¯á€¶á€€á€­á€¯ high-level overview á€•á€±á€¸á€•á€¼á€®á€¸ simple deep RL agent (FCQ neural network á€–á€¼á€„á€·á€º Q-function approximate) á€€á€­á€¯ introduce á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€ºá‹

**á€á€„á€ºá€šá€°á€á€²á€·á€á€Šá€ºá€™á€»á€¬á€¸:**

1. **Sampled feedback** â€” deep RL á third dimension; exhaustive sampling impossible á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸
2. **High-dimensional vs Continuous** â€” state space complexity á á‚ dimensions
3. **Function approximation** â€” generalization á€–á€¼á€„á€·á€º unseen states á€€á€­á€¯á€œá€Šá€ºá€¸ handle; value function relationships discover
4. **NFQ algorithm** â€” first value-based DRL method; batch + fitting approach
5. **7 Decision points** â€” value function, architecture, objective, targets, exploration, loss, optimizer á€›á€½á€±á€¸á€á€»á€šá€ºá€•á€¯á€¶
6. **IID violation** â€” online data is correlated and non-identically distributed
7. **Non-stationary targets** â€” targets change as network updates
8. **Terminal state handling** â€” time limits vs real failures á€€á€­á€¯ distinguish á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€™á€Šá€º

```mermaid
graph TD
    NFQ["NFQ Algorithm"] --> COMP["Components"]
    COMP --> VF["Q(s,a;Î¸)<br/>Value Function"]
    COMP --> ARCH["State-in, Values-out<br/>FCQ Architecture"]
    COMP --> TGT["Off-policy TD target<br/>r + Î³ max Q(s',a';Î¸)"]
    COMP --> EXP["Îµ-greedy<br/>Îµ=0.5 Exploration"]
    COMP --> LOSS["MSE Loss<br/>L2"]
    COMP --> OPT["RMSprop<br/>lr=0.0005"]
    
    NFQ --> ISSUES["âš ï¸ Known Issues"]
    ISSUES --> IID["Data NOT IID<br/>(correlated)"]
    ISSUES --> NST["Non-stationary targets<br/>(moving target)"]
    
    ISSUES -->|"Chapter 9 á€™á€¾á€¬ Solve"| CH9["DQN:<br/>Target Networks +<br/>Experience Replay"]
    
    style NFQ fill:#ffd43b,color:#000
    style ISSUES fill:#ef5350,color:#fff
    style CH9 fill:#4CAF50,color:#fff
```

**á€’á€® chapter á€•á€¼á€®á€¸á€”á€±á€¬á€€á€º:**
- âœ… Sequential, evaluative, and sampled feedback á€€á€”á€± learn á€á€á€ºá€á€±á€¬ agent á€–á€”á€ºá€á€®á€¸á€”á€­á€¯á€„á€º
- âœ… Continuous state-space RL problems solve á€”á€­á€¯á€„á€º
- âœ… Value-based DRL methods á components á€”á€¾á€„á€·á€º issues á€”á€¬á€¸á€œá€Šá€ºá€”á€­á€¯á€„á€º

> ğŸ’¡ **Chapter 9 Preview:** NFQ á€á€Šá€º DRL á foundation á€•á€±á€™á€šá€·á€º sample efficiency á€”á€Šá€ºá€¸á€•á€¼á€®á€¸ stability issues á€›á€¾á€­á€•á€«á€á€šá€ºá‹ Chapter 9 á€™á€¾á€¬ DQN (<ins>Deep Q-Network</ins>) á€–á€¼á€„á€·á€º á€’á€® issues á€á€¿á€€á€­á€¯ address á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€™á€Šá€º â€” **experience replay** á€–á€¼á€„á€·á€º IID violation á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€•á€¼á€®á€¸ **target network** á€–á€¼á€„á€·á€º non-stationary target problem á€€á€­á€¯ á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€™á€Šá€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º!
