# á€¡á€á€”á€ºá€¸ áá€ â€” Sample-Efficient Value-Based Methods (á€”á€™á€°á€”á€¬ á€‘á€­á€›á€±á€¬á€€á€ºá€™á€¾á€¯á€™á€¼á€„á€·á€º Value-based á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€™á€»á€¬á€¸)

> *"á€‰á€¬á€á€ºá€›á€Šá€ºá€‰á€¬á€á€ºá€á€½á€±á€¸á€á€Šá€º á€™á€»á€­á€¯á€¸á€…á€­á€á€ºá€á€…á€ºá€á€¯á€á€»á€„á€ºá€¸á€…á€® áá€„á€ºá€¸á€á€­á€¯á€·á€›á€¾á€„á€ºá€á€”á€ºá€›á€”á€ºá€œá€­á€¯á€á€±á€¬á€™á€»á€¬á€¸á€€á€­á€¯ á€™á€Šá€ºá€™á€»á€¾ á€‘á€­á€›á€±á€¬á€€á€ºá€œá€¬á€á€Šá€ºá€€á€­á€¯ á€¡á€á€¼á€±á€á€¶á á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€á€Šá€º"*
> â€” Charles Darwin
> (English naturalist, geologist, and biologist; á€€á€™á€¹á€˜á€¬á€· evolution science á á€€á€¼á€®á€¸á€™á€¬á€¸á€á€±á€¬á€•á€¶á€·á€•á€­á€¯á€¸á€á€°)

## á€’á€®á€¡á€á€”á€ºá€¸á€™á€¾á€¬ á€á€„á€ºá€šá€°á€›á€™á€Šá€·á€ºá€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

- Value-based deep RL methods á nuances á€€á€­á€¯ exploit á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ deep neural network architecture á€€á€­á€¯ implement á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º
- Experiences á€€á€­á€¯ á€™á€Šá€ºá€™á€»á€¾ surprising á€–á€¼á€…á€ºá€á€Šá€ºá€€á€­á€¯á€¡á€á€¼á€±á€™á€°á prioritize á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ replay buffer á€€á€­á€¯ á€–á€”á€ºá€á€®á€¸á€™á€Šá€º
- á€šá€á€„á€º value-based DRL agents á€¡á€¬á€¸á€œá€¯á€¶á€¸á€‘á€€á€º episodes á€”á€Šá€ºá€¸á€”á€Šá€ºá€¸á€–á€¼á€„á€·á€º near-optimal policy á€€á€­á€¯ train á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€±á€¬ agent á€€á€­á€¯ build á€™á€Šá€º

---

## áá€.á â€” á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º: á€’á€®á€”á€±á€· á€˜á€¬á€‘á€•á€º á€†á€±á€¬á€„á€ºá€›á€½á€€á€ºá€™á€Šá€ºá€œá€²

á€€á€»á€½á€”á€ºá€¯á€•á€ºá€á€­á€¯á€·á€á€Šá€º á€šá€á€„á€º chapters á€á€½á€„á€º:

| Algorithm | Focus |
|---|---|
| **NFQ (Ch 8)** | RL + Neural Networks á á€•á€‘á€™á€†á€¯á€¶á€¸ attempt |
| **DQN (Ch 9)** | Experience Replay + Target Network â†’ stability |
| **DDQN (Ch 9)** | Double learning â†’ overestimation bias â†“ |
| **Dueling DDQN (Ch 10)** | RL-aware architecture â†’ sample efficiency â†‘ |
| **PER (Ch 10)** | Prioritized replay â†’ sample efficiency â†‘â†‘ |

á€’á€® chapter á improvements á€á€½á€±á€á€Šá€º stability á€¡á€á€½á€€á€º á€™á€Ÿá€¯á€á€ºá€˜á€² **sample efficiency** á€€á€­á€¯ focus á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º:
1. **Dueling DDQN** â€” Q-function á€€á€­á€¯ V(s) + A(s,a) á€Ÿá€¯ split á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ architecture
2. **PER (Prioritized Experience Replay)** â€” Surprising experiences á€€á€­á€¯ more frequently sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

```mermaid
graph TD
    DQN["DQN<br/>Replay Buffer + Target Network"] --> DDQN["DDQN<br/>+ Double Learning"]
    DDQN --> DUAL["Dueling DDQN<br/>+ Dueling Architecture<br/>(Ch 10: Sample Efficiency â†‘)"]
    DUAL --> PER["Dueling DDQN + PER<br/>+ Prioritized Replay<br/>(Ch 10: Sample Efficiency â†‘â†‘)"]
    
    style DQN fill:#9E9E9E,color:#fff
    style DDQN fill:#2196F3,color:#fff
    style DUAL fill:#4CAF50,color:#fff
    style PER fill:#9C27B0,color:#fff
```

---

## áá€.á‚ â€” Dueling DDQN: RL-Aware Neural Network Architecture

### áá€.á‚.á â€” RL á€á€Šá€º Supervised Learning á€™á€Ÿá€¯á€á€ºá€•á€«

Chapter 9 á€á€½á€„á€º RL á€€á€­á€¯ supervised learning á€†á€„á€ºá€á€°á€¡á€±á€¬á€„á€º á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€”á€º effort á€•á€±á€¸á€á€²á€·á€•á€«á€á€šá€º:
- Experience replay â†’ data IID á€–á€¼á€…á€ºá€•á€¯á€¶á€•á€±á€«á€º
- Target networks â†’ targets static á€–á€¼á€…á€ºá€•á€¯á€¶á€•á€±á€«á€º

á€’á€«á€á€Šá€º training á€€á€­á€¯ stabilize á€•á€¼á€¯á€…á€±á€•á€«á€á€šá€ºâ€”á€’á€«á€•á€±á€™á€šá€·á€º RL problems á nuances á€€á€­á€¯ ignore á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€á€Šá€º optimal approach á€™á€Ÿá€¯á€á€ºá€•á€«!

**RL á á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º nuances á€á€…á€ºá€á€¯á€™á€¾á€¬:** Value functions á€á€…á€ºá€á€¯á€”á€¾á€„á€·á€º á€á€…á€ºá€á€¯á€€á€¼á€¬á€¸ relationship á€–á€¼á€…á€ºá€•á€«á€á€šá€º:

- **State-value function $V(s)$** â€” State á€á€…á€ºá€á€¯á overall goodness
- **Action-value function $Q(s,a)$** â€” State á€™á€¾ action á€á€…á€ºá€á€¯á€€á€­á€¯ á€šá€°á€•á€¼á€®á€¸ policy á€€á€­á€¯ follow á€†á€€á€ºá€•á€«á€€ expected return
- **Action-advantage function $A(s,a)$** â€” Policy á default action á€”á€¾á€„á€·á€º á€šá€¾á€‰á€ºá€œá€»á€¾á€„á€º action $a$ á€€ á€™á€Šá€ºá€™á€»á€¾ á€•á€­á€¯á€€á€±á€¬á€„á€ºá€¸á€á€Šá€º

**Value functions á Formal Definitions:**

$$Q^\pi(s, a) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0=s, A_0=a, \pi\right]$$

$$V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0=s, \pi\right]$$

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

**Key relationship:**

$$Q(s, a) = V(s) + A(s, a)$$

$$\mathbb{E}_{a \sim \pi}\left[A^\pi(s, a)\right] = 0$$

> ğŸ’¡ Policy á default action á€€á€­á€¯ average á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€€ zero advantage â€” á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬ default action á€€á€”á€± advantage á€™á€›á€¾á€­á€›!

---

### â‚â‚€.â‚‚.â‚‚ â€” Value Functions á Relationship á€€á€­á€¯ Exploit á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

**á€šá€á€¯ RA architecture (naive):**

```
Experience (s,a,r,s',d) â†’ Q(s,a) update
                        â†’ Only Q(s,a) learns from this sample!
                        â†’ Q(s,a2), Q(s,a3) unchanged!
```

**á€‘á€•á€ºá€á€­á€¯á€¸ insight:** $V(s)$ á€á€Šá€º state $s$ áŒ actions **á€¡á€¬á€¸á€œá€¯á€¶á€¸**á€€á€­á€¯ share á€•á€¼á€¯á€á€±á€¬ component á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

```
Experience (s,a1,r,s',d) â†’ V(s) + A(s,a1) update
                         â†’ V(s) changes â†’ Q(s,a1) AND Q(s,a2) BOTH improve!
```

**Cart-pole á€¥á€•á€™á€¬:**

```
State: [0.02, -0.01, -0.02, -0.04] â† pole almost vertical, cart near center
V(s) = HIGH value (good state!)
A(s, left)  = -small  (doesn't matter much which direction)
A(s, right) = +small  (doesn't matter much which direction)
Q(s, left)  = V(s) + A(s, left)  â‰ˆ high
Q(s, right) = V(s) + A(s, right) â‰ˆ high (similar!)

State: [-0.16, -1.97, 0.24, 3.01] â† pole falling right!
V(s) = LOW value (bad state!)
A(s, left)  = -large  (going left = BAD, pole falls more!)
A(s, right) = +large  (pushing right = GOOD, counter the fall!)
Q(s, left)  = V(s) + A(s, left)  << 0
Q(s, right) = V(s) + A(s, right) > 0
```

```mermaid
graph TD
    subgraph NAIVE["Naive Q-network (current)"]
        EXP1["Experience (s,a1)"] --> Q1["Update Q(s,a1) only"]
        Q1 -.->|"indirect only"| Q2["Q(s,a2) â† barely learns"]
    end
    
    subgraph DUAL["Dueling Network âœ…"]
        EXP2["Experience (s,a1)"] --> VS["Update V(s)"]
        EXP2 --> AS["Update A(s,a1)"]
        VS -->|"SHARED by all actions"| QA["Q(s,a1) = V(s) + A(s,a1)  improved!"]
        VS -->|"SHARED by all actions"| QB["Q(s,a2) = V(s) + A(s,a2)  ALSO improved!"]
        AS --> QA
    end
    
    style NAIVE fill:#ef5350,color:#fff
    style DUAL fill:#4CAF50,color:#fff
```

**Key insight â€” "Taking the worst action in a good state could be better than taking the best action in a bad state":**

> RL áŒ states á values á€á€Šá€º actions á values á€€á€­á€¯ directly affect á€•á€«á€á€šá€º! V(s) á€€á€­á€¯ accurately estimate á€•á€¼á€¯á€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸á€á€Šá€º all actions á€€á€¡á€á€½á€€á€º equally important á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

**History:**

**0001 A Bit of History:**
Dueling network architecture á€€á€­á€¯ 2015 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º **Ziyu Wang** á€€ University of Oxford PhD student á€˜á€á€á€½á€„á€º "Dueling Network Architectures for Deep Reinforcement Learning" paper á€á€½á€„á€º introduce á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º! á€’á€«á€á€Šá€º value-based deep RL methods á€¡á€á€½á€€á€º **custom neural network architecture** á€€á€­á€¯á€¡á€‘á€°á€¸ design á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ á€•á€‘á€™á€†á€¯á€¶á€¸ paper á€–á€¼á€…á€ºá€•á€«á€á€šá€º! Ziyu á€á€Šá€º á€šá€á€¯ Google DeepMind research scientist á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

---

### â‚â‚€.â‚‚.â‚ƒ â€” Dueling Network Architecture

**Architecture á structure:**

```mermaid
graph LR
    INPUT["Input<br/>4 variables<br/>(cart-pole state)"] --> SHARED1["Hidden Layer 1<br/>512 nodes (shared)"]
    SHARED1 --> SHARED2["Hidden Layer 2<br/>128 nodes (shared)"]
    
    SHARED2 --> VSTREAM["Value Stream<br/>1 node (V(s))"]
    SHARED2 --> ASTREAM["Advantage Stream<br/>2 nodes (A(s,left), A(s,right))"]
    
    VSTREAM -->|"V(s)"| AGG["Aggregation Layer<br/>Q(s,a) = V(s) + A(s,a) âˆ’ mean(A(s,Â·))"]
    ASTREAM -->|"A(s,a)"| AGG
    
    AGG --> OUTPUT["Q-values output<br/>[Q(s,left), Q(s,right)]"]
    
    style INPUT fill:#ff922b,color:#fff
    style SHARED1 fill:#64B5F6,color:#fff
    style SHARED2 fill:#64B5F6,color:#fff
    style VSTREAM fill:#4CAF50,color:#fff
    style ASTREAM fill:#9C27B0,color:#fff
    style AGG fill:#ef5350,color:#fff
    style OUTPUT fill:#2196F3,color:#fff
```

**Notation:**
- $\theta$ = shared layers á weights
- $\alpha$ = action-advantage stream á weights
- $\beta$ = state-value stream á weights

### â‚â‚€.â‚‚.â‚„ â€” Aggregation Equation

**NaÃ¯ve approach (don't use):**

$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + A(s, a; \theta, \alpha)$$

**á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€º naÃ¯ve approach áŒ problem á€›á€¾á€­á€á€œá€²?**
$Q(s,a)$ á€™á€¾ $V(s)$ á€”á€¾á€„á€·á€º $A(s,a)$ á€€á€­á€¯ **uniquely recover á€™á€›**á€•á€«!

> e.g. V(s)=+10, A(s,a)=-5 â†’ Q=5
> V(s)=+20, A(s,a)=-15 â†’ Q=5 (same Q, different V and A!)

**Practical approach (used in code):**

$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left(A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s, a'; \theta, \alpha)\right)$$

**Mean á€€á€­á€¯ subtract á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á effects:**
- V(s) á€”á€¾á€„á€·á€º A(s,a) á€€á€­á€¯ constant á€–á€¼á€„á€·á€º off-center á€–á€¼á€…á€ºá€…á€±á€•á€¼á€®á€¸ **true meaning á€€á€­á€¯ lose** á€•á€¼á€¯á€œá€¯á€•á€º
- á€’á€«á€•á€±á€™á€šá€·á€º **optimization á€€á€­á€¯ stabilize** á€•á€¼á€¯á€œá€¯á€•á€º
- Q-function á **rank** á€€á€­á€¯á€á€±á€¬á€· á€™á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€² (same ordering of actions)

---

### Python Code â€” FCDuelingQ Network

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FCDuelingQ(nn.Module):
    def __init__(self, 
                 input_dim,           # State variables count (cart-pole: 4)
                 output_dim,          # Actions count (cart-pole: 2)
                 hidden_dims=(32, 32),        # Hidden layer sizes
                 activation_fc=F.relu):       # Activation function
        super(FCDuelingQ, self).__init__()
        self.activation_fc = activation_fc
        
        # Shared input layer
        self.input_layer = nn.Linear(input_dim, hidden_dims[0])
        
        # Shared hidden layers
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])
            self.hidden_layers.append(hidden_layer)
        
        # TWO separate output heads (dueling!)
        self.value_output = nn.Linear(hidden_dims[-1], 1)          # V(s): single node
        self.advantage_output = nn.Linear(hidden_dims[-1], output_dim)  # A(s,a): n_actions nodes
    
    def _format(self, state):
        x = state
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, device=self.device, dtype=torch.float32)
            x = x.unsqueeze(0)
        return x
    
    def forward(self, state):
        x = self._format(state)
        
        # === Shared layers ===
        x = self.activation_fc(self.input_layer(x))
        for hidden_layer in self.hidden_layers:
            x = self.activation_fc(hidden_layer(x))
        
        # === Two separate streams ===
        a = self.advantage_output(x)   # A(s,a): shape [batch, n_actions]
        v = self.value_output(x)       # V(s):   shape [batch, 1]
        v = v.expand_as(a)             # expand V to match A's shape
        
        # === Aggregation: Q = V + A - mean(A) ===
        q = v + a - a.mean(1, keepdim=True).expand_as(a)
        return q  # shape [batch, n_actions]
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. **Input + hidden layers (shared)** â€” state features á€€á€­á€¯ extract á€•á€¼á€¯á€á€±á€¬ common representation
2. `value_output` â€” **1 node** output â†’ $V(s)$
3. `advantage_output` â€” **n_actions nodes** output â†’ $A(s, a_1), A(s, a_2), ...$
4. `v.expand_as(a)` â€” V (shape [batch,1]) á€€á€­á€¯ A (shape [batch,n_actions]) shape á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º broadcast
5. `a.mean(1, keepdim=True)` â€” actions dimension á€–á€¼á€„á€·á€º mean â†’ shape [batch, 1]
6. `v + a - mean(a)` â€” **Aggregation formula** â† dueling network á core!

> ğŸ’¡ **Cart-pole architecture:** 4 â†’ 512 â†’ 128 â†’ {V: 1 node, A: 2 nodes} â†’ Q: 2 nodes
>
> FCQ (old): `output_layer = nn.Linear(128, 2)`
> FCDuelingQ (new): `value_output = nn.Linear(128, 1)` + `advantage_output = nn.Linear(128, 2)`

---

### â‚â‚€.â‚‚.â‚… â€” Polyak Averaging: Target Network Update á€€á€­á€¯ Smooth á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

**á€šá€á€„á€º Hard update (DQN/DDQN):**

$$\theta^- \leftarrow \theta \quad \text{(every C steps, sudden full copy)}$$

**Problems:**
- C steps á€€á€¼á€¬á€á€»á€­á€”á€ºá€á€½á€„á€º progressively stale estimates
- Update á€–á€¼á€…á€ºá€á€»á€­á€”á€ºá€á€½á€„á€º sudden large change â†’ loss landscape á€€á€¼á€®á€¸á€™á€¬á€¸á€…á€½á€¬ shift

**Polyak Averaging (smooth alternative):**

$$\theta^- \leftarrow (1 - \tau) \theta^- + \tau \theta \quad \text{(every step!)}$$

- $\tau = 0.1$ â†’ 10% online + 90% target weights á€€á€­á€¯ mix in every step
- Target á€á€Šá€º always lag á€•á€±á€™á€šá€·á€º **smooth, continuous** á€–á€¼á€„á€·á€º update
- Hard update á "too conservative + too aggressive" problem á€€á€­á€¯ á€›á€¾á€±á€¬á€„á€ºá€›á€¾á€¬á€¸

```mermaid
graph LR
    subgraph HARD["Hard Update (DQN) âŒ"]
        H1["Î¸- frozen<br/>steps 1-15"] -->|"step 16: sudden full copy!"| H2["Î¸- â† Î¸ (100%)"]
        H2 --> H3["Î¸- frozen again<br/>steps 17-30"]
    end
    
    subgraph POLYAK["Polyak Averaging (Dueling DDQN) âœ…"]
        P1["Step t"] -->|"Ï„=0.1 mix"| P2["Î¸- â† 0.9Î¸- + 0.1Î¸"]
        P2 -->|"Ï„=0.1 mix"| P3["Step t+1: Î¸- â† 0.9Î¸- + 0.1Î¸"]
        P3 --> P4["Smooth, continuous lag..."]
    end
    
    style HARD fill:#ef5350,color:#fff
    style POLYAK fill:#4CAF50,color:#fff
```

---

### Python Code â€” Polyak Averaging Update

```python
class DuelingDDQN:
    def __init__(self, ..., tau=0.1):
        self.tau = tau  # mix-in ratio (0.1 = 10% online per step)
        # ... (other init code)
    
    def update_network(self, tau=None):
        """Polyak averaging: Î¸- â† (1-Ï„)Î¸- + Ï„Î¸ (every step!)"""
        tau = self.tau if tau is None else tau
        
        for target, online in zip(
                self.target_model.parameters(),
                self.online_model.parameters()):
            
            # Mix target and online weights
            target_ratio = (1.0 - tau) * target.data    # (1-Ï„) Ã— Î¸-
            online_ratio = tau * online.data             # Ï„ Ã— Î¸
            mixed_weights = target_ratio + online_ratio  # combine
            
            target.data.copy_(mixed_weights)  # update Î¸-
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `tau` â€” online weights á mix-in ratio (`tau=1.0` â†’ full copy = hard update)
2. `(1.0 - tau) * target.data` â€” target network á existing weights á€€á€­á€¯ (1-Ï„) fraction á€–á€¼á€„á€·á€º keep
3. `tau * online.data` â€” online network weights á€€á€­á€¯ Ï„ fraction á€–á€¼á€„á€·á€º mix in
4. `target.data.copy_()` â€” in-place copy â†’ no new Tensors á€–á€”á€ºá€á€®á€¸á€˜á€² update

> ğŸ’¡ `tau=None` fallback á€€á€á€±á€¬á€· caller á€€ override á€•á€¼á€¯á€œá€¯á€•á€ºá€œá€­á€¯á€œá€»á€¾á€„á€º á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º á€Ÿá€¯á€†á€­á€¯á€œá€­á€¯á€•á€«á€á€šá€º!

---

### â‚â‚€.â‚‚.â‚† â€” Dueling DDQN Algorithm Full Summary

**Dueling DDQN = DDQN + Dueling Architecture + Polyak Averaging**

| Component | DDQN | Dueling DDQN |
|---|---|---|
| **Network** | FCQ (Q-only) | **FCDuelingQ (V + A streams)** |
| **Architecture** | 4 â†’ 512 â†’ 128 â†’ 2 | 4 â†’ 512 â†’ 128 â†’ {V:1, A:2} â†’ 2 |
| **Target update** | Hard copy every 15 steps | **Polyak averaging Ï„=0.1 every step** |
| **Double learning** | âœ… | âœ… |
| **Replay buffer** | 50k, uniform | 50k, uniform |
| **Optimizer** | RMSprop, lr=0.0007 | RMSprop, lr=0.0007 |
| **Loss** | Huber/MSE (grad clip inf) | Huber/MSE (grad clip inf) |
| **Exploration** | Exp decay Îµ (1.0â†’0.3) | Exp decay Îµ (1.0â†’0.3) |

**Algorithm Steps (same 3 steps as DDQN):**

```mermaid
graph LR
    S1["1. Collect experience<br/>(s,a,r,s',d) â†’ replay buffer"] --> S2["2. Sample mini-batch<br/>Calculate off-policy TD targets<br/>r + Î³ max Q(s',a';Î¸-) [double learning]"]
    S2 --> S3["3. Fit Q(s,a;Î¸)<br/>MSE + RMSprop<br/>(Dueling network updates V+A)"]
    S3 -->|"Every step: Polyak"| UPDATE["Î¸- â† (1-Ï„)Î¸- + Ï„Î¸"]
    UPDATE --> S1
    S3 --> S1
    
    style S1 fill:#ff922b,color:#fff
    style S2 fill:#2196F3,color:#fff
    style S3 fill:#4CAF50,color:#fff
    style UPDATE fill:#9C27B0,color:#fff
```

**Performance:**
- Dueling DDQN â‰ˆ DDQN (similar episodes) â† cart-pole simple outcome
- Dueling DDQN **narrower training bounds** â† more stable
- Dueling DDQN **lower cart displacement** â† better action evaluation in "tied" states
- Dueling DDQN slightly **slower** (Polyak every step + dueling network overhead)

---

## â‚â‚€.â‚ƒ â€” PER: Meaningful Experiences á€€á€­á€¯ Prioritize á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸

### â‚â‚€.â‚ƒ.â‚ â€” Uniform Sampling á á€•á€¼á€¿á€”á€¬

DQN/DDQN áŒ replay buffer á€™á€¾ uniform random sampling á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€ºâ€”mathematically sound á€•á€±á€™á€šá€·á€º **intuitively suboptimal** á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

> Uniform sampling â†’ unimportant experiences á€€á€­á€¯á€œá€Šá€ºá€¸ equally learn â†’ resources waste!

**á€™á€¾á€¬á€¸á€á€±á€¬ approach á:** High reward experiences á€€á€­á€¯á€á€¬ replay
**á€™á€¾á€¬á€¸á€á€±á€¬ approach á‚:** Highest absolute reward experiences á€€á€­á€¯ replay

**Miguel á á€‘á€­á€¯á€€á€²á€·á€á€­á€¯á€· experiment á€™á€¾ á€á€„á€ºá€á€”á€ºá€¸á€…á€¬:**

> á€„á€«á€·á€á€™á€®á€¸á€€á€­á€¯ chocolate á€á€½á€±á€á€»á€²á€·á€•á€±á€¸á€›á€„á€ºá€¸á€”á€¾á€„á€·á€º á€á€»á€°á€•á€ºá€á€­á€¯á€„á€ºá€¸á€™á€­á€á€¬á€œá€­á€¯á€•á€² â€” high reward experiences á€€á€­á€¯á€á€¬ replay á€•á€¼á€¯á€œá€¯á€•á€ºá€›á€„á€º agent á€á€½á€± overfit á€–á€¼á€…á€ºá€•á€¼á€®á€¸ diverse experiences á€™á€¾ learn á€œá€¯á€•á€ºá€–á€­á€¯á€· fail á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º! Agents (and humans) need mundane, bad, AND good experiences to learn well!

**á€™á€¾á€”á€ºá€€á€”á€ºá€á€±á€¬ approach:** á€™á€Šá€ºá€á€Šá€·á€º experiences á€€ **á€†á€±á€¸á€™á€¼á€„á€ºá€™á€‘á€¬á€¸á€á€±á€¬ / unexpected** á€–á€¼á€…á€ºá€•á€«á€á€œá€²?

---

### â‚â‚€.â‚ƒ.â‚‚ â€” TD Error á€€á€­á€¯ Priority Measure á€¡á€–á€¼á€…á€ºá€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸

**"Surprise" á RL measure = Absolute TD error!**

$$\delta_t = \left| Q(s_t, a_t; \theta) - y_t \right| = \left| Q(s_t, a_t; \theta) - \left( r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-) \right) \right|$$

- **Current estimate $Q(s,a;\theta)$** = Agent's current belief â† expectation
- **Target $y$** = New estimate (reality check)
- **$|\delta|$** = á€™á€Šá€ºá€™á€»á€¾ wrong were we? â†’ learning opportunity á measure!

**Priorities â† TD errors â† Probabilities:**

```
High |Î´| â†’ High surprise â†’ High priority â†’ Sampled more often â†’ Learn more
Low  |Î´| â†’ Low surprise  â†’ Low priority  â†’ Sampled less often â†’ Learn less
```

**History:**

**0001 A Bit of History:**
"Prioritized Experience Replay" paper á€€á€­á€¯ 2015 á€á€¯á€”á€¾á€…á€ºá€á€½á€„á€º Google DeepMind academic group á€™á€¾ **Tom Schaul** (main author) á€€ dueling architecture paper á€”á€¾á€„á€·á€º á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º publish á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º! Tom á€á€Šá€º Technical University of Munich á€™á€¾ PhD (2011)áŠ NYU postdoc á‚ á€”á€¾á€…á€ºáŠ á€‘á€­á€¯á€·á€”á€±á€¬á€€á€º DeepMind Technologies á€€á€­á€¯ join á€•á€¼á€¯á€á€Šá€º (Google á€€á€á€šá€ºá€šá€°á€™á€Šá€·á€º á† á€œ á€™á€á€­á€¯á€„á€ºá€™á€®)! Tom á€á€Šá€º PyBrain (early ML framework for Python) á€”á€¾á€„á€·á€º PyVGDL (video game description language) á core developer á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

---

### â‚â‚€.â‚ƒ.â‚ƒ â€” Priorities á€™á€¾ Probabilities á€†á€®

**TD errors á€á€…á€ºá€á€¯á€á€Šá€ºá€¸ á€™á€•á€¼á€Šá€·á€ºá€…á€¯á€¶ â€” á€˜á€¬á€€á€¼á€±á€¬á€„á€·á€ºá€†á€­á€¯á€á€±á€¬:**
1. TD error = 0 á€–á€¼á€…á€ºá€•á€«á€€ experience á€€á€­á€¯ never replay á€–á€¼á€…á€ºá€™á€Šá€º
2. Function approximators áŒ errors slowly shrink â†’ small subset á€€á€­á€¯á€á€¬ concentrate update
3. TD errors noisy á€–á€¼á€…á€ºá€•á€«á€á€šá€º (stochastic environments + noisy networks)

**Solution: Stochastic prioritization** â€” greedy á€™á€Ÿá€¯á€á€ºá€˜á€² probabilistic á€–á€¼á€„á€·á€º sample á€•á€¼á€¯á€œá€¯á€•á€º!

**Three-tier structure:**

$$\text{TD error} \xrightarrow{\text{+Îµ}} \text{priority } p_i \xrightarrow{\text{Ã· sum, Ã— Î±}} \text{probability } P(i) \xrightarrow{\text{sample}} \text{experience}$$

---

### â‚â‚€.â‚ƒ.â‚„ â€” Proportional Prioritization

**Step 1: Priority Calculation:**

$$p_i = |\delta_i| + \varepsilon$$

- $|\delta_i|$ = absolute TD error of experience $i$
- $\varepsilon$ = small constant (e.g., 0.01) â†’ zero TD error experience á€€á€­á€¯á€œá€Šá€ºá€¸ replay chance á€•á€±á€¸

**Step 2: Probabilities from Priorities:**

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$

- $\alpha$ = prioritization degree (hyperparameter, 0 â‰¤ Î± â‰¤ 1)
  - $\alpha = 0$ â†’ uniform sampling (all equal)
  - $\alpha = 1$ â†’ pure priority sampling (proportional to TD error)
  - Typical: $\alpha = 0.6$

```mermaid
graph LR
    A0["Î± = 0"] -->|"all probabilities equal"| U["Uniform Sampling"]
    A1["Î± = 1"] -->|"proportional to  Î´ "| P["Full Priority Sampling"]  
    AH["Î± = 0.6 (typical)"] -->|"blend"| B["Blended Sampling âœ…"]
    
    style B fill:#4CAF50,color:#fff
```

---

### â‚â‚€.â‚ƒ.â‚… â€” Rank-based Prioritization (Alternative)

**Proportional approach á issue:** Outlier experiences (noise á€€á€¼á€±á€¬á€„á€·á€º huge TD error) á€€á€­á€¯ overly sample á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€Šá€º!

**Alternative: Rank-based prioritization:**

1. Experiences á€€á€­á€¯ absolute TD error á€–á€¼á€„á€·á€º sort (descending)
2. Rank á€€á€­á€¯ assign: highest TD error = rank 1

$$p_i = \frac{1}{\text{rank}(i)}$$

$$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha} \quad \text{(same Î± formula)}$$

**Comparison:**

| Method | Outlier sensitivity | Implementation |
|---|---|---|
| **Proportional** | Sensitive to outliers | Simpler, use +Îµ |
| **Rank-based** | Robust to outliers | Needs sorting (slower) |

> ğŸ’¡ PER paper implementation á€™á€¾á€¬ proportional approach á€€á€­á€¯ preferred á€–á€¼á€…á€ºá€•á€¼á€®á€¸ Îµ=0.01, Î±=0.6 á‚á€½á€„á€·á€º á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¯á€¶á€¸á€•á€«á€á€šá€º!

---

### â‚â‚€.â‚ƒ.â‚† â€” Prioritization Bias á€”á€¾á€„á€·á€º Importance Sampling

**á€•á€¼á€¿á€”á€¬:** Priority distribution á€–á€¼á€„á€·á€º sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸á€á€Šá€º **biased gradient estimates** á€€á€­á€¯ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€…á€±á€•á€«á€á€šá€º!

RL Q-function update á expectation á€á€Šá€º **true data-generating distribution** á€™á€¾ á€–á€¼á€…á€ºá€›á€•á€«á€™á€Šá€º:

$$\nabla_\theta \mathbb{E}_{(s,a,r,s') \sim U(D)} \left[\mathcal{L}(\theta)\right]$$

á€’á€«á€•á€±á€™á€šá€·á€º PER á€á€Šá€º priority distribution $P(i) \neq U(D)$ á€–á€¼á€„á€·á€º sample á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«á€á€šá€º â†’ **biased updates!**

**Solution: Weighted Importance Sampling (IS weights):**

$$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$

$$\tilde{w}_i = \frac{w_i}{\max_j w_j}$$

- $N$ = replay buffer size
- $\beta$ = correction degree (0 = no correction, 1 = full correction)
- Normalize by max weight â†’ largest weight = 1, all others < 1
- Training start: $\beta$ low (less correction), anneal toward 1 (full correction near convergence)

**IS weights á role:**
$$\text{Corrected TD error} = \tilde{w}_i \cdot \delta_i$$

â†’ High priority samples (sampled too often) â† small weight â†’ downscale gradient
â†’ Low priority samples (sampled too rarely) â† large weight â†’ upscale gradient
â†’ Net effect: updates behave as if sampled uniformly!

**Full gradient update (Dueling DDQN + PER):**

$$\theta \leftarrow \theta + \alpha \sum_{i} \tilde{w}_i \delta_i \nabla_\theta Q(s_i, a_i; \theta)$$

where $(s_i, a_i, r_i, s'_i) \sim P(\cdot)$ (priority distribution)

---

### Python Code â€” Prioritized Replay Buffer (Part 1/2)

```python
import numpy as np

EPS = 1e-6  # small constant to avoid zero priority

class PrioritizedReplayBuffer:
    def __init__(self, max_samples=10000, batch_size=64,
                 rank_based=False, alpha=0.6,
                 beta0=0.1, beta_rate=0.99992):
        
        self.max_samples = max_samples
        self.batch_size = batch_size
        self.rank_based = rank_based  # True=rank, False=proportional
        self.alpha = alpha             # Priority blend (0=uniform, 1=full priority)
        self.beta = beta0              # IS correction (anneals toward 1.0)
        self.beta_rate = beta_rate     # Anneal rate per step
        
        self.next_index = 0
        self.n_entries = 0
        
        # Storage: each row = [td_error, sample_array]
        self.td_error_index = 0
        self.sample_index = 1
        self.memory = np.empty(shape=(max_samples, 2), dtype=np.ndarray)
    
    def _update_beta(self):
        """Beta á€€á€­á€¯ anneal á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ (0.1 â†’ 1.0 gradually)"""
        self.beta = min(1.0, self.beta / self.beta_rate)
    
    def store(self, sample):
        """Experience á€á€…á€ºá€á€¯á€€á€­á€¯ maximum priority á€–á€¼á€„á€·á€º insert á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        # New experiences always get maximum priority (guarantees replay at least once)
        priority = 1.0
        if self.n_entries > 0:
            priority = self.memory[:self.n_entries, self.td_error_index].max()
        
        # Store priority + experience
        self.memory[self.next_index, self.td_error_index] = priority
        self.memory[self.next_index, self.sample_index] = np.array(sample)
        
        # Update counters (circular buffer)
        self.n_entries = min(self.n_entries + 1, self.max_samples)
        self.next_index += 1
        self.next_index = self.next_index % self.max_samples
    
    def update(self, idxs, td_errors):
        """Replayed experiences á TD errors á€€á€­á€¯ update á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        self.memory[idxs, self.td_error_index] = np.abs(td_errors)
        
        if self.rank_based:
            # Sort by TD error descending (for rank-based priorities)
            sorted_arg = self.memory[:self.n_entries, 
                                     self.td_error_index].argsort()[::-1]
            self.memory[:self.n_entries] = self.memory[sorted_arg]
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º (store):**
1. New experience â†’ maximum priority assign â†’ buffer áŒ **at least once replay** á€€á€­á€¯ guarantee
2. `n_entries` â†’ buffer size cap (max_samples)
3. `next_index % max_samples` â†’ circular buffer (oldest evicted)

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º (update):**
1. Replayed experiences á TD errors á€€á€­á€¯ absolute value á€–á€¼á€„á€·á€º update
2. Rank-based á€†á€­á€¯á€›á€„á€º sort á€•á€¼á€¯á€œá€¯á€•á€º â†’ O(N log N) operation (expensive!)

---

### Python Code â€” Prioritized Replay Buffer (Part 2/2)

```python
    def sample(self, batch_size=None):
        """Priority-based probability á€–á€¼á€„á€·á€º mini-batch sample á€•á€¼á€¯á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸"""
        batch_size = self.batch_size if batch_size is None else batch_size
        
        # Beta annealing (bias correction increases over time)
        self._update_beta()
        
        entries = self.memory[:self.n_entries]
        
        # === Step 1: Calculate priorities ===
        if self.rank_based:
            # Rank-based: priority = 1/rank
            priorities = 1 / (np.arange(self.n_entries) + 1)  # [1, 1/2, 1/3, ...]
        else:
            # Proportional: priority = |Î´| + Îµ
            priorities = entries[:, self.td_error_index] + EPS
        
        # === Step 2: Probabilities from priorities ===
        scaled_priorities = priorities ** self.alpha    # blend: 0=uniform, 1=full priority
        pri_sum = np.sum(scaled_priorities)
        probs = np.array(scaled_priorities / pri_sum, dtype=np.float64)  # normalize
        
        # === Step 3: Importance-sampling weights ===
        weights = (self.n_entries * probs) ** (-self.beta)  # (NÂ·P(i))^{-Î²}
        normalized_weights = weights / weights.max()          # max weight = 1
        
        # === Step 4: Sample indices using priorities ===
        idxs = np.random.choice(self.n_entries, batch_size, replace=False, p=probs)
        
        # === Step 5: Extract experiences ===
        samples = np.array([entries[idx] for idx in idxs])
        samples_stacks = [np.vstack(batch_type) for batch_type in 
                          np.vstack(samples[:, self.sample_index]).T]
        
        idxs_stack = np.vstack(idxs)
        weights_stack = np.vstack(normalized_weights[idxs])
        
        return idxs_stack, weights_stack, samples_stacks
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `_update_beta()` â€” Î² á€€á€­á€¯ anneal â†’ training á€€á€¼á€¬á€œá€¬á€œá€»á€¾á€„á€º bias correction á€•á€­á€¯á€€á€¼á€®á€¸
2. Rank-based: `1/(rank+1)` â†’ sorted array á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º index = rank-1
3. `priorities ** alpha` â†’ blend uniform â†” priority
4. `(NÂ·P(i))^{-Î²}` â€” IS weight formula
5. `weights / weights.max()` â€” normalize so max weight = 1
6. `np.random.choice(..., p=probs)` â€” priority-weighted sampling

---

### Python Code â€” PER Agent (optimize_model)

```python
class PER:
    def optimize_model(self, experiences):
        # Unpack experiences (now includes idxs and IS weights!)
        idxs, weights, (states, actions, rewards, 
                        next_states, is_terminals) = experiences
        batch_size = len(is_terminals)
        
        # === DDQN double learning targets ===
        argmax_a_q_sp = self.online_model(next_states).max(1)[1]  # online â†’ SELECT
        q_sp = self.target_model(next_states).detach()             # target â†’ EVALUATE
        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp]
        max_a_q_sp = max_a_q_sp.unsqueeze(1)
        max_a_q_sp *= (1 - is_terminals)
        target_q_sa = rewards + (self.gamma * max_a_q_sp)
        
        # === Current Q-values ===
        q_sa = self.online_model(states).gather(1, actions)
        
        # === TD errors ===
        td_error = q_sa - target_q_sa
        
        # === PER: IS-weighted loss ===
        # Without PER: value_loss = td_error.pow(2).mul(0.5).mean()
        # With PER:    multiply by IS weights to correct bias!
        value_loss = (weights * td_error).pow(2).mul(0.5).mean()
        
        # === Optimize ===
        self.optimizer.zero_grad()
        value_loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.online_model.parameters(), 
            self.max_gradient_norm)
        self.optimizer.step()
        
        # === Update priorities in replay buffer ===
        priorities = np.abs(td_error.detach().cpu().numpy())
        self.replay_buffer.update(idxs, priorities)  # â† new step!
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `idxs, weights, samples` â€” PER buffer á 3-tuple return
2. DDQN double learning targets (same as DDQN)
3. **`(weights * td_error).pow(2).mul(0.5).mean()`** â† IS weight-corrected loss (key PER change!)
4. `self.replay_buffer.update(idxs, priorities)` â† **replay buffer áŒ priorities update** (new step!)

> âš ï¸ **PER áŒ ONLY change:** Loss function áŒ IS weights multiply + replay buffer priorities update!

---

### â‚â‚€.â‚ƒ.â‚‡ â€” PER Training Loop

```python
class PER:
    def train(self, ...):
        for episode in range(1, max_episodes + 1):
            state = env.reset()
            
            for step in count():
                # === Step 1: Interact ===
                state, is_terminal = self.interaction_step(state, env)
                
                # === Step 2: Sample from prioritized buffer ===
                if len(self.replay_buffer) > min_samples:
                    experiences = self.replay_buffer.sample()
                    idxs, weights, samples = experiences
                    
                    # Load samples to device (GPU if available)
                    experiences = self.online_model.load(samples)
                    
                    # Reassemble with indices and weights
                    experiences = (idxs, weights) + (experiences,)
                    
                    # === Step 3: Optimize with IS-weighted loss ===
                    self.optimize_model(experiences)
                
                # === Polyak averaging target update ===
                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:
                    self.update_network()
                
                if is_terminal:
                    break
```

**Code á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€á€»á€€á€º:**
1. `self.replay_buffer.sample()` â†’ `(idxs, weights, samples)` 3-tuple return
2. `self.online_model.load(samples)` â†’ GPU á€•á€±á€«á€ºá€á€­á€¯á€· transfer
3. `(idxs, weights) + (experiences,)` â†’ optimize_model á€€á€­á€¯ expected format á€–á€¼á€„á€·á€º pass
4. `self.replay_buffer.update(idxs, priorities)` â†’ optimize_model á€‘á€² call á€•á€¼á€¯á€‘á€¬á€¸á€•á€¼á€®á€¸

---

## â‚â‚€.â‚„ â€” Complete PER Algorithm

**Dueling DDQN + PER á hyperparameters:**

| Component | Value |
|---|---|
| **Architecture** | Dueling: 4 â†’ 512 â†’ 128 â†’ {V:1, A:2} â†’ 2 |
| **Optimizer** | RMSprop, lr=0.0007 |
| **Loss** | Huber (MSE, clip_norm=âˆ) |
| **Exploration** | Exp decay Îµ (1.0 â†’ 0.3, ~20k steps) |
| **Target update** | Polyak averaging, Ï„=0.1, every step |
| **Replay buffer size** | max 10,000 (smaller than DDQN's 50k!) |
| **Min samples** | 320 |
| **Batch size** | 64 |
| **Priority Î±** | 0.6 (blend uniform + priority) |
| **IS Î² initial** | 0.1 (low correction at start) |
| **Î² anneal rate** | 0.99992 (fully annealed ~30k steps) |

**4 Main Steps:**

```mermaid
graph LR
    S1["1. Collect experience<br/>(s,a,r,s',d) â†’<br/>PER buffer (max priority!)"] --> S2["2. Sample mini-batch<br/>(priority-weighted)<br/>get idxs, IS weights, samples"]
    S2 --> S3["3. Fit Q(s,a;Î¸)<br/>IS-weighted MSE<br/>+ double learning targets"]
    S3 --> S4["4. Update priorities<br/>in replay buffer<br/>using new  Î´  values"]
    S4 -->|"Polyak Î¸- â† (1-Ï„)Î¸-+Ï„Î¸"| S1
    
    style S1 fill:#ff922b,color:#fff
    style S2 fill:#2196F3,color:#fff
    style S3 fill:#4CAF50,color:#fff
    style S4 fill:#9C27B0,color:#fff
```

---

## â‚â‚€.â‚… â€” Algorithm Comparison: Full Picture

```mermaid
graph TD
    subgraph ALGOS["Value-based DRL Algorithm Family"]
        NFQ["NFQ<br/>Batch + K iterations<br/>Single network"]
        DQN["DQN<br/>+ Experience Replay<br/>+ Target Network (hard)"]
        DDQN["DDQN<br/>+ Double Learning<br/>(online select, target eval)"]
        DUELING["Dueling DDQN<br/>+ Dueling Architecture V+A<br/>+ Polyak Averaging"]
        PER["Dueling DDQN + PER<br/>+ Prioritized Replay<br/>+ IS Weights"]
        
        NFQ -->|"+Stability"| DQN
        DQN -->|"+Reduce bias"| DDQN
        DDQN -->|"+Sample efficiency via architecture"| DUELING
        DUELING -->|"+Sample efficiency via replay"| PER
    end
    
    style NFQ fill:#9E9E9E,color:#fff
    style DQN fill:#ff922b,color:#fff
    style DDQN fill:#2196F3,color:#fff
    style DUELING fill:#4CAF50,color:#fff
    style PER fill:#9C27B0,color:#fff
```

**Performance Summary (Cart-pole):**

| Algorithm | Episodes | Steps | Training time |
|---|---|---|---|
| **NFQ** | ~2,500 | ~250,000 | ~80 sec |
| **DQN** | ~250 | ~50,000 | ~5 min |
| **DDQN** | ~250 | ~50,000 | ~5 min |
| **Dueling DDQN** | slightly < DDQN | slightly < DDQN | ~5.5 min |
| **PER** | **fewest** | **fewest** | slower (impl. overhead) |

> âš ï¸ **PER á slowness:** Array sorting (O(N log N) per update) á€€á€¼á€±á€¬á€„á€·á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€º! High-quality implementations (SumTree data structure) á€–á€¼á€„á€·á€º O(log N) á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º fix á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º!

---

## â‚â‚€.â‚† â€” Key Equations Summary

| Equation | Formula |
|---|---|
| **Q decomposition** | $Q(s,a) = V(s) + A(s,a)$ |
| **Advantage definition** | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ |
| **Dueling aggregation** | $Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \left(A(s,a;\theta,\alpha) - \frac{1}{\|\mathcal{A}\|}\sum_{a'} A(s,a')\right)$ |
| **Polyak averaging** | $\theta^- \leftarrow (1-\tau)\theta^- + \tau\theta$ |
| **TD Priority** | $p_i = \|\delta_i\| + \varepsilon$ (proportional) |
| **Rank priority** | $p_i = 1/\text{rank}(i)$ (rank-based) |
| **Priority â†’ Probability** | $P(i) = p_i^\alpha / \sum_k p_k^\alpha$ |
| **IS weight** | $w_i = (N \cdot P(i))^{-\beta}$ |
| **Normalized IS weight** | $\tilde{w}_i = w_i / \max_j w_j$ |
| **PER loss** | $\mathcal{L} = \frac{1}{N}\sum_i \tilde{w}_i \cdot \delta_i^2 / 2$ |

---

## â‚â‚€.â‚‡ â€” Value-based Methods á Limitations

Value-based methods á€á€½á€±á€á€Šá€º powerful á€–á€¼á€…á€ºá€•á€±á€™á€šá€·á€º **cons** á€á€…á€ºá€á€»á€­á€¯á€· á€›á€¾á€­á€€á€¼á€±á€¬á€„á€ºá€¸ á€á€­á€‘á€¬á€¸á€–á€­á€¯á€· á€œá€­á€¯á€•á€«á€á€šá€º:

**1. Hyperparameter Sensitivity**
Value-based methods á€á€½á€±á€á€Šá€º hyperparameters á€€á€­á€¯ **sensitive** á€–á€¼á€…á€ºá€•á€«á€á€šá€º! á€€á€±á€¬á€„á€ºá€¸á€á€²á€· hyperparameter values á€‘á€€á€º á€™á€€á€±á€¬á€„á€ºá€¸á€á€²á€· values á€€á€­á€¯ á€•á€­á€¯á€œá€»á€¾á€„á€ºá€á€½á€±á€·á€›á€á€Šá€ºá€Ÿá€¯ á€†á€­á€¯á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º!

**2. Markovian Assumption**
Value-based methods á€á€½á€± environment á€€á€­á€¯ **Markovian** á€Ÿá€¯á€šá€°á€†á€•á€«á€á€šá€º â€” states á€á€Šá€º agent áŒ á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬á€á€á€„á€ºá€¸á€¡á€á€»á€€á€ºá€¡á€œá€á€ºá€¡á€¬á€¸á€œá€¯á€¶á€¸á€•á€«á€á€„á€ºá€•á€«á€á€Šá€ºá€Ÿá€¯ á€šá€°á€†á€•á€«á€á€šá€º! Non-Markovian environments áŒ performance á€€á€»á€†á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€º!

**3. The Deadly Triad**

```mermaid
graph TD
    Boot["Bootstrapping<br/>(TD learning)"] --- Offpol["Off-policy Learning<br/>(Q-learning targets)"]
    Offpol --- FA["Function Approximation<br/>(Neural Networks)"]
    FA --- Boot
    
    Boot -->|"combined"| DT["DEADLY TRIAD<br/>âš ï¸ Known to cause divergence!"]
    
    style DT fill:#ef5350,color:#fff
    style Boot fill:#ff922b,color:#fff
    style Offpol fill:#2196F3,color:#fff
    style FA fill:#9C27B0,color:#fff
```

Target networks + Replay buffers + Double learning á€–á€¼á€„á€·á€º practical á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º mitigate á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€±á€™á€šá€·á€º **fundamental divergence risk** á€€á€­á€¯ eliminate á€•á€¼á€¯á€œá€¯á€•á€ºá€™á€›á€•á€« â€” research á€†á€€á€ºá€œá€€á€ºá€–á€¼á€±á€›á€¾á€„á€ºá€¸á€”á€±á€†á€²á€–á€¼á€…á€ºá€•á€«á€á€šá€º!

**Practical advice:**
- Target networks âœ…
- Replay buffers âœ…
- Double learning âœ…
- Sufficiently small learning rate (but not too small) âœ…
- A little bit of patience âœ… ğŸ˜„

---

## â‚â‚€.â‚ˆ â€” á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

á€’á€® chapter á€™á€¾á€¬ value-based DRL methods á **sample efficiency** á€€á€­á€¯ improve á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ á‚ á€”á€Šá€ºá€¸á€€á€­á€¯ learn á€•á€¼á€¯á€œá€¯á€•á€ºá€á€²á€·á€•á€«á€á€šá€º:

**Technique 1 â€” Dueling Network Architecture:**
- $Q(s,a) = V(s) + A(s,a)$ á€€á€­á€¯ explicitly model á€•á€¼á€¯á€œá€¯á€•á€º
- Experience á€á€…á€ºá€á€¯á€á€Šá€º $V(s)$ á€€á€­á€¯ update á€–á€¼á€á€ºá€•á€¼á€®á€¸ **all actions** á€†á€® improve
- Polyak averaging á€–á€¼á€„á€·á€º smooth target updates

**Technique 2 â€” Prioritized Experience Replay (PER):**
- TD error á€€á€­á€¯ "surprise measure" á€¡á€–á€¼á€…á€ºá€á€¯á€¶á€¸
- Surprise/unexpected experiences á€€á€­á€¯ more frequently replay
- IS weights á€–á€¼á€„á€·á€º bias correction

```mermaid
graph TD
    subgraph CH10["Chapter 10 Takeaways"]
        DUAL2["Dueling Architecture<br/>âœ… V(s) shared by all actions<br/>âœ… Better evaluation of similar actions<br/>âœ… Polyak averaging smooth update"]
        PER2["Prioritized Experience Replay<br/>âœ… TD error = surprise measure<br/>âœ… Stochastic prioritization<br/>âœ… IS weights correct bias"]
    end
    
    DUAL2 --> SE1["Sample Efficiency â†‘"]
    PER2 --> SE2["Sample Efficiency â†‘â†‘"]
    SE1 --> GOAL["Near-optimal policy in FEWER episodes"]
    SE2 --> GOAL
    
    style DUAL2 fill:#4CAF50,color:#fff
    style PER2 fill:#9C27B0,color:#fff
    style GOAL fill:#ef5350,color:#fff
```

**By now, you can:**
- RL problems á€€á€­á€¯ continuous state spaces á€–á€¼á€„á€·á€º solve á€•á€¼á€¯á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º
- Value-based DRL agents á€€á€­á€¯ stabilize á€•á€¼á€¯á€”á€­á€¯á€„á€º (Ch 9)
- Value-based DRL agents á€€á€­á€¯ more sample efficient á€–á€¼á€…á€ºá€¡á€±á€¬á€„á€º á€•á€¼á€¯á€”á€­á€¯á€„á€º (Ch 10)

> ğŸ’¡ **Chapter 11 Preview:** Value-based methods á€€á€”á€± **Policy-gradient methods** á€†á€® á€€á€°á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€™á€Šá€º!
> REINFORCE, VPG, A3C, GAE, A2C â€” directly policy á€€á€­á€¯ optimize á€•á€¼á€¯á€œá€¯á€•á€ºá€á€±á€¬ algorithms!
> Value-based strengths: discrete actions, sample efficiency
> Policy-gradient strengths: continuous actions, stochastic policies, no Markovian assumption required!
