# Chapter 2: Mathematical Foundations of Reinforcement Learning - á€¡á€€á€»á€‰á€ºá€¸á€á€»á€¯á€•á€º

## 1. Complex Sequential Decision-Making Under Uncertainty

RL á€•á€¼á€¿á€”á€¬á€á€½á€±á€€á€­á€¯ á€”á€¬á€¸á€œá€Šá€ºá€–á€­á€¯á€· á€…á€€á€¬á€¸á€œá€¯á€¶á€¸ (áƒ) á€œá€¯á€¶á€¸á€€á€­á€¯ á€á€­á€‘á€¬á€¸á€›á€•á€«á€™á€šá€º:

- **Complex** â€” Agent á€á€½á€±á€€ vast state/action spaces á€‘á€²á€™á€¾á€¬ á€á€„á€ºá€šá€°á€›á€•á€«á€á€šá€ºá‹ **Sampled feedback** á€€á€”á€± generalize á€œá€¯á€•á€ºá€›á€á€¬ á€…á€­á€”á€ºá€á€±á€«á€ºá€™á€¾á€¯á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
- **Sequential** â€” Action á€á€½á€±á€›á€²á€· consequences á€á€½á€±á€€ delayed á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ Credit assignment á€€á€­á€¯ **sequential feedback** á€€á€”á€± á€œá€¯á€•á€ºá€›á€á€¬ á€á€€á€ºá€á€²á€•á€«á€á€šá€ºá‹
- **Uncertainty** â€” World á€›á€²á€· inner workings á€€á€­á€¯ á€™á€á€­á€á€²á€·á€¡á€á€½á€€á€º exploration á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€šá€ºá‹ Exploration-exploitation balance á€€á€­á€¯ **evaluative feedback** á€€á€”á€± á€›á€¾á€¬á€›á€•á€«á€á€šá€ºá‹

á€’á€®á€•á€¼á€¿á€”á€¬á€á€½á€±á€€á€­á€¯ **Markov Decision Processes (MDPs)** á€†á€­á€¯á€á€²á€· mathematical framework á€”á€²á€· model á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

---

## 2. RL á€›á€²á€· á€¡á€“á€­á€€ Components (á‚) á€á€¯

### Agent (Decision Maker)
- Agent á€†á€­á€¯á€á€¬ decision-making code á€€á€­á€¯ á€†á€­á€¯á€œá€­á€¯á€•á€«á€á€šá€ºá‹ Robot arm á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º agent á€™á€Ÿá€¯á€á€ºá€•á€« â€” decision á€á€»á€á€²á€· code á€•á€² agent á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
- Agent á€á€­á€¯á€„á€ºá€¸á€™á€¾á€¬ step (áƒ) á€á€¯ á€›á€¾á€­á€•á€«á€á€šá€º:
  1. **Interact** â€” Environment á€”á€²á€· interact á€œá€¯á€•á€ºá€•á€¼á€®á€¸ data á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸
  2. **Evaluate** â€” á€œá€€á€ºá€›á€¾á€­ behavior á€€á€­á€¯ á€¡á€€á€²á€–á€¼á€á€ºá€á€¼á€„á€ºá€¸
  3. **Improve** â€” Inner components á€á€½á€±á€€á€­á€¯ á€•á€¼á€¯á€•á€¼á€„á€ºá€•á€¼á€®á€¸ performance á€á€­á€¯á€¸á€á€€á€ºá€…á€±á€á€¼á€„á€ºá€¸

### Environment (Everything Else)
- Agent á€•á€¼á€„á€ºá€•á€€ á€¡á€›á€¬á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€ environment á€–á€¼á€…á€ºá€•á€«á€á€šá€º â€” robot arm, network latency, motor noise á€¡á€¬á€¸á€œá€¯á€¶á€¸ environment á€‘á€²á€•á€«á€•á€«á€á€šá€ºá‹
- Environment á€€á€­á€¯ **MDP** framework á€”á€²á€· represent á€œá€¯á€•á€ºá€•á€«á€á€šá€ºá‹

### RL Interaction Cycle

$$\text{Agent} \xrightarrow{\text{Action}} \text{Environment} \xrightarrow{\text{Observation, Reward}} \text{Agent} \xrightarrow{\text{Action}} \cdots$$

```mermaid
graph LR
    A["ğŸ¤– Agent<br/>(Decision Maker)"] -->|"Action a_t"| E["ğŸŒ Environment<br/>(MDP)"]
    E -->|"Observation s_t+1<br/>Reward r_t+1"| A
```

### Agent á€›á€²á€· Internal Process

```mermaid
graph TD
    I["ğŸ”„ Interact<br/>Data á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"] --> EV["ğŸ“Š Evaluate<br/>Behavior á€¡á€€á€²á€–á€¼á€á€ºá€á€¼á€„á€ºá€¸"]
    EV --> IM["ğŸ“ˆ Improve<br/>Performance á€á€­á€¯á€¸á€á€€á€ºá€…á€±á€á€¼á€„á€ºá€¸"]
    IM --> I
```

---

## 3. Markov Decision Process (MDP) á€›á€²á€· Components á€™á€»á€¬á€¸

MDP á€€á€­á€¯ á€¡á€±á€¬á€€á€ºá€•á€« components á€á€½á€±á€”á€²á€· á€–á€½á€²á€·á€…á€Šá€ºá€¸á€•á€«á€á€šá€º:

$$\text{MDP} = \langle S, A, T, R, S_\theta, \gamma, H \rangle$$

| Component | á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º |
|---|---|
| $S$ | **State Space** â€” á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€²á€· state á€¡á€¬á€¸á€œá€¯á€¶á€¸á€›á€²á€· set |
| $A$ | **Action Space** â€” á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€²á€· action á€¡á€¬á€¸á€œá€¯á€¶á€¸á€›á€²á€· set |
| $T$ | **Transition Function** â€” State-action pair á€€á€”á€± next state á€€á€­á€¯ probability mapping |
| $R$ | **Reward Function** â€” Transition á€€á€­á€¯ scalar reward mapping |
| $S_\theta$ | **Initial State Distribution** â€” Starting state probability |
| $\gamma$ | **Discount Factor** â€” Future reward á€›á€²á€· present value á€€á€­á€¯ á€á€»á€­á€”á€ºá€Šá€¾á€­á€á€²á€· factor |
| $H$ | **Horizon** â€” Task á€›á€²á€· time step limit |

---

## 4. State Space: Environment á€›á€²á€· Configuration

- **State** á€†á€­á€¯á€á€¬ environment á€›á€²á€· unique configuration á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
- **State space** $S^+$ á€€á€­á€¯ finite (FrozenLake: 16 states) á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º infinite (continuous variables) á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹
- State á€€á€­á€¯ á€–á€½á€²á€·á€…á€Šá€ºá€¸á€á€²á€· variables set á€€á€á€±á€¬á€· finite á€–á€¼á€…á€ºá€›á€•á€«á€™á€šá€ºá‹
- **State vs Observation** â€” State á€€ environment á€›á€²á€· true configuration á€–á€¼á€…á€ºá€•á€¼á€®á€¸ observation á€€ agent á€™á€¼á€„á€ºá€›á€á€¬á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹ MDP á€™á€¾á€¬ state = observation á€–á€¼á€…á€ºá€•á€¼á€®á€¸ POMDP á€™á€¾á€¬ observation â‰  state á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

### Terminal States
- Terminal state á€†á€­á€¯á€á€¬ episode á€•á€¼á€®á€¸á€†á€¯á€¶á€¸á€á€²á€· state á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
- Terminal state á€›á€²á€· action á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€ probability 1 á€”á€²á€· á€á€°á€·á€€á€­á€¯á€šá€ºá€á€° á€•á€¼á€”á€º transition á€œá€¯á€•á€ºá€•á€¼á€®á€¸ reward 0 á€•á€±á€¸á€›á€•á€«á€á€šá€ºá‹

### Markov Property

$$P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, a_0, s_1, a_1, \ldots, s_t, a_t)$$

Next state á€›á€²á€· probability á€€á€­á€¯ current state á€”á€²á€· action á€€á€”á€±á€•á€² á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ History (á€¡á€›á€„á€º states/actions) á€™á€œá€­á€¯á€¡á€•á€ºá€•á€«á‹ á€’á€«á€€á€­á€¯ **Markov property** (memoryless property) á€œá€­á€¯á€· á€á€±á€«á€ºá€•á€«á€á€šá€ºá‹

```mermaid
graph LR
    s0["sâ‚€"] -->|aâ‚€| s1["sâ‚"]
    s1 -->|aâ‚| s2["sâ‚‚"]
    s2 -->|aâ‚‚| s3["sâ‚ƒ = s_t"]
    s3 -->|"a_t"| s4["s_t+1"]
    
    style s0 fill:#ccc,stroke:#999
    style s1 fill:#ccc,stroke:#999
    style s2 fill:#ccc,stroke:#999
    style s3 fill:#4CAF50,color:#fff
    style s4 fill:#2196F3,color:#fff
```

> ğŸ’¡ $s_{t+1}$ á€€á€­á€¯ á€†á€¯á€¶á€¸á€–á€¼á€á€ºá€–á€­á€¯á€· $s_t$ á€”á€²á€· $a_t$ á€•á€² á€œá€­á€¯á€•á€«á€á€šá€ºá‹ $s_0, a_0, \ldots$ history á€™á€œá€­á€¯á€•á€«á‹

---

## 5. Action Space: Environment á€€á€­á€¯ Influence á€œá€¯á€•á€ºá€á€²á€· Mechanism

- Action space $A(s)$ á€†á€­á€¯á€á€¬ state $s$ á€™á€¾á€¬ á€›á€¾á€­á€á€²á€· actions set á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
- **Discrete actions** â€” FrozenLake: Left(0), Down(1), Right(2), Up(3)
- **Continuous actions** â€” á€¥á€•á€™á€¬ steering angle, acceleration force
- Agent á€€ action á€€á€­á€¯ deterministic (lookup table) á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º stochastic (probability distribution) á€”á€²á€· á€›á€½á€±á€¸á€á€»á€šá€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

---

## 6. Transition Function: Action á€›á€²á€· Consequences

$$T(s, a, s') = P(s_{t} = s' | s_{t-1} = s, a_{t-1} = a)$$

- State $s$ á€™á€¾á€¬ action $a$ á€šá€°á€•á€¼á€®á€¸á€›á€„á€º state $s'$ á€€á€­á€¯ á€›á€±á€¬á€€á€ºá€™á€šá€·á€º probability á€€á€­á€¯ á€•á€±á€¸á€•á€«á€á€šá€ºá‹
- **Deterministic** â€” BW environment: Left action $\rightarrow$ 100% left á€€á€­á€¯ á€á€½á€¬á€¸á€™á€šá€º
- **Stochastic** â€” FrozenLake: 33.3% á€á€»á€® intended direction + 33.3% á€á€»á€® orthogonal directions á‚ á€á€¯
- Probability distribution á€–á€¼á€…á€ºá€á€²á€·á€¡á€á€½á€€á€º: $\sum_{s' \in S^+} T(s, a, s') = 1$
- **Stationarity assumption** â€” Transition probabilities á€á€Šá€º training/evaluation á€á€…á€ºá€œá€»á€¾á€±á€¬á€€á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€¼á€„á€ºá€¸ á€™á€›á€¾á€­á€á€„á€·á€ºá€•á€«á‹

### Deterministic vs Stochastic Transition á€•á€¯á€¶

```mermaid
graph LR
    subgraph Deterministic["Deterministic - BW"]
        S1["S(1)"] -->|"Left, p=1.0"| H0["H(0) âŒ"]
        S1 -->|"Right, p=1.0"| G2["G(2) ğŸ† +1"]
    end

    subgraph Stochastic["Stochastic - BSW"]
        S1b["S(1)"] -->|"Right, p=0.8"| G2b["G(2) ğŸ† +1"]
        S1b -->|"Right, p=0.2"| H0b["H(0) âŒ"]
        S1b -->|"Left, p=0.8"| H0c["H(0) âŒ"]
        S1b -->|"Left, p=0.2"| G2c["G(2) ğŸ† +1"]
    end
```

---

## 7. Reward Function: Carrots and Sticks

$$R(s, a, s') \rightarrow \mathbb{R}$$

- Transition tuple $(s, a, s')$ á€€á€­á€¯ scalar reward value mapping á€•á€±á€¸á€•á€«á€á€šá€ºá‹
- **Positive reward** â€” Income/reward (goal á€›á€±á€¬á€€á€ºá€›á€„á€º +1)
- **Negative reward** â€” Cost/penalty (time step cost: -1)
- Reward function á€€á€­á€¯ explicit form $R(s,a,s')$, $R(s,a)$ á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º $R(s)$ á€¡á€–á€¼á€…á€º á€›á€±á€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹
- FrozenLake: State 15 (Goal) á€€á€­á€¯ á€›á€±á€¬á€€á€ºá€›á€„á€º +1, á€€á€»á€”á€ºá€¡á€¬á€¸á€œá€¯á€¶á€¸ 0

---

## 8. Horizon á€”á€¾á€„á€·á€º Episode

| Task á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ | á€–á€±á€¬á€ºá€•á€¼á€á€»á€€á€º |
|---|---|
| **Episodic task** | Terminal state á€›á€¾á€­á€á€²á€· finite time step task |
| **Continuing task** | Terminal state á€™á€›á€¾á€­á€˜á€² forever á€†á€€á€ºá€œá€¯á€•á€ºá€›á€á€²á€· task |
| **Greedy horizon** | Planning horizon = 1 (Bandit environments) |
| **Finite horizon** | á€€á€¼á€­á€¯á€á€„á€ºá€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€²á€· time step limit |
| **Infinite horizon** | Time limit á€™á€›á€¾á€­ (á€’á€«á€•á€±á€™á€šá€·á€º terminal state á€›á€±á€¬á€€á€ºá€›á€„á€º á€›á€•á€ºá€”á€­á€¯á€„á€º) |
| **Indefinite horizon** | Infinite horizon + episodic (most common in RL) |

- **Episode** á€†á€­á€¯á€á€¬ initial state á€€á€”á€± terminal state á€‘á€­ consecutive time steps sequence á€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹

---

## 9. Discount Factor ($\gamma$): Future á€€á€­á€¯ á€¡á€˜á€šá€ºá€™á€¾á€¬ á€á€”á€ºá€–á€­á€¯á€¸á€‘á€¬á€¸á€™á€œá€²

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

| Gamma á€á€”á€ºá€–á€­á€¯á€¸ | Behavior | á€¥á€•á€™á€¬ |
|---|---|---|
| $\gamma = 0$ | Myopic/Greedy â€” immediate reward á€•á€² á€‚á€›á€¯á€…á€­á€¯á€€á€º | $G_t = R_{t+1}$ |
| $\gamma = 0.5$ | Future reward á€€á€­á€¯ 50% á€á€»á€® discount | $G_t = R_{t+1} + 0.5R_{t+2} + 0.25R_{t+3} + \cdots$ |
| $\gamma = 0.99$ | Future reward á€€á€­á€¯ almost equal á€á€”á€ºá€–á€­á€¯á€¸á€‘á€¬á€¸ | BW, FL environments |
| $\gamma = 1$ | No discounting (finite horizon only) | $G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots$ |

Discount factor á€›á€²á€· á€›á€Šá€ºá€›á€½á€šá€ºá€á€»á€€á€ºá€™á€»á€¬á€¸:
1. Infinite sums converge á€–á€¼á€…á€ºá€…á€±á€á€¼á€„á€ºá€¸
2. Future uncertainty á€€á€­á€¯ á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€…á€‰á€ºá€¸á€…á€¬á€¸á€á€¼á€„á€ºá€¸
3. Variance reduction
4. Agent á€€á€­á€¯ urgency á€á€¶á€…á€¬á€¸á€…á€±á€á€¼á€„á€ºá€¸

### Discount Effect Visualization

$$\text{Value of +1 reward at time } k = \gamma^k \cdot (+1)$$

| Time step $k$ | $\gamma = 0.5$ | $\gamma = 0.9$ | $\gamma = 0.99$ |
|---|---|---|---|
| 0 | 1.000 | 1.000 | 1.000 |
| 5 | 0.031 | 0.590 | 0.951 |
| 10 | 0.001 | 0.349 | 0.904 |
| 50 | â‰ˆ 0 | 0.005 | 0.605 |
| 100 | â‰ˆ 0 | â‰ˆ 0 | 0.366 |

### Recursive Definition of Return

$$G_t = R_{t+1} + \gamma G_{t+1}$$

---

## 10. Concrete Environment Examples

### Bandit Walk (BW) â€” Deterministic

```mermaid
graph LR
    H["âŒ H(0)<br/>Hole"] <-->|"p=1.0"| H
    S["ğŸŸ¢ S(1)<br/>Start"] -->|"Left, p=1.0<br/>r=0"| H
    S -->|"Right, p=1.0<br/>r=+1"| G["ğŸ† G(2)<br/>Goal"]
    G <-->|"p=1.0"| G
    
    style H fill:#ff6b6b,color:#fff
    style S fill:#51cf66,color:#fff
    style G fill:#ffd43b,color:#000
```

- States: 3 ($|S^+| = 3$), Actions: $A = \{\text{Left}(0), \text{Right}(1)\}$
- Transition: Deterministic â€” $T(1, \text{Right}, 2) = 1.0$
- Reward: $R(1, \text{Right}, 2) = +1$, á€€á€»á€”á€ºá€¡á€¬á€¸á€œá€¯á€¶á€¸ $0$

### Bandit Slippery Walk (BSW) â€” Stochastic

```mermaid
graph LR
    H2["âŒ H(0)<br/>Hole"] <-->|"p=1.0"| H2
    S2["ğŸŸ¢ S(1)<br/>Start"] -->|"Right, p=0.8, r=+1"| G3["ğŸ† G(2)<br/>Goal"]
    S2 -->|"Right, p=0.2, r=0"| H2
    S2 -->|"Left, p=0.8, r=0"| H2
    S2 -->|"Left, p=0.2, r=+1"| G3
    G3 <-->|"p=1.0"| G3
    
    style H2 fill:#ff6b6b,color:#fff
    style S2 fill:#51cf66,color:#fff
    style G3 fill:#ffd43b,color:#000
```

- BW á€”á€²á€· á€á€°á€á€šá€º, á€’á€«á€•á€±á€™á€šá€·á€º **slippery surface** á€›á€¾á€­á€•á€«á€á€šá€º
- $T(1, \text{Right}, 2) = 0.8$, $T(1, \text{Right}, 0) = 0.2$

### Frozen Lake (FL) â€” 4x4 Grid

```mermaid
graph TD
    subgraph FL["Frozen Lake 4x4"]
        direction LR
        s0["S 0"] --- s1["Â· 1"] --- s2["Â· 2"] --- s3["Â· 3"]
        s4["Â· 4"] --- s5["H 5 âŒ"] --- s6["Â· 6"] --- s7["H 7 âŒ"]
        s8["Â· 8"] --- s9["Â· 9"] --- s10["Â· 10"] --- s11["H 11 âŒ"]
        s12["H 12 âŒ"] --- s13["Â· 13"] --- s14["Â· 14"] --- s15["G 15 ğŸ†"]
    end
    
    style s0 fill:#51cf66,color:#fff
    style s5 fill:#ff6b6b,color:#fff
    style s7 fill:#ff6b6b,color:#fff
    style s11 fill:#ff6b6b,color:#fff
    style s12 fill:#ff6b6b,color:#fff
    style s15 fill:#ffd43b,color:#000
```

- States: $|S^+| = 16$ (4Ã—4 grid), Actions: $A = \{\text{Left}, \text{Down}, \text{Right}, \text{Up}\}$
- Stochastic transitions: $T(s, a, s_{\text{intended}}) = \frac{1}{3}$, $T(s, a, s_{\text{orthogonal}}) = \frac{1}{3}$ each
- Terminal states: $S_{\text{terminal}} = \{5, 7, 11, 12, 15\}$
- Reward: $R(\cdot, \cdot, 15) = +1$, á€€á€»á€”á€ºá€¡á€¬á€¸á€œá€¯á€¶á€¸ $0$

---

## 11. MDP Extensions

| Extension | á€–á€±á€¬á€ºá€•á€¼á€á€»á€€á€º |
|---|---|
| **POMDP** | Agent á€€ environment state á€€á€­á€¯ partially observe á€•á€² á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€¼á€„á€ºá€¸ |
| **FMDP** | Transition/reward function á€€á€­á€¯ compact á€–á€±á€¬á€ºá€•á€¼á€á€¼á€„á€ºá€¸ |
| **Continuous MDP** | Time, action, state á€á€½á€± continuous á€–á€¼á€…á€ºá€á€¼á€„á€ºá€¸ |
| **RMDP** | Probabilistic + relational knowledge á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸ |
| **SMDP** | Multiple time steps á€€á€¼á€¬á€á€²á€· abstract actions |
| **MMDP** | Multiple agents á€á€…á€ºá€•á€¼á€­á€¯á€„á€ºá€”á€€á€º interact á€á€¼á€„á€ºá€¸ |
| **Dec-MDP** | Multiple agents collaborate á€•á€¼á€®á€¸ common reward maximize á€á€¼á€„á€ºá€¸ |

### MDP vs POMDP

$$\text{MDP} = \langle S, A, T, R, S_\theta, \gamma, H \rangle$$

$$\text{POMDP} = \langle S, A, T, R, S_\theta, \gamma, H, O, E \rangle$$

POMDP á€™á€¾á€¬ observation space $O$ á€”á€²á€· emission probability $E$ (state á€•á€±á€«á€ºá€™á€°á€á€Šá€ºá€•á€¼á€®á€¸ observation á€•á€±á€¸á€á€²á€· probability) á€‘á€•á€ºá€•á€«á€•á€«á€á€šá€ºá‹

---

## 12. Python MDP Representation

MDP á€€á€­á€¯ Python dictionary á€¡á€–á€¼á€…á€º á€–á€±á€¬á€ºá€•á€¼á€•á€«á€á€šá€º:

```python
P = {
    state: {
        action: [(probability, next_state, reward, done), ...]
    }
}
```

- **probability** â€” Transition probability
- **next_state** â€” á€›á€±á€¬á€€á€ºá€™á€šá€·á€º state
- **reward** â€” á€›á€™á€šá€·á€º reward
- **done** â€” Next state á€€ terminal á€Ÿá€¯á€á€º/á€™á€Ÿá€¯á€á€º (Boolean)

OpenAI Gym framework á€€ initial state distribution, discount factor, horizon á€…á€á€¬á€á€½á€±á€€á€­á€¯ internally handle á€•á€«á€á€šá€ºá‹

---

## 13. á€”á€­á€‚á€¯á€¶á€¸á€á€»á€¯á€•á€º

> RL á€•á€¼á€¿á€”á€¬á€€ agent á€”á€²á€· environment á€›á€²á€· interaction á€–á€¼á€…á€ºá€•á€¼á€®á€¸ MDP framework á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á virtually any complex sequential decision-making problem á€€á€­á€¯ model á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹

### MDP Components Overview

```mermaid
graph TB
    MDP["MDP = âŸ¨S, A, T, R, SÎ¸, Î³, HâŸ©"]
    MDP --> S["S: State Space<br/>Environment configurations"]
    MDP --> A["A: Action Space<br/>Available actions"]
    MDP --> T["T(s,a,s'): Transition Function<br/>State dynamics"]
    MDP --> R["R(s,a,s'): Reward Function<br/>Scalar feedback"]
    MDP --> Sth["SÎ¸: Initial Distribution<br/>Starting states"]
    MDP --> gam["Î³: Discount Factor<br/>Future value weight"]
    MDP --> H["H: Horizon<br/>Time constraint"]
    
    style MDP fill:#4CAF50,color:#fff,stroke:#333,stroke-width:2px
```

### Chapter 2 Key Equations

| Equation | Formula |
|---|---|
| MDP Definition | $\langle S, A, T, R, S_\theta, \gamma, H \rangle$ |
| Transition Function | $T(s,a,s') = P(s_t = s' \mid s_{t-1} = s, a_{t-1} = a)$ |
| Probability Constraint | $\sum_{s'} T(s, a, s') = 1, \; \forall s, a$ |
| Reward Function | $R(s, a, s') \rightarrow \mathbb{R}$ |
| Markov Property | $P(s_{t+1} \mid s_t, a_t) = P(s_{t+1} \mid h_t, s_t, a_t)$ |
| Discounted Return | $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ |
| Recursive Return | $G_t = R_{t+1} + \gamma G_{t+1}$ |
| POMDP Extension | $\langle S, A, T, R, S_\theta, \gamma, H, O, E \rangle$ |

Chapter 3 á€€á€”á€±á€…á€•á€¼á€®á€¸ agent á€›á€²á€· inner workings (policy, value functions, planning algorithms) á€á€½á€±á€€á€­á€¯ á€œá€±á€·á€œá€¬á€á€½á€¬á€¸á€•á€«á€™á€šá€ºá‹
